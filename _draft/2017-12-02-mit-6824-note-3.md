---
layout: post
date: 2017-12-02T15:19:31+08:00
title: MIT 6.824 学习笔记（三）
category: 读书笔记
---

本系列文章是对 [MIT 6.824](https://pdos.csail.mit.edu/6.824/schedule.html) 课程的学习笔记。

# Spinnaker

## Introduction

> Spinnaker is an experimental datastore that is designed to run on a large cluster of commodity servers in a single datacenter. This paper describes Spinnaker’s Paxos-based replication protocol. The use of Paxos ensures that a data partition in Spinnaker will be available for reads and writes as long a majority of its replicas are alive.

实现持续可用性的一个解决方案是**主从复制**，但主从复制存在以下缺陷：

<img src="/assets/images/mit-6824-note-3/illustration-1.png" width="800" />

在上图的例子中，主从节点都从 LSN=10 开始（a），之后 slave 节点挂了（b），master 节点继续接收写请求，一直到 LSN=20。之后 master 节点也挂了（c），之后 slave 节点恢复（d），然而，在此时 slave 节点不能接收任何读写请求因为它缺失了 LSN=11 到LSN＝20 之间的记录。如果要避免这种情况，只有在任意节点挂掉的时候，都阻塞写请求。但这样就降低了整个系统的 availability。

分布式系统中，**一致性模型描述了如何使不同的 relicas 保持同步**。**强一致性**保证了所有的 replicas 都是一致的，但要实现强一致性需要牺牲 availability 或网络分区容忍性。CAP 理论提出 **Consistency**、**Availability** 和 **Partition tolerance** 三者最多只能同时满足两项。

比如 Dynamo 这样的系统，使用**最终一致性**模型来提供高可用性和分区容忍性。Dynamo 是一个 AP 系统，牺牲了 Consistency。

Spinnaker 使用基于 Paxos 的协议来实现日志提交和故障恢复。Paxos 确保了系统在大多数节点存活的情况下可以运作。Spinnaker 是一个 CA 系统，用于单一的 datacenter，并使用另外的 replication strategy 来保证跨 datacenter 的容错性。 

## Related work

**Two-phase commit (2PC)** 是保持 replicas 一致的一种方式。但 2PC 更偏向于将每个 participant 当作一个独立的资源管理者，而不仅仅是 replica。使用 2PC 来实现 replication 有些 overkill，并且还有不少缺陷。首先单一节点失败会导致系统 abort。其次每个 transaction 都发起 2PC 会导致极差的性能。每次 2PC 都需要两次磁盘强制写和传输两条信息的时延。最后，2PC 在 coordinator 挂掉时无法运作。

Amazon 的 Dynamo 通过**分布式时钟**来解决最终一致性的问题。

Google 的 Bigtable 提供了强一致性，和 spinnaker 不同的是，Bigtable 依赖 GFS 来存储数据和日志，还有实现 replication。这样每个 transaction 的 workload 就加重了（需要和 gfs 的 master 交互）。

## Architecture

> Like Bigtable and PNUTS, Spinnaker distributes the rows of a table across its cluster using range partitioning. Each node is assigned a base key range, which is replicated on the next N − 1 nodes (N = 3 by default).

<img src="/assets/images/mit-6824-note-3/illustration-2.png" width="800" />

> Each group of nodes involved in replicating a key range is denoted as a **cohort**. Note that cohorts overlap.

每个日志由一个 LSN 唯一的标记。Commit queue 是在内存的数据结构，用于存放 pending writes。写操作只有在接收到大多数 cohort 的 ack 之后才能提交。在此之前都存放在 commit queue 中。

<img src="/assets/images/mit-6824-note-3/illustration-3.png" width="800" />

已经提交的写操作存于 memtable 中，并被定期刷到被称为 SSTable 的 immutable disk structure。SSTable 会被定期合并以提升读性能并删除不需要的数据。

## The replication protocol 

<img src="/assets/images/mit-6824-note-3/illustration-4.png" width="800" />

提交一个写操作需要三次日志强制写和四条信息交互，不过大多数操作都是重叠的（可以并行）。

## Recovery

Follower 的恢复需要两个阶段：**local recovery** 和 **catch up**。定义 f.cmt 表示 follower 的最后一个提交日志的 LSN，f.lst 表示 follower 的最后一个日志的 LSN。Local recovery 阶段，follower 从最近一次 checkpoint 开始重新执行日志直到 f.cmt，之后进入 catch up 阶段。Catch up 阶段，follower 通知 leader 自己的 f.cmt，leader 回复 f.cmt 之后所有的 commit writes。Leader 将会阻塞所有新的写请求直到 follower 已经跟上。

当 leader 挂掉，新的 leader 将被选举，并且会确保新的 leader 会包含所有已提交的写操作。在老的 leader 挂掉时，有可能其提交的写操作在某些 followers 还处于 pending 的状态。新 leader 将使用下图的算法，继续提交所有 unresolved 写操作。

<img src="/assets/images/mit-6824-note-3/illustration-5.png" width="800" />


## Leader election

选举算法如下图：

<img src="/assets/images/mit-6824-note-3/illustration-6.png" width="800" />

# Time, Clocks, and the Ordering of Events in a Distributed System 

## Introduction 

 A system is distributed if the message transmission delay is not negligible compared
to the time between events in a single process. 

In a distributed system, it is sometimes impossible to say that one of two events occurred first. The relation "happened before" is therefore only a partial ordering
of the events in the system. 

## The Partial Ordering 

Most people would probably say that an event a
happened before an event b if a happened at an earlier
time than b. They might justify this definition in terms
of physical theories of time. 
 there is still the problem that such
clocks are not perfectly accurate and do not keep precise
physical time. We will therefore define the "happened
before" relation without using physical clocks.

Most people would probably say that an event a
happened before an event b if a happened at an earlier
time than b. They might justify this definition in terms
of physical theories of time.  We are assuming that the events of a process form
a sequence, where a occurs before b in this sequence if
a happens before b. In other words, a single process is
defined to be a set of events with an a priori total
ordering. 

Definition. The relation "->" on the set of events of
a system is the smallest relation satisfying the following
three conditions: (1) If a and b are events in the same
process, and a comes before b, then a -> b. (2) If a is the
sending of a message by one process and b is the receipt
of the same message by another process, then a -> b. (3)
If a -> b and b -> c then a -> c. Two distinct events a
and b are said to be concurrent if a is not -> b and b is not -> a.

 -> is an
irreflexive partial ordering on the set of all events in the
system


<img src="/assets/images/mit-6824-note-3/illustration-7.png" width="800" />

Another way of viewing the definition is to say that
a -> b means that it is possible for event a to causally
affect event b. Two events are concurrent if neither can
causally affect the other. 

For example, events pa and q:~
of Figure 1 are concurrent. Even though we have drawn
the diagram to imply that q3 occurs at an earlier physical
time than 1)3, process P cannot know what process Q did
at qa until it receives the message at p, 

## Logical Clocks 

We now introduce clocks into the system. We begin
with an abstract point of view in which a clock is just a
way of assigning a number to an event, where the number
is thought of as the time at which the event occurred. 
More precisely, we define a clock Ci for each process Pi
to be a function which assigns a number Ci(a) to any
event a in that process. The entire system ofclbcks is
represented by the function C which assigns to any event
b the number C(b), where C(b) = C/(b) if b is an event
in process Pj. 

Clock Condition. For any events a, b:
if a---> b then C(a) < C(b). 

It is easy to see from our definition of the relation
"---~" that the Clock Condition is satisfied if the following
two conditions hold.
C 1. If a and b are events in process P~, and a comes
before b, then Ci(a) < Ci(b).
C2. If a is the sending of a message by process Pi
and b is the receipt of that message by process Pi, then
Ci(a) < Ci(b). 

We will show how to introduce clocks into the
processes which satisfy the Clock Condition. Process Pi's
clock is represented by a register Ci, so that C~(a) is the
value contained by C~ during the event a. The value of
C~ will change between events, so changing Ci does not
itself constitute an event. 

To guarantee that the system of clocks satisfies the
Clock Condition, we will insure that it satisfies conditions
C 1 and C2. Condition C 1 is simple; the processes need
only obey the following implementation rule:

IR1. Each process P~ increments Ci between any
two successive events. 

To meet condition C2, we require that each message
m contain a timestamp Tm which equals the time at which
the message was sent. Upon receiving a message timestamped
Tin, a process must advance its clock to be later
than Tin. 

IR2. (a) If event a is the sending of a message m
by process P~, then the message m contains a timestamp
Tm= Ci(a). (b) Upon receiving a message m, process
Pi sets Ci greater than or equal to its present value and
greater than Tin. 

## Ordering the Events Totally

We can use a system of clocks satisfying the Clock
Condition to place a total ordering on the set of all
system events. We simply order the events by the times 

we
define a relation ~ as follows: if a is an event in process
Pi and b is an event in process Pj, then a ~ b if and only
if either (i) Ci{a) < Cj(b) or (ii) El(a) ----" Cj(b) and Pi
< Py

Consider
a system composed of a fixed collection of processes
which share a single resource. Only one process can use
the resource at a time, so the processes must synchronize
themselves to avoid conflict. We wish to find an algorithm
for granting the resource to a process which satisfies
the following three conditions: (I) A process which
has been granted the resource must release it before it
can be granted to another process. (II) Different requests
for the resource must be granted in the order in which
they are made. (III) If every process which is granted the
resource eventually releases it, then every request is
eventually granted. 

Using a central scheduling process which grants
requests in the order they are received will not work,
unless additional assumptions are made. 

To solve the problem, we implement a system of clocks with'rules IR 1 and IR2, and use them to define a total ordering ~ of all events.

We assume first of all
that for any two processes P/and Pj, the messages sent
from Pi to Pi are received in the same order as they are
sent. Moreover, we assume that every message is eventually
received.  We also assume that a process can send
messages directly to every other process. 

Each process maintains its own request queue. We assume that the
request queues initially contain the single message To:Po
requests resource, where Po is the process initially granted
the resource and To is less than the initial value of any
clock. 

The algorithm is then defined by the following five
rules. For convenience, the actions defined by each rule
are assumed to form a single event. 

1. To request the resource, process Pi sends the message
TIn:P/requests resource to every other process, and
puts that message on its request queue, where T,~ is the
timestamp of the message.
2. When process Pj receives the message T,~:P~ requests
resource, it places it on its request queue and sends
a (timestamped) acknowledgment message to P~.'~
3. To release the resource, process P~ removes any
Tm:Pi requests resource message from its request queue
and sends a (timestamped) Pi releases resource message
to every other process.
4. When process Pj receives a Pi releases resource
message, it removes any Tm:P~ requests resource message
from its request queue. 

5. Process P/is granted the resource when the following
two conditions are satisfied: (i) There is a Tm:Pi
requests resource message in its request queue which is
ordered before any other request in its queue by the
relation ~. 
 (ii)
P~ has received a message from every other process timestamped
later than Tin

因为任一进程都要把自己的请求，不管是申请资源还是释放资源，通知到其他所有进程。如果规则5(ii)成立，说明Pi已经得知了其他所有进程至少在Tm之前的所有请求状态。假设此时不应该是Pi获得资源而应该是Pj获得资源，则Pj申请资源的事件的逻辑时钟序应该小于Tm，因此在Pi本地队列中有这一个消息，而且该消息在全序关系中比(Tm:Pi request resource)小，这跟规则5(i)矛盾。所以Pi仅根据本地判定自己获得资源是符合要求的。所以要求I是满足的。
因为只有3和4会删除消息，所以除非Pi释放了资源，否则(Tm:Pi request resource)就会一直在消息队列中，且占据着全序最小的位置，其他进程是不会判定自己可以获得资源的。
因为我们是按照全序关系来判定的，而全序又是从偏序扩展来的，因此不会违背偏序定义的顺序。要求II因此满足。规则3、4在删除了(Tm:Pi request resource)之后，必定有另外一个进程Pj的申请消息会占据全序的最小值位置，而ack消息也能保证5(ii)成立。因此III成立。

This is a distributed algorithm. Each process independently
follows these rules, and there is no central
synchronizing process or central storage. 

 The
synchronization is specified in terms of a State Machine,
consisting of a set C of possible commands, a set S of
possible states, and a function e: C× S--~ S. The relation
e(C, S) -- S' means that executing the command C with
the machine in state S causes the machine state to change
to S'. In our example, the set C consists of all the
commands Pi requests resource and P~ releases resource,
and the state consists of a queue of waiting request
commands, where the request at the head of the queue
is the currently granted one. Executing a request command
adds the request to the tail of the queue, and
executing a release command removes a command from
'he queue

Each process independently simulates the execution
of the State Machine, using the commands issued by all
the processes. Synchronization is achieved because all
processes order the commands according to their timestamps
(using the relation ~), so each process uses the
same sequence of commands. 

## Anomalous Behavior

 This permits
the following type of "anomalous behavior." Consider a
nationwide system of interconnected computers. Suppose
a person issues a request A on a computer A, and then
telephones a friend in another city to have him issue a
request B on a different computer B. It is quite possible
for request B to receive a lower timestamp and be ordered
before request A. This can happen because the system
has no way of knowing that A actually preceded B, since
that precedence informatiori is based on messages external
to the system.

There are two possible ways to avoid such anomalous
behavior. The first way is to explicitly introduce into the
system the necessary information about the ordering->. 

The second approach is to construct a system of
clocks which satisfies the following condition. 

## Physical Clocks 



http://loopjump.com/time_clock_order/

# Dynamo: Amazon’s Highly Available Key-value Store 

## Introduction

一个系统的**可靠性**和**可扩展性**取决于如何管理应用状态。Amazon 采用一种高度去中心化，松耦合，由数百个服务组成的面向服务架构。在这种环境中尤其需要一个高可用的存储技术。

Dynamo 被设计用于管理服务状态，要求具备非常高的可靠性，而且需要严格控制**可用性、一致性、成本效益和性能**的之间的权衡。

Amazon 服务平台中许多服务只需要主键访问数据存储。通常关系数据库模式会导致效率低下，且可扩展性和可用性也有限。Dynamo 提供了一个简单且唯一的主键接口，以满足这些应用的要求。

Dynamo 综合一些著名的技术来实现可伸缩性和可用性：使用一致性哈希划分和复制数据，一致性由对象版本来实现。更新时副本之间的一致性是由类似仲裁（quorum-like）的技术和去中心化的副本同步协议来保证。Dynamo 采用了基于 gossip 的分布式故障检测和 membership protocol。

## System Assumptions and Requirements

Dynamo 的目标应用程序是通过较弱的一致性（ACID中的“C”）来达到高可用性。Dynamo 不提供任何数据隔离（ACID中的“I”）保证，只允许单一主键更新。

## Design Considerations

Data replication algorithms used in commercial systems
traditionally perform synchronous replica coordination in order to
provide a strongly consistent data access interface. To achieve this
level of consistency, these algorithms are forced to tradeoff the
availability of the data under certain failure scenarios.

对于容易出现的服务器和网络故障的系统，可使用乐观复制技术来提高系统的可用性，其变化可以在后台传播到副本，同时，并发和断开(disconnected)是可以容忍的。这种方法的挑战在于，它会导致更改冲突，而这些冲突必须检测并协调解决。这种协调冲突的过程引入了两个问题：何时协调它们，谁协调它们。Dynamo被设计成最终一致性(eventually consistent)的数据存储，即所有的更新操作，最终达到所有副本。

This process of conflict resolution
introduces two problems: when to resolve them and who resolves
them. Dynamo is designed to be an eventually consistent data
store; that is all updates reach all replicas eventually. 

An important design consideration is to decide when to perform
the process of resolving update conflicts, i.e., whether conflicts
should be resolved during reads or writes. Dynamo targets the design space of an “always writeable” data store. This requirement forces us to push the
complexity of conflict resolution to the reads in order to ensure
that writes are never rejected. 

Other key principles embraced in the design are:

* Incremental scalability:  Dynamo should be able to scale out one
storage host (henceforth, referred to as “node”) at a time, with
minimal impact on both operators of the system and the system
itself. 
* Symmetry:  Every node in Dynamo should have the same set of
responsibilities as its peers; there should be no distinguished node
or nodes that take special roles or extra set of responsibilities.
* Decentralization:  An extension of symmetry, the design should
favor decentralized peer-to-peer techniques over centralized
control. 
* Heterogeneity: The system needs to be able to exploit
heterogeneity in the infrastructure it runs on.   the work
distribution must be proportional to the capabilities of the
individual servers. 

## RELATED WORK 

Dynamo differs from the aforementioned decentralized storage
systems in terms of its target requirements. First, Dynamo is
targeted mainly at applications that need an “always writeable”
data store where no updates are rejected due to failures or
concurrent writes. 

 Dynamo is built for
latency sensitive applications that require at least 99.9% of read
and write operations to be performed within a few hundred
milliseconds. To meet these stringent latency requirements, it was
imperative for us to avoid routing requests through multiple nodes 

This is because multihop
routing increases variability in response times, thereby
increasing the latency at higher percentiles. Dynamo can be
characterized as a zero-hop DHT, where each node maintains
enough routing information locally to route a request to the
appropriate node directly. 

## SYSTEM ARCHITECTURE 


<img src="/assets/images/mit-6824-note-3/illustration-8.png" width="800" />

Dynamo stores objects associated with a key through a simple
interface; it exposes two operations: get() and put(). The get(key)
operation locates the object replicas associated with the key in the
storage system and returns a single object or a list of objects with
conflicting versions along with a context. 

 The put(key, context,
object) operation determines where the replicas of the object
should be placed based on the associated key, and writes the
replicas to disk. The context encodes system metadata about the
object that is opaque to the caller and includes information such as
the version of the object. 

One of the key design requirements for Dynamo is that it must
scale incrementally. This requires a mechanism to dynamically
partition the data over the set of nodes (i.e., storage hosts) in the
system. Dynamo’s partitioning scheme relies on consistent
hashing to distribute the load across multiple storage hosts. 

 The principle
advantage of consistent hashing is that departure or arrival of a
node only affects its immediate neighbors and other nodes remain
unaffected. 

The basic consistent hashing algorithm presents some challenges.
First, the random position assignment of each node on the ring
leads to non-uniform data and load distribution. Second, the basic
algorithm is oblivious to the heterogeneity in the performance of
nodes.

To address these issues, Dynamo uses a variant of
consistent hashing (similar to the one used in [10, 20]): instead of
mapping a node to a single point in the circle, each node gets
assigned to multiple points in the ring. To this end, Dynamo uses
the concept of “virtual nodes”. A virtual node looks like a single
node in the system, but each node can be responsible for more
than one virtual node. Effectively, when a new node is added to
the system, it is assigned multiple positions (henceforth, “tokens”)
in the ring. 

In order to provide this kind of guarantee, Dynamo treats the
result of each modification as a new and immutable version of the
data. It allows for multiple versions of an object to be present in
the system at the same time. 

Most of the time, new versions
subsume the previous version(s), and the system itself can
determine the authoritative version (syntactic reconciliation).
However, version branching may happen, in the presence of
failures combined with concurrent updates, resulting in
conflicting versions of an object. In these cases, the system cannot
reconcile the multiple versions of the same object and the client
must perform the reconciliation in order to collapse multiple
branches of data evolution back into one (semantic
reconciliation).

A node handling a read or write operation is known as the
coordinator. Typically, this is the first among the top N nodes in
the preference list.

 If the requests are received through a load
balancer, requests to access a key may be routed to any random
node in the ring. In this scenario, the node that receives the
request will not coordinate it if the node is not in the top N of the
requested key’s preference list. Instead, that node will forward the
request to the first among the top N nodes in the preference list. 

To maintain consistency among its replicas, Dynamo uses a
consistency protocol similar to those used in quorum systems.
This protocol has two key configurable values: R and W. R is the
minimum number of nodes that must participate in a successful
read operation. W is the minimum number of nodes that must
participate in a successful write operation. Setting R and W such that R + W > N yields a quorum-like system.  In this model, the
latency of a get (or put) operation is dictated by the slowest of the
R (or W) replicas. For this reason, R and W are usually
configured to be less than N, to provide better latency. 

Upon receiving a put() request for a key, the coordinator generates
the vector clock for the new version and writes the new version
locally. The coordinator then sends the new version (along with the new vector clock) to the N highest-ranked reachable nodes. If
at least W-1 nodes respond then the write is considered
successful. 

Similarly, for a get() request, the coordinator requests all existing
versions of data for that key from the N highest-ranked reachable
nodes in the preference list for that key, and then waits for R
responses before returning the result to the client.  If the
coordinator ends up gathering multiple versions of the data, it
returns all the versions it deems to be causally unrelated. The
divergent versions are then reconciled and the reconciled version
superseding the current versions is written back. 

If Dynamo used a traditional quorum approach it would be
unavailable during server failures and network partitions, and
would have reduced durability even under the simplest of failure
conditions. To remedy this it does not enforce strict quorum
membership and instead it uses a “sloppy quorum”. all read and
write operations are performed on the first N healthy nodes from
the preference list, which may not always be the first N nodes
encountered while walking the consistent hashing ring. 

If node A is temporarily down or
unreachable during a write operation then a replica that would
normally have lived on A will now be sent to node D. This is done
to maintain the desired availability and durability guarantees. The
replica sent to D will have a hint in its metadata that suggests
which node was the intended recipient of the replica (in this case
A). Nodes that receive hinted replicas will keep them in a
separate local database that is scanned periodically. Upon
detecting that A has recovered, D will attempt to deliver the
replica to A. 

Using hinted handoff, Dynamo ensures that the read and write
operations are not failed due to temporary node or network
failures. 

Dynamo is configured such that
each object is replicated across multiple data centers. In essence,
the preference list of a key is constructed such that the storage
nodes are spread across multiple data centers. These datacenters
are connected through high speed network links. This scheme of
replicating across multiple datacenters allows us to handle entire
data center failures without a data outage. 

Dynamo implements an anti-entropy (replica
synchronization) protocol to keep the replicas synchronized. 

To detect the inconsistencies between replicas faster and to
minimize the amount of transferred data, Dynamo uses Merkle
trees [13]. A Merkle tree is a hash tree where leaves are hashes of
the values of individual keys. The principal advantage of
Merkle tree is that each branch of the tree can be checked
independently without requiring nodes to download the entire tree
or the entire data set. Moreover, Merkle trees help in reducing the
amount of data that needs to be transferred while checking for
inconsistencies among replicas. 

Dynamo uses Merkle trees for anti-entropy as follows: Each node
maintains a separate Merkle tree for each key range.

A gossip-based protocol
propagates membership changes and maintains an eventually
consistent view of membership. Each node contacts a peer chosen
at random every second and the two nodes efficiently reconcile
their persisted membership change histories. 

When a node starts for the first time, it chooses its set of tokens
(virtual nodes in the consistent hash space) and maps nodes to
their respective token sets. The mapping is persisted on disk and initially contains only the local node and token set. The mappings
stored at different Dynamo nodes are reconciled during the same
communication exchange that reconciles the membership change
histories. Therefore, partitioning and placement information also
propagates via the gossip-based protocol and each storage node is
aware of the token ranges handled by its peers. This allows each
node to forward a key’s read/write operations to the right set of
nodes directly. 

 a purely local notion of failure detection is entirely sufficient: node
A may consider node B failed if node B does not respond to node
A’s messages (even if B is responsive to node C's messages). 

Decentralized failure detection protocols use a simple gossip-style
protocol that enable each node in the system to learn about the
arrival (or departure) of other nodes. 

## IMPLEMENTATION 

In Dynamo, each storage node has three main software
components: request coordination, membership and failure
detection, and a local persistence engine. 

Dynamo’s local persistence component allows for different
storage engines to be plugged in. The main reason for designing a pluggable
persistence component is to choose the storage engine best suited
for an application’s access patterns. 

The request coordination component is built on top of an eventdriven
messaging substrate where the message processing pipeline
is split into multiple stages.

 Each
client request results in the creation of a state machine on the node
that received the client request. The state machine contains all the
logic for identifying the nodes responsible for a key, sending the
requests, waiting for responses, potentially doing retries,
processing the replies and packaging the response to the client.
Each state machine instance handles exactly one client request.

##
 http://blog.csdn.net/yfkiss/article/details/39966087
