---
layout: post
date: 2018-01-11T22:14:04+08:00
title: MIT 6.824 学习笔记（五）
category: 读书笔记
---

本系列文章是对 [MIT 6.824](https://pdos.csail.mit.edu/6.824/schedule.html) 课程的学习笔记。

# FaRM Note

FaRM writes go to RAM, not disk -- eliminates a huge bottleneck. Can write RAM in 200 ns, but takes 10 ms to write hard drive, 100 us for SSD, but RAM loses content in power failure! Not persistent by itself.  

Just write to RAM of f+1 machines, to tolerate f failures? Might be enough if failures were always independent, but power failure is not independent -- may strike 100% of machines!

So batteries in every rack, can run machines for a few minutes: **"non-volatile RAM"**. What if crash prevents s/w from writing SSD, e.g bug in FaRM or kernel, or cpu/memory/hardware error. FaRM copes with single-machine crashes by copying data from RAM of machines' replicas to other machines to ensure always f+1 copies. Crashes (other than power failure) must be independent!


why is the network often a performance bottleneck?

```
    the usual setup:
    app                       app
    ---                       ---
    socket buffers            buffers
    TCP                       TCP
    NIC driver                driver
    NIC  -------------------- NIC
```

lots of expensive CPU operations:

* system calls
* copy messages
* interrupts
    
and all twice if RPC! It's hard to build RPC than can deliver more than a few 100,000 / second wire b/w (e.g. 10 gigabits/second) is rarely the limit for short RPC. These per-packet CPU costs are the limiting factor for small messages.


Two classes of concurrency control for transactions:

* pessimistic:
    wait for lock on first use of object; hold until commit/abort, called two-phase locking. Conflicts cause delays

* optimistic:
    access object without locking; commit "validates" to see if OK. 
    * Valid: do the writes
    * Invalid: abort
    called **Optimistic Concurrency Control (OCC)**


FaRM uses OCC. The reason: OCC lets FaRM read using one-sided RDMA reads, server storing the object does not need to set a lock, due to OCC.

FaRM transaction API (simplified):

```
  txCreate()
  o = txRead(oid)  -- RDMA
  o.f += 1
  txWrite(oid, o)  -- purely local
  ok = txCommit()  -- Figure 4
```

What's in an oid: <region #, address>. ```region #``` indexes a mapping to [ primary, backup1, ... ]. Target NIC can use address directly to read or write RAM so target CPU doesn't have to be involved.


Server memory layout: regions, each an array of objects
Object layout: header with version # and lock

Every region replicated on one primary, f backups -- f+1 replicas. Only the primary serves reads; all f+1 see commits+writes replication yields availability if <= f failures, i.e. available as long as one replica stays alive; better than Raft!


* txRead
    one-sided RDMA to fetch object direct from primary's memory -- fast!
    also fetches object's version number, to detect concurrent writes

* txWrite
    must be preceded by txRead
    just writes local copy; no communication

* transaction execution / commit protocol without failure -- Figure 4
  
  * LOCK (first message in commit protocol)
  
        TC sends to primary of each written object
        TC uses RDMA to append to its log at each primary
        LOCK record contains oid, version # xaction read, new value
        primary s/w polls log, sees LOCK, validates, sends "yes" or "no" reply message
        note LOCK is both logged in primary's NVRAM *and* an RPC exchange

* what does primary CPU do on receipt of LOCK?

    (for each object)
    if object locked, or version != what xaction read, reply "no"
    otherwise set the lock flag and return "yes"
    note: does *not* block if object is already locked

* TC waits for all LOCK reply messages

    if any "no", abort
    send ABORT to primaries so they can release locks
    returns "no" from txCommit()

* TC sends COMMIT-PRIMARY to primary of each written object

    uses RDMA to append to primary's log
    TC only waits for hardware ack -- does not wait for primary to process log entry
    TC returns "yes" from txCommit()

* what does primary do when it processes the COMMIT-PRIMARY in its log?

    copy new value over object's memory
    increment object's version #
    clear object's lock flag


# Paxos Made Simple

## The Consensus Algorithm
Assume a collection of processes that can propose values. 

 The safety
requirements for consensus are:
• Only a value that has been proposed may be chosen,
• Only a single value is chosen, and
• A process never learns that a value has been chosen unless it actually has been

We let the three roles in the consensus algorithm be performed by three classes of agents: proposers, acceptors, and learners.

A proposer sends a proposed value to a
set of acceptors. An acceptor may accept the proposed value. The value is
chosen when a large enough set of acceptors have accepted it. Because any two majorities
have at least one acceptor in common, this works if an acceptor can accept
at most one value. 


In the absence of failure or message loss, we want a value to be chosen
even if only one value is proposed by a single proposer. This suggests the
requirement:
P1. An acceptor must accept the first proposal that it receives.

P1 and the requirement that a value is chosen only when it is accepted
by a majority of acceptors imply that an acceptor must be allowed to accept
more than one proposal. We keep track of the different proposals that an
acceptor may accept by assigning a (natural) number to each proposal, so a
proposal consists of a proposal number and a value. To prevent confusion,
we require that different proposals have different numbers.  A
value is chosen when a single proposal with that value has been accepted by
a majority of the acceptors. In that case, we say that the proposal (as well
as its value) has been chosen.

We can allow multiple proposals to be chosen, but we must guarantee
that all chosen proposals have the same value. By induction on the proposal
number, it suffices to guarantee:
P2. If a proposal with value v is chosen, then every higher-numbered proposal
that is chosen has value v.

To be chosen, a proposal must be accepted by at least one acceptor. So,
we can satisfy P2 by satisfying:
P2a
. If a proposal with value v is chosen, then every higher-numbered proposal
accepted by any acceptor has value v.

Because communication
is asynchronous, a proposal could be chosen with some particular
acceptor c never having received any proposal. Suppose a new proposer
“wakes up” and issues a higher-numbered proposal with a different value.
P1 requires c to accept this proposal, violating P2a
. Maintaining both P1
and P2a
requires strengthening P2a
to:
P2b
. If a proposal with value v is chosen, then every higher-numbered proposal
issued by any proposer has value v.

Since any set S consisting of a majority of acceptors contains at least one
member of C , we can conclude that a proposal numbered n has value v by
ensuring that the following invariant is maintained:
P2c
. For any v and n, if a proposal with value v and number n is issued,
then there is a set S consisting of a majority of acceptors such that
either (a) no acceptor in S has accepted any proposal numbered less
than n, or (b) v is the value of the highest-numbered proposal among
all proposals numbered less than n accepted by the acceptors in S.

We can therefore satisfy P2b by maintaining the invariance of P2c
.

To maintain the invariance of P2c
, a proposer that wants to issue a proposal
numbered n must learn the highest-numbered proposal with number
less than n, if any, that has been or will be accepted by each acceptor in
some majority of acceptors.

1. A proposer chooses a new proposal number n and sends a request to
each member of some set of acceptors, asking it to respond with:
(a) A promise never again to accept a proposal numbered less than
n, and
(b) The proposal with the highest number less than n that it has
accepted, if any.
I will call such a request a prepare request with number n.

2. If the proposer receives the requested responses from a majority of
the acceptors, then it can issue a proposal with number n and value
v, where v is the value of the highest-numbered proposal among the
responses, or is any value selected by the proposer if the responders
reported no proposals.

A proposer issues a proposal by sending, to some set of acceptors, a request
that the proposal be accepted. (This need not be the same set of acceptors
that responded to the initial requests.) Let’s call this an accept request

P1a
. An acceptor can accept a proposal numbered n iff it has not responded
to a prepare request having a number greater than n.


 The final
algorithm is obtained by making one small optimization.
Suppose an acceptor receives a prepare request numbered n, but it has
already responded to a prepare request numbered greater than n, thereby
promising not to accept any new proposal numbered n. There is then no
reason for the acceptor to respond to the new prepare request, since it will
not accept the proposal numbered n that the proposer wants to issue. We also have it ignore
a prepare request for a proposal it has already accepted.

Phase 1. (a) A proposer selects a proposal number n and sends a prepare
request with number n to a majority of acceptors.
(b) If an acceptor receives a prepare request with number n greater
than that of any prepare request to which it has already responded,
then it responds to the request with a promise not to accept any more
proposals numbered less than n and with the highest-numbered proposal
(if any) that it has accepted.

Phase 2. (a) If the proposer receives a response to its prepare requests
(numbered n) from a majority of acceptors, then it sends an accept
request to each of those acceptors for a proposal numbered n with a
value v, where v is the value of the highest-numbered proposal among
the responses, or is any value if the responses reported no proposals.
(b) If an acceptor receives an accept request for a proposal numbered
n, it accepts the proposal unless it has already responded to a prepare
request having a number greater than n.

It’s easy to construct a scenario in which two proposers each keep issuing
a sequence of proposals with increasing numbers, none of which are ever
chosen. Proposer p completes phase 1 for a proposal number n1. Another
proposer q then completes phase 1 for a proposal number n2 > n1. Proposer
p’s phase 2 accept requests for a proposal numbered n1 are ignored because
the acceptors have all promised not to accept any new proposal numbered
less than n2.

To guarantee progress, a distinguished proposer must be selected as the
only one to try issuing proposals.

The Paxos algorithm [5] assumes a network of processes. In its consensus
algorithm, each process plays the role of proposer, acceptor, and learner.
The algorithm chooses a leader, which plays the roles of the distinguished proposer and the distinguished learner. 


All that remains is to describe the mechanism for guaranteeing that no
two proposals are ever issued with the same number. Different proposers
choose their numbers from disjoint sets of numbers, so two different proposers
never issue a proposal with the same number. Each proposer remembers
(in stable storage) the highest-numbered proposal it has tried to issue,
and begins phase 1 with a higher proposal number than any it has already
used.

# MDCC: Multi-Data Center Consistency

## Introduction

MDCC (Multi-Data Center Consistency) is an
optimistic commit protocol for geo-replicated transactions,
that does not require a master or static partitioning, and is
strongly consistent at a cost similar to eventually consistent
protocols. 

Replication across geographically diverse data centers
(called geo-replication) is qualitatively different from replication
within a cluster, data center or region, because interdata
center network delays are in the hundreds of milliseconds
and vary significantly. These delays are close enough
to the limit on total latency that users will tolerate, so it becomes
crucial to reduce the number of message round-trips taken between data centers, and desirable to avoid waiting
for the slowest data center to respond.

The traditional mechanism for transactions that are
distributed across databases is two-phase commit (2PC), but
this has serious drawbacks in a geo-replicated system. 2PC
depends on a reliable coordinator to determine the outcome
of a transaction, so it will block for the duration of a coordinator
failure, and (even worse) the blocked transaction will
be holding locks that prevent other transactions from making
progress until the recovery is completed.

MDCC requires only a single wide-area message
round-trip to commit a transaction in the common case, and
is “master-bypassing”, meaning it can read or update from
any node in any data center. 

MDCC is the first protocol to use Generalized Paxos [15] as a commit protocol on a per record basis, combining it with techniques from
the database community (escrow transactions [19] and demarcation
[3]). The key idea is to achieve single round-trip
commits by 1) executing parallel Generalized Paxos on each
record, 2) ensuring every prepare has been received by a
fast quorum of replicas, 3) disallowing aborts for successfully
prepared records, and 4) piggybacking notification of
commit state on subsequent transactions.


## Architecture Overview

MDCC uses a library-centric approach similar to the architectures
of DBS3 [5], Megastore [2] or Spanner [8] (as
shown in Figure 1). This architecture separates the stateful
component of a database system as a distributed record manager.
All higher-level functionality (such as query processing
and transaction management) is provided through a stateless
DB library, which can be deployed at the application server.

As a result, the only stateful component of the architecture,
the storage node, is significantly simplified and scalable
through standard techniques such as range partitioning,
whereas all higher layers of the database can be replicated
freely with the application tier because they are stateless.

MDCC places storage nodes in geographically distributed
data centers, with every node being responsible for one or more horizontal partitions. we assume
for the remainder of the paper that every data center
contains a full replica of the data, and the data within a single
data center is partitioned across machines.

The DB library provides a programming model for transactions
and is mainly responsible for coordinating the replication
and consistency of the data using MDCC’s commit
protocol. The DB library also acts as a transaction manager
and is responsible to determine the outcome of a transaction.

In contrast to many other systems, MDCC supports an individual
master per record, which can either be storage nodes
or app-server and is responsible to coordinate the updates
to a record.

## The MDCC Protocol

### Classic Paxos

Paxos distinguishes between clients, proposers, acceptors
and learners. These can be directly mapped to our scenario,
where clients are app-servers, proposers are masters, acceptors
are storage nodes and all nodes are learners.

The basic idea in Classic Paxos [14], as applied for replicating
a transaction’s updates to data, is as follows: Every
record has a master responsible for coordinating updates to
the record. At the end of a transaction, the app-server sends
the update requests to the masters of each the record. The master informs all
storage nodes responsible for the record that it is the master
for the next update. It is possible that multiple masters exist
for a record, but to make progress, eventually only one
master is allowed. The master processes the client request
by attempting to convince the storage nodes to agree on it.
A storage node accepts an update if and only if it comes
from the most recent master the node knows of, and it has
not already accepted a more recent update for the record.

the Classic Paxos algorithm operates in
two phases. Phase 1 tries to establish the mastership for
an update for a specific record r. A master P, selects a
proposal number m, also referred to as a ballot number or
round, higher than any known proposal number and sends
a Phase1a request with m to at least a majority of storage
nodes responsible for r. The proposal numbers must be unique for each master because they are used to determine
the latest request. If a storage node receives a Phase1a request
greater than any proposal number it has already responded
to, it responds with a Phase1b message containing
m, the highest-numbered update (if any) including its proposal
number n, and promises not to accept any future requests
less than or equal to m. If P receives responses containing
its proposal number m from a majority QC of storage
nodes, it has been chosen as a master. Now, only P will be
able to commit a value for proposal number m.

Phase 2 tries to write a value. P sends an accept request
Phase2a to all the storage nodes of Phase 1 with the ballot
number m and value v. v is either the update of the highestnumbered
proposal among the Phase1b responses, or the
requested update from the client if no Phase1b responses
contained a value.

### Transaction Support

We guarantee this consistency level by using a Paxos instance
per record to accept an option to execute the update,
instead of writing the value directly. After the app-server
learns the options for all the records in a transaction, it commits
the transaction and asynchronously notifies the storage
nodes to execute the options. If an option is not yet executed,
it is called an outstanding option.

Updates to records create new versions, and are represented
in the form vread → vwrite, where vread is the
version of the record read by the transaction and vwrite is
the new version of the record. This allows MDCC to detect
write-write conflicts by comparing the current version of a
record with vread. If they are not equal, the record was modified
between the read and write and a write-write conflict
was encountered.

The app-server coordinates the transaction by trying to
get the options accepted for all updates. It proposes the options
to the Paxos instances running for each record, with the
participants being the replicas of the record. Every storage
node responds to the app-server with an accept or reject of
the option, depending on if vread is valid. Hence, the storage nodes make an
active decision to accept or reject the option. This is fundamentally
different than existing uses of Paxos , which require to send a fixed value (e.g., the “final” accept or commit
decision) and only decide based on the ballot number if the
value should be accepted. The reason why this change does
not violate the Paxos assumptions is that we defined at the
end of Section 3.1.1 that a new record version can only be
chosen if the previous version was successfully determined.
Thus, all storage nodes will always make the same abort or
commit decision.

Just as in 2PC, the app-server commits a transaction when
it learns all options as accepted, and aborts a transaction
when it learns any option as rejected. The app-server learns
an option if and only if a majority of storage nodes agrees
on the option. 

In contrast to 2PC we made another important
change: MDCC does not allow clients or app-servers
to abort a transaction once it has been proposed. Decisions
are determined and stored by the distributed storage nodes
with MDCC, instead of being decided by a single coordinator
with 2PC. This ensures that the commit status of a
transaction depends only on the status of the learned options
and hence is always deterministic even with failures.

If the app-server determines that the transaction is aborted
or committed, it informs involved storage nodes through a
Learned message about the decision. The storage nodes in
turn execute the option (make visible) or mark it as rejected.

 We apply a simple
pessimistic strategy to avoid deadlocks. The core idea is to
relax the requirement that we can only learn a new version if
the previous instance is committed.