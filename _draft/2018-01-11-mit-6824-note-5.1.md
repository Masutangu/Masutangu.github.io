---
layout: post
date: 2018-01-11T22:14:04+08:00
title: MIT 6.824 学习笔记（五）
category: 读书笔记
---

本系列文章是对 [MIT 6.824](https://pdos.csail.mit.edu/6.824/schedule.html) 课程的学习笔记。

# FaRM Note

FaRM writes go to RAM, not disk -- eliminates a huge bottleneck. Can write RAM in 200 ns, but takes 10 ms to write hard drive, 100 us for SSD, but RAM loses content in power failure! Not persistent by itself.  

Just write to RAM of f+1 machines, to tolerate f failures? Might be enough if failures were always independent, but power failure is not independent -- may strike 100% of machines!

So batteries in every rack, can run machines for a few minutes: **"non-volatile RAM"**. What if crash prevents s/w from writing SSD, e.g bug in FaRM or kernel, or cpu/memory/hardware error. FaRM copes with single-machine crashes by copying data from RAM of machines' replicas to other machines to ensure always f+1 copies. Crashes (other than power failure) must be independent!


why is the network often a performance bottleneck?

```
    the usual setup:
    app                       app
    ---                       ---
    socket buffers            buffers
    TCP                       TCP
    NIC driver                driver
    NIC  -------------------- NIC
```

lots of expensive CPU operations:

* system calls
* copy messages
* interrupts
    
and all twice if RPC! It's hard to build RPC than can deliver more than a few 100,000 / second wire b/w (e.g. 10 gigabits/second) is rarely the limit for short RPC. These per-packet CPU costs are the limiting factor for small messages.


Two classes of concurrency control for transactions:

* pessimistic:
    wait for lock on first use of object; hold until commit/abort, called two-phase locking. Conflicts cause delays

* optimistic:
    access object without locking; commit "validates" to see if OK. 
    * Valid: do the writes
    * Invalid: abort
    called **Optimistic Concurrency Control (OCC)**


FaRM uses OCC. The reason: OCC lets FaRM read using one-sided RDMA reads, server storing the object does not need to set a lock, due to OCC.

FaRM transaction API (simplified):

```
  txCreate()
  o = txRead(oid)  -- RDMA
  o.f += 1
  txWrite(oid, o)  -- purely local
  ok = txCommit()  -- Figure 4
```

What's in an oid: <region #, address>. ```region #``` indexes a mapping to [ primary, backup1, ... ]. Target NIC can use address directly to read or write RAM so target CPU doesn't have to be involved.


Server memory layout: regions, each an array of objects
Object layout: header with version # and lock

Every region replicated on one primary, f backups -- f+1 replicas. Only the primary serves reads; all f+1 see commits+writes replication yields availability if <= f failures, i.e. available as long as one replica stays alive; better than Raft!


* txRead
    one-sided RDMA to fetch object direct from primary's memory -- fast!
    also fetches object's version number, to detect concurrent writes

* txWrite
    must be preceded by txRead
    just writes local copy; no communication

* transaction execution / commit protocol without failure -- Figure 4
  
  * LOCK (first message in commit protocol)
  
        TC sends to primary of each written object
        TC uses RDMA to append to its log at each primary
        LOCK record contains oid, version # xaction read, new value
        primary s/w polls log, sees LOCK, validates, sends "yes" or "no" reply message
        note LOCK is both logged in primary's NVRAM *and* an RPC exchange

* what does primary CPU do on receipt of LOCK?

    (for each object)
    if object locked, or version != what xaction read, reply "no"
    otherwise set the lock flag and return "yes"
    note: does *not* block if object is already locked

* TC waits for all LOCK reply messages

    if any "no", abort
    send ABORT to primaries so they can release locks
    returns "no" from txCommit()

* TC sends COMMIT-PRIMARY to primary of each written object

    uses RDMA to append to primary's log
    TC only waits for hardware ack -- does not wait for primary to process log entry
    TC returns "yes" from txCommit()

* what does primary do when it processes the COMMIT-PRIMARY in its log?

    copy new value over object's memory
    increment object's version #
    clear object's lock flag


# Paxos Made Simple

## The Consensus Algorithm
Assume a collection of processes that can propose values. 

 The safety
requirements for consensus are:
• Only a value that has been proposed may be chosen,
• Only a single value is chosen, and
• A process never learns that a value has been chosen unless it actually has been

We let the three roles in the consensus algorithm be performed by three classes of agents: proposers, acceptors, and learners.

A proposer sends a proposed value to a
set of acceptors. An acceptor may accept the proposed value. The value is
chosen when a large enough set of acceptors have accepted it. Because any two majorities
have at least one acceptor in common, this works if an acceptor can accept
at most one value. 


In the absence of failure or message loss, we want a value to be chosen
even if only one value is proposed by a single proposer. This suggests the
requirement:
P1. An acceptor must accept the first proposal that it receives.

P1 and the requirement that a value is chosen only when it is accepted
by a majority of acceptors imply that an acceptor must be allowed to accept
more than one proposal. We keep track of the different proposals that an
acceptor may accept by assigning a (natural) number to each proposal, so a
proposal consists of a proposal number and a value. To prevent confusion,
we require that different proposals have different numbers.  A
value is chosen when a single proposal with that value has been accepted by
a majority of the acceptors. In that case, we say that the proposal (as well
as its value) has been chosen.

We can allow multiple proposals to be chosen, but we must guarantee
that all chosen proposals have the same value. By induction on the proposal
number, it suffices to guarantee:
P2. If a proposal with value v is chosen, then every higher-numbered proposal
that is chosen has value v.

To be chosen, a proposal must be accepted by at least one acceptor. So,
we can satisfy P2 by satisfying:
P2a
. If a proposal with value v is chosen, then every higher-numbered proposal
accepted by any acceptor has value v.

Because communication
is asynchronous, a proposal could be chosen with some particular
acceptor c never having received any proposal. Suppose a new proposer
“wakes up” and issues a higher-numbered proposal with a different value.
P1 requires c to accept this proposal, violating P2a
. Maintaining both P1
and P2a
requires strengthening P2a
to:
P2b
. If a proposal with value v is chosen, then every higher-numbered proposal
issued by any proposer has value v.

Since any set S consisting of a majority of acceptors contains at least one
member of C , we can conclude that a proposal numbered n has value v by
ensuring that the following invariant is maintained:
P2c
. For any v and n, if a proposal with value v and number n is issued,
then there is a set S consisting of a majority of acceptors such that
either (a) no acceptor in S has accepted any proposal numbered less
than n, or (b) v is the value of the highest-numbered proposal among
all proposals numbered less than n accepted by the acceptors in S.

We can therefore satisfy P2b by maintaining the invariance of P2c
.

To maintain the invariance of P2c
, a proposer that wants to issue a proposal
numbered n must learn the highest-numbered proposal with number
less than n, if any, that has been or will be accepted by each acceptor in
some majority of acceptors.

1. A proposer chooses a new proposal number n and sends a request to
each member of some set of acceptors, asking it to respond with:
(a) A promise never again to accept a proposal numbered less than
n, and
(b) The proposal with the highest number less than n that it has
accepted, if any.
I will call such a request a prepare request with number n.

2. If the proposer receives the requested responses from a majority of
the acceptors, then it can issue a proposal with number n and value
v, where v is the value of the highest-numbered proposal among the
responses, or is any value selected by the proposer if the responders
reported no proposals.

A proposer issues a proposal by sending, to some set of acceptors, a request
that the proposal be accepted. (This need not be the same set of acceptors
that responded to the initial requests.) Let’s call this an accept request

P1a
. An acceptor can accept a proposal numbered n iff it has not responded
to a prepare request having a number greater than n.


 The final
algorithm is obtained by making one small optimization.
Suppose an acceptor receives a prepare request numbered n, but it has
already responded to a prepare request numbered greater than n, thereby
promising not to accept any new proposal numbered n. There is then no
reason for the acceptor to respond to the new prepare request, since it will
not accept the proposal numbered n that the proposer wants to issue. We also have it ignore
a prepare request for a proposal it has already accepted.

Phase 1. (a) A proposer selects a proposal number n and sends a prepare
request with number n to a majority of acceptors.
(b) If an acceptor receives a prepare request with number n greater
than that of any prepare request to which it has already responded,
then it responds to the request with a promise not to accept any more
proposals numbered less than n and with the highest-numbered proposal
(if any) that it has accepted.

Phase 2. (a) If the proposer receives a response to its prepare requests
(numbered n) from a majority of acceptors, then it sends an accept
request to each of those acceptors for a proposal numbered n with a
value v, where v is the value of the highest-numbered proposal among
the responses, or is any value if the responses reported no proposals.
(b) If an acceptor receives an accept request for a proposal numbered
n, it accepts the proposal unless it has already responded to a prepare
request having a number greater than n.

It’s easy to construct a scenario in which two proposers each keep issuing
a sequence of proposals with increasing numbers, none of which are ever
chosen. Proposer p completes phase 1 for a proposal number n1. Another
proposer q then completes phase 1 for a proposal number n2 > n1. Proposer
p’s phase 2 accept requests for a proposal numbered n1 are ignored because
the acceptors have all promised not to accept any new proposal numbered
less than n2.

To guarantee progress, a distinguished proposer must be selected as the
only one to try issuing proposals.

The Paxos algorithm [5] assumes a network of processes. In its consensus
algorithm, each process plays the role of proposer, acceptor, and learner.
The algorithm chooses a leader, which plays the roles of the distinguished proposer and the distinguished learner. 


All that remains is to describe the mechanism for guaranteeing that no
two proposals are ever issued with the same number. Different proposers
choose their numbers from disjoint sets of numbers, so two different proposers
never issue a proposal with the same number. Each proposer remembers
(in stable storage) the highest-numbered proposal it has tried to issue,
and begins phase 1 with a higher proposal number than any it has already
used.


# Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing

## RDD Abstraction
Formally, an RDD is a read-only, partitioned collection
of records. RDDs can only be created through deterministic
operations on either (1) data in stable storage or (2)
other RDDs. We call these operations transformations to differentiate them from other operations on RDDs.