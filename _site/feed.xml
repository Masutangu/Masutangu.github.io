<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Masutangu</title>
    <description>也許我這一生　始終在追逐那顆九號球</description>
    <link>http://masutangu.com/</link>
    <atom:link href="http://masutangu.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 18 Sep 2018 22:49:23 +0800</pubDate>
    <lastBuildDate>Tue, 18 Sep 2018 22:49:23 +0800</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>Core dump 原理探究学习笔记（一）</title>
        <description>&lt;p&gt;本系列文章是读&lt;a href=&quot;https://blog.csdn.net/xuzhina/article/category/1322964&quot;&gt;《coredump问题原理探究》&lt;/a&gt;的读书笔记。&lt;/p&gt;

&lt;h2&gt;函数调用帧&lt;/h2&gt;

&lt;p&gt;先看一个例子&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;// test.cpp

int func(int c, char* s, int off) {
  int a = 0x12345678;
  int *p = &amp;amp;a;
  int res = c + *(s + off);
  return *p + res;
}

int main() {
  int b = 0x87654321;
  return b + func(0x100, &amp;quot;hello&amp;quot;, 3);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;g++ -o test test.cpp&lt;/code&gt; 编译后，gdb test 看下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) disassemble func
Dump of assembler code for function _Z4funciPci:
  0x00000000004005b0 &amp;lt;+0&amp;gt;: push %rbp
  0x00000000004005b1 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
  0x00000000004005b4 &amp;lt;+4&amp;gt;: mov %edi,-0x14(%rbp)
  0x00000000004005b7 &amp;lt;+7&amp;gt;: mov %rsi,-0x20(%rbp)
  0x00000000004005bb &amp;lt;+11&amp;gt;: mov %edx,-0x18(%rbp)
  0x00000000004005be &amp;lt;+14&amp;gt;: movl $0x12345678,-0x10(%rbp)
  0x00000000004005c5 &amp;lt;+21&amp;gt;: lea -0x10(%rbp),%rax
  0x00000000004005c9 &amp;lt;+25&amp;gt;: mov %rax,-0x8(%rbp)
  0x00000000004005cd &amp;lt;+29&amp;gt;: mov -0x18(%rbp),%eax
  0x00000000004005d0 &amp;lt;+32&amp;gt;: movslq %eax,%rdx
  0x00000000004005d3 &amp;lt;+35&amp;gt;: mov -0x20(%rbp),%rax
  0x00000000004005d7 &amp;lt;+39&amp;gt;: add %rdx,%rax
  0x00000000004005da &amp;lt;+42&amp;gt;: movzbl (%rax),%eax
  0x00000000004005dd &amp;lt;+45&amp;gt;: movsbl %al,%edx
  0x00000000004005e0 &amp;lt;+48&amp;gt;: mov -0x14(%rbp),%eax
  0x00000000004005e3 &amp;lt;+51&amp;gt;: add %edx,%eax
  0x00000000004005e5 &amp;lt;+53&amp;gt;: mov %eax,-0xc(%rbp)
  0x00000000004005e8 &amp;lt;+56&amp;gt;: mov -0x8(%rbp),%rax
  0x00000000004005ec &amp;lt;+60&amp;gt;: mov (%rax),%edx
  0x00000000004005ee &amp;lt;+62&amp;gt;: mov -0xc(%rbp),%eax
  0x00000000004005f1 &amp;lt;+65&amp;gt;: add %edx,%eax
  0x00000000004005f3 &amp;lt;+67&amp;gt;: pop %rbp
  0x00000000004005f4 &amp;lt;+68&amp;gt;: retq
End of assembler dump.
(gdb) disassemble main
Dump of assembler code for function main:
  0x00000000004005f5 &amp;lt;+0&amp;gt;: push %rbp
  0x00000000004005f6 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
  0x00000000004005f9 &amp;lt;+4&amp;gt;: sub $0x10,%rsp
  0x00000000004005fd &amp;lt;+8&amp;gt;: movl $0x87654321,-0x4(%rbp)
  0x0000000000400604 &amp;lt;+15&amp;gt;: mov $0x3,%edx
  0x0000000000400609 &amp;lt;+20&amp;gt;: mov $0x4006b0,%esi
  0x000000000040060e &amp;lt;+25&amp;gt;: mov $0x100,%edi
  0x0000000000400613 &amp;lt;+30&amp;gt;: callq 0x4005b0 &amp;lt;_Z4funciPci&amp;gt;
  0x0000000000400618 &amp;lt;+35&amp;gt;: mov -0x4(%rbp),%edx
  0x000000000040061b &amp;lt;+38&amp;gt;: add %edx,%eax
  0x000000000040061d &amp;lt;+40&amp;gt;: leaveq
  0x000000000040061e &amp;lt;+41&amp;gt;: retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到，每个函数调用的开始和结束都有如下指令：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;push %rbp     // 入栈 rbp 保存下旧的 rbp，此时 rsp -= 8 (64 位下)
mov %rsp,%rbp // 设置 rbp = rsp
...
pop %rbp      // 调用结束 出栈恢复 rbp，此时 rsp += 8 (64 位下)
retq          // 把 rbp 指向地址下一个单元的内容放到 rip，此时 rsp += 8 (64 位下)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;打个断点验证下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) tbreak *0x0000000000400613
Temporary breakpoint 1 at 0x400613
(gdb) r
Starting program: test3
Temporary breakpoint 1, 0x0000000000400613 in main ()
Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.tl2.2.x86_64 libgcc-4.8.5-4.el7.x86_64 libstdc++-4.8.5-4.el7.x86_64
(gdb) i r rsp rbp
rsp 0x7fffffffe460 0x7fffffffe460
rbp 0x7fffffffe470 0x7fffffffe470
(gdb) x /4x $rsp
0x7fffffffe460: 0xffffe550 0x00007fff 0x00000000 0x87654321
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;si 跟进去，发现 rsp 比之前小了 8 字节（从 &lt;strong&gt;0x7fffffffe460&lt;/strong&gt; 变成了 &lt;strong&gt;0x7fffffffe458&lt;/strong&gt;）。这是因为调用 call 指令会把返回地址（也就是 &lt;code&gt;callq 0x4005b0 &amp;lt;_Z4funciPci&amp;gt;&lt;/code&gt; 的下一条指令 &lt;code&gt;mov -0x4(%rbp),%edx&lt;/code&gt; 的地址）入栈。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) si
0x00000000004005b0 in func(int, char*, int) ()
(gdb) i r rsp rbp
rsp 0x7fffffffe458 0x7fffffffe458
rbp 0x7fffffffe470 0x7fffffffe470
(gdb) x /4x $rsp
0x7fffffffe458: 0x00400618 0x00000000 0xffffe550 0x00007fff
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;继续执行 &lt;code&gt;push %rbp&lt;/code&gt; 之后：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) ni
0x00000000004005b1 in func(int, char*, int) ()
(gdb) i r rsp rbp
rsp 0x7fffffffe450 0x7fffffffe450
rbp 0x7fffffffe470 0x7fffffffe470
(gdb) x /4x $rsp
0x7fffffffe450: 0xffffe470 0x00007fff 0x00400618 0x00000000
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看出 &lt;code&gt;push %rbp&lt;/code&gt; 之后 rsp 指向的地址存放了 rbp 的值：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x7fffffffe450: 0xffffe470 0x00007fff
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;继续执行 &lt;code&gt;mov %rsp,%rbp&lt;/code&gt;，此时 rsp 和 rbp 的值一致。rbp 指向上个函数帧的 rbp（本样例为 &lt;code&gt;0xffffe470&lt;/code&gt;），rbp 的下个单元指向返回地址（本样例为 &lt;code&gt;0x00400618&lt;/code&gt;） ，info symbol 该返回地址可以打印出函数名称（本样例为 &lt;code&gt;main + 35 in section .text of test3&lt;/code&gt;）。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) ni
0x00000000004005b4 in func(int, char*, int) ()
(gdb) i r rsp rbp
rsp 0x7fffffffe450 0x7fffffffe450
rbp 0x7fffffffe450 0x7fffffffe450
(gdb) x /4x $rbp
0x7fffffffe450: 0xffffe470 0x00007fff 0x00400618 0x00000000
(gdb) info symbol 0x00400618
main + 35 in section .text of test3
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看出函数调用时帧布局:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/coredump-note-1/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;rbp 地址大于等于 rsp&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rbp 下一个单元的内容存放返回地址，info symbol 会显示出函数名称&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rbp 所指向的单元是上一帧的 rbp，亦遵守上述两个规则&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;栈溢出调试&lt;/h2&gt;

&lt;p&gt;构造一个栈溢出的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;// test2.cpp 
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

int overflow(int level, char* str) {
  char buff[16];
  strcpy(buff, str);
  printf(&amp;quot;buffer:%s&amp;quot;, buff);
  return ++level;
}

int wrapper1(int level, char* str) {
  return overflow( ++level, str );
}

int wrapper2(int level, char* str) {
  return wrapper1(++level, str);
}

int wrapper3(int level, char* str) {
  return wrapper2(++level, str);
}

int main(int argc, char* argv[]) {
  if ( argc &amp;lt; 2 ) {
    return -1;
  }
  return wrapper3(0, argv[1]);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;执行 &lt;code&gt;./test2 WeAreHumanBeingsNothingCanNotStopUsWe&lt;/code&gt; core 掉。gdb coredump 文件：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) bt
#0 0x0000000000400680 in overflow(int, char*) ()
#1 0x6f7453746f4e6e61 in ?? ()
#2 0x0000006557735570 in ?? ()
#3 0x00000003e167f5b6 in ?? ()
#4 0x00007ffeccfd8200 in ?? ()
#5 0x00000000004006cb in wrapper2(int, char*) ()
Cannot access memory at address 0x43676e6968746f56
(gdb) i r rsp rbp
rsp 0x7ffeccfd81c8 0x7ffeccfd81c8
rbp 0x43676e6968746f4e 0x43676e6968746f4e
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;查看 rsp rbp 发现 rbp 小于 rsp，根据上节的总结，我们知道 rbp 的值是非法的。&lt;/p&gt;

&lt;p&gt;查看下 rsp 的内容，尝试找到正确的 rbp：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /32x $rsp
0x7ffeccfd81c8: 0x6f4e6e61 0x6f745374 0x57735570 0x00000065
0x7ffeccfd81d8: 0xe167f5b6 0x00000003 0xccfd8200 0x00007ffe
0x7ffeccfd81e8: 0x004006cb 0x00000000 0xccfd979d 0x00007ffe
0x7ffeccfd81f8: 0xe0a97890 0x00000002 0xccfd8220 0x00007ffe
0x7ffeccfd8208: 0x004006f1 0x00000000 0xccfd979d 0x00007ffe
0x7ffeccfd8218: 0x00000000 0x00000001 0xccfd8240 0x00007ffe
0x7ffeccfd8228: 0x00400727 0x00000000 0xccfd8328 0x00007ffe
0x7ffeccfd8238: 0x00000000 0x00000002 0x00000000 0x00000000
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;0x7ffeccfd8200, 0x00007ffeccfd979d, 0x00007ffeccfd8220, 0x00007ffeccfd8240, 0x00007ffeccfd8328 都大于 0x7ffeccfd81c8，info symbol 这三个地址的下个单元地址看看能否正常显示函数：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) info symbol 0x004006cb
wrapper2(int, char*) + 36 in section .text of test
(gdb) info symbol 0xe0a97890
No symbol matches 0xe0a97890.
(gdb) info symbol 0x004006f1
wrapper3(int, char*) + 36 in section .text of test
(gdb) info symbol 0x00400727
main + 52 in section .text of test
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到 0x7ffeccfd8200, 0x00007ffeccfd8220, 0x00007ffeccfd8240 都可以正常显示下个单元的地址，结论 1 和 2都满足，接下来看看第 3 个结论是否也满足：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /2x 0x7ffeccfd8200
0x7ffeccfd8200: 0xccfd8220 0x00007ffe
(gdb) x /2x 0x00007ffeccfd8220
0x7ffeccfd8220: 0xccfd8240 0x00007ffe
(gdb) x /2x 0x00007ffeccfd8240
0x7ffeccfd8240: 0x00000000 0x00000000
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;0x7ffeccfd8200 刚好指向 0x00007ffeccfd8220，0x00007ffeccfd8220 也刚好指向 0x00007ffeccfd8240。所以这三个都是合法的 rbp。这样我们已经恢复了真实的调用栈：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;wrapper2(int, char*) + 36 in section .text of test
wrapper3(int, char*) + 36 in section .text of test
main + 52 in section .text of test
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看看 wrapper2 的汇编：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) disassemble wrapper2
Dump of assembler code for function _Z8wrapper2iPc:
0x00000000004006a7 &amp;lt;+0&amp;gt;: push %rbp
0x00000000004006a8 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
0x00000000004006ab &amp;lt;+4&amp;gt;: sub $0x10,%rsp
0x00000000004006af &amp;lt;+8&amp;gt;: mov %edi,-0x4(%rbp)
0x00000000004006b2 &amp;lt;+11&amp;gt;: mov %rsi,-0x10(%rbp)
0x00000000004006b6 &amp;lt;+15&amp;gt;: addl $0x1,-0x4(%rbp)
0x00000000004006ba &amp;lt;+19&amp;gt;: mov -0x10(%rbp),%rdx
0x00000000004006be &amp;lt;+23&amp;gt;: mov -0x4(%rbp),%eax
0x00000000004006c1 &amp;lt;+26&amp;gt;: mov %rdx,%rsi
0x00000000004006c4 &amp;lt;+29&amp;gt;: mov %eax,%edi
0x00000000004006c6 &amp;lt;+31&amp;gt;: callq 0x400681 &amp;lt;_Z8wrapper1iPc&amp;gt;
0x00000000004006cb &amp;lt;+36&amp;gt;: leaveq
0x00000000004006cc &amp;lt;+37&amp;gt;: retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到这里返回地址为 &lt;code&gt;0x00000000004006cb &amp;lt;+36&amp;gt;: leaveq&lt;/code&gt;。
从前面&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x7ffeccfd81e8: 0x004006cb 0x00000000 0xccfd979d 0x00007ffe
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看到栈地址 0x7ffeccfd81e8 保存 wrapper1 的返回地址为 0x004006cb ，和 wrapper2 的汇编相符
，故继续看看 &lt;code&gt;callq 0x400681 &amp;lt;_Z8wrapper1iPc&amp;gt;&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;shell c++filt 查看下函数的原型：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) shell c++filt _Z8wrapper1iPc
wrapper1(int, char*)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看下 wrapper1 的汇编：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) disassemble wrapper1
Dump of assembler code for function _Z8wrapper1iPc:
0x0000000000400681 &amp;lt;+0&amp;gt;: push %rbp
0x0000000000400682 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
0x0000000000400685 &amp;lt;+4&amp;gt;: sub $0x10,%rsp
0x0000000000400689 &amp;lt;+8&amp;gt;: mov %edi,-0x4(%rbp)
0x000000000040068c &amp;lt;+11&amp;gt;: mov %rsi,-0x10(%rbp)
0x0000000000400690 &amp;lt;+15&amp;gt;: addl $0x1,-0x4(%rbp)
0x0000000000400694 &amp;lt;+19&amp;gt;: mov -0x10(%rbp),%rdx
0x0000000000400698 &amp;lt;+23&amp;gt;: mov -0x4(%rbp),%eax
0x000000000040069b &amp;lt;+26&amp;gt;: mov %rdx,%rsi
0x000000000040069e &amp;lt;+29&amp;gt;: mov %eax,%edi
0x00000000004006a0 &amp;lt;+31&amp;gt;: callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;
0x00000000004006a5 &amp;lt;+36&amp;gt;: leaveq
0x00000000004006a6 &amp;lt;+37&amp;gt;: retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;&lt;/code&gt; 之前都是 rbp rsp 操作，推测出执行 &lt;code&gt;callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;&lt;/code&gt; 时引起的栈溢出。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /16x $rsp
0x7ffeccfd81c8: 0x6f4e6e61 0x6f745374 0x57735570 0x00000065
0x7ffeccfd81d8: 0xe167f5b6 0x00000003 0xccfd8200 0x00007ffe
0x7ffeccfd81e8: 0x004006cb 0x00000000 0xccfd979d 0x00007ffe
0x7ffeccfd81f8: 0xe0a97890 0x00000002 0xccfd8220 0x00007ffe
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;0x7ffeccfd81c8 的内容本应该是 &lt;code&gt;0x004006a5 0x00000000&lt;/code&gt; （指向返回地址 &lt;code&gt;x00000000004006a5 &amp;lt;+36&amp;gt;: leaveq&lt;/code&gt;），但由于栈溢出被覆盖了。因此猜测是 _Z8overflowiPc 函数引起的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：为什么是 0x7ffeccfd81c8？前面知道 0x7ffeccfd81e8 保存调用 wrapper1 的返回地址。由上图可以看出，调用 wrapper1 时， 减去 8 个字节的 &lt;code&gt;push %rbp&lt;/code&gt;，减去 16 字节的局部变量（&lt;code&gt;sub $0x10,%rsp&lt;/code&gt;），再减去 8 个字节的 &lt;code&gt;callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;&lt;/code&gt;，即 0x7ffeccfd81e8 - 32 = 0x7ffeccfd81c8f 保存调用 _Z8overflowiPc 的返回地址0x00000000004006a5&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) shell c++filt _Z8overflowiPc
overflow(int, char*)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;因此定位到 &lt;code&gt;overflow&lt;/code&gt; 函数代码中用到 &lt;code&gt;strcpy&lt;/code&gt; 导致溢出。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 22:47:56 +0800</pubDate>
        <link>http://masutangu.com/2018/09/coredump-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/09/coredump-note-1/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Designing Data-Intensive Applications 读书笔记（二）</title>
        <description>&lt;h1&gt;Chapter 4. Encoding and Evolution&lt;/h1&gt;

&lt;p&gt;When a data format or schema changes, a corresponding change to application code often needs to happen. However, &lt;strong&gt;in a large application, code changes often cannot happen instantaneously&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;With server-side applications you may want to perform a rolling upgrade, deploying the new version to a few nodes at a time, checking whether the new version is running smoothly, and gradually working your way through all the nodes. This allows new versions to be deployed without service downtime, and thus encourages more frequent releases and better evolvability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With client-side applications you’re at the mercy of the user, who may not install the update for some time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This means that &lt;strong&gt;old and new versions of the code, and old and new data formats, may potentially all coexist in the system at the same time&lt;/strong&gt;. In order for the system to continue running smoothly, we need to maintain compatibility in both directions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Backward compatibility&lt;/strong&gt;: Newer code can read data that was written by older code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Forward compatibility&lt;/strong&gt;: Older code can read data that was written by newer code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this chapter we will look at several formats for encoding data, including JSON, XML, Protocol Buffers, Thrift, and Avro. In particular, we will look at &lt;strong&gt;how they handle schema changes and how they support systems where old and new data and code need to coexist&lt;/strong&gt;. We will then discuss how those formats are used &lt;strong&gt;for data storage and for communication&lt;/strong&gt;: in web services, Representational State Transfer (REST), and remote procedure calls (RPC), as well as message-passing systems such as actors and message queues.&lt;/p&gt;

&lt;h2&gt;Formats for Encoding Data&lt;/h2&gt;

&lt;p&gt;Programs usually work with data in (at least) two different representations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so on. These data structures are optimized for efficient access and manipulation by the CPU (typically using pointers).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you want to write data to a file or send it over the network, &lt;strong&gt;you have to encode it as some kind of self-contained sequence of bytes&lt;/strong&gt; (for example, a JSON document). Since a pointer wouldn’t make sense to any other process, this sequence-of-bytes representation looks quite different from the data structures that are normally used in memory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, we need some kind of translation between the two representations. The translation from the in-memory representation to a byte sequence is called &lt;strong&gt;encoding (also known as serialization or marshalling)&lt;/strong&gt;, and the reverse is called &lt;strong&gt;decoding (parsing, deserialization, unmarshalling)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Language-Specific Formats&lt;/h3&gt;

&lt;p&gt;Many programming languages come with built-in support for encoding in-memory objects into byte sequences. However, they also have a number of deep problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The encoding is often tied to a particular programming language&lt;/strong&gt;, and reading the data in another language is very difficult.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to restore data in the same object types, the decoding process needs to be able to instantiate arbitrary classes. &lt;strong&gt;This is frequently a source of security problems: if an attacker can get your application to decode an arbitrary byte sequence, they can instantiate arbitrary classes, which in turn often allows them to do terrible things such as remotely executing arbitrary code.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Versioning data is often an afterthought in these libraries&lt;/strong&gt;: as they are intended for quick and easy encoding of data, they often neglect the inconvenient problems of forward and backward compatibility.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Efficiency (CPU time taken to encode or decode, and the size of the encoded structure) is also often an afterthought. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;For these reasons it’s generally a bad idea to use your language’s built-in encoding for anything other than very transient purposes.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;JSON, XML, and Binary Variants&lt;/h3&gt;

&lt;p&gt;JSON, XML, and CSV are textual formats, and thus somewhat human-readable. Besides the superficial syntactic issues, they also have some subtle problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;There is a lot of ambiguity around the encoding of numbers.&lt;/strong&gt; This is a problem when dealing with large numbers; for example, integers greater than 253 cannot be exactly represented in an IEEE 754 double-precision floating-point number, so such numbers become inaccurate when parsed in a language that uses floating-point numbers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;JSON and XML have good support for Unicode character strings (i.e., human- readable text), but &lt;strong&gt;they don’t support binary strings&lt;/strong&gt; (sequences of bytes without a character encoding). Binary strings are a useful feature, so people get around this limitation by encoding the binary data as text using Base64. This works, but it’s somewhat hacky and increases the data size by 33%.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Thrift and Protocol Buffers&lt;/h3&gt;

&lt;p&gt;Both Thrift and Protocol Buffers require a schema for any data that is encoded.&lt;/p&gt;

&lt;h2&gt;Modes of Dataflow&lt;/h2&gt;

&lt;p&gt;In this chapter we will explore some of the most common ways how data flows between processes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Via databases &lt;/li&gt;
&lt;li&gt;Via service calls&lt;/li&gt;
&lt;li&gt;Via asynchronous message passing&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Dataflow Through Databases&lt;/h3&gt;

&lt;p&gt;Backward compatibility is clearly necessary here; otherwise your future self won’t be able to decode what you previously wrote.&lt;/p&gt;

&lt;p&gt;A value in the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. Thus, forward compatibility is also often required for databases.&lt;/p&gt;

&lt;p&gt;If you decode a database value into model objects in the application, and later reencode those model objects, &lt;strong&gt;the unknown field might be lost in that translation process&lt;/strong&gt;. Solving this is not a hard problem; you just need to be aware of it.&lt;/p&gt;

&lt;h3&gt;Dataflow Through Services: REST and RPC&lt;/h3&gt;

&lt;p&gt;There are two popular approaches to web services: &lt;strong&gt;REST and SOAP&lt;/strong&gt;. They are almost diametrically opposed in terms of philosophy.&lt;/p&gt;

&lt;p&gt;REST is not a protocol, but rather a design philosophy that builds upon the principles of HTTP. &lt;strong&gt;It emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication, and content type negotiation.&lt;/strong&gt; An API designed according to the principles of REST is called RESTful.&lt;/p&gt;

&lt;p&gt;By contrast, SOAP is an XML-based protocol for making network API requests. Although it is most commonly used over HTTP, it aims to be independent from HTTP and avoids using most HTTP features.&lt;/p&gt;

&lt;p&gt;The API of a SOAP web service is described using an XML-based language called the Web Services Description Language, or WSDL. WSDL enables code generation so that a client can access a remote service using local classes and method calls. This is useful in statically typed programming languages, but less so in dynamically typed ones.&lt;/p&gt;

&lt;p&gt;RESTful APIs tend to favor simpler approaches, typically involving less code generation and automated tooling. &lt;/p&gt;

&lt;h4&gt;Data encoding and evolution for RPC&lt;/h4&gt;

&lt;p&gt;The RPC model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process (&lt;strong&gt;this abstraction is called location transparency&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;It is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, you only need &lt;strong&gt;backward compatibility on requests, and forward compatibility on responses&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The backward and forward compatibility properties of an RPC scheme are inherited from whatever encoding it uses.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service compatibility is made harder by the fact that RPC is often used for communication across organizational boundaries&lt;/strong&gt;, so the provider of a service often has no control over its clients and cannot force them to upgrade. Thus, compatibility needs to be maintained for a long time, perhaps indefinitely. &lt;/p&gt;

&lt;h3&gt;Message-Passing Dataflow&lt;/h3&gt;

&lt;p&gt;Asynchronous message-passing systems are similar to RPC in that a client’s request (usually called a message) is delivered to another process with low latency. They are similar to databases in that the message is not sent via a direct network connection, but goes via an intermediary called a message broker (also called a message queue or message-oriented middleware), which stores the message temporarily.&lt;/p&gt;

&lt;p&gt;Using a message broker has several advantages compared to direct RPC:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It can act as a buffer if the recipient is unavailable or overloaded, and thus improve system reliability.&lt;/li&gt;
&lt;li&gt;It can automatically redeliver messages to a process that has crashed, and thus prevent messages from being lost.&lt;/li&gt;
&lt;li&gt;It avoids the sender needing to know the IP address and port number of the recipient (which is particularly useful in a cloud deployment where virtual machines often come and go).&lt;/li&gt;
&lt;li&gt;It allows one message to be sent to several recipients.&lt;/li&gt;
&lt;li&gt;It logically decouples the sender from the recipient (the sender just publishes messages and doesn’t care who consumes them).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, a difference compared to RPC is that &lt;strong&gt;message-passing communication is usually one-way: a sender normally doesn’t expect to receive a reply to its messages&lt;/strong&gt;. It is possible for a process to send a response, but this would usually be done on a separate channel. This communication pattern is &lt;strong&gt;asynchronous&lt;/strong&gt;: the sender doesn’t wait for the message to be delivered, but simply sends it and then forgets about it.&lt;/p&gt;

&lt;h4&gt;Distributed actor frameworks&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;The actor model is a programming model for concurrency in a single process.&lt;/strong&gt; Rather than dealing directly with threads (and the associated problems of race conditions, locking, and deadlock), logic is encapsulated in actors. &lt;strong&gt;Each actor typically represents one client or entity, it may have some local state (which is not shared with any other actor), and it communicates with other actors by sending and receiving asynchronous messages.&lt;/strong&gt; Since each actor processes only one message at a time, it doesn’t need to worry about threads, and each actor can be scheduled independently by the framework.&lt;/p&gt;

&lt;p&gt;In distributed actor frameworks, this programming model is used to scale an application across multiple nodes. &lt;strong&gt;The same message-passing mechanism is used, no matter whether the sender and recipient are on the same node or different nodes.&lt;/strong&gt; If they are on different nodes, the message is transparently encoded into a byte sequence, sent over the network, and decoded on the other side.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Many services need to support rolling upgrades, where a new version of a service is gradually deployed to a few nodes at a time, rather than deploying to all nodes simultaneously. Rolling upgrades allow new versions of a service to be released without downtime. &lt;strong&gt;These properties are hugely beneficial for evolvability, the ease of making changes to an application.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During rolling upgrades, or for various other reasons, we must assume that &lt;strong&gt;different nodes are running the different versions of our application’s code&lt;/strong&gt;. Thus, it is important that &lt;strong&gt;all data flowing around the system is encoded in a way that provides backward compatibility (new code can read old data) and forward compatibility (old code can read new data)&lt;/strong&gt;.&lt;/p&gt;

&lt;h1&gt;Chapter 5. Replication&lt;/h1&gt;

&lt;p&gt;There are various reasons why you might want to distribute a database across multiple machines:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your data volume, read load, or write load grows bigger than a single machine can handle, you can potentially spread the load across multiple machines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fault tolerance/high availability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your application needs to continue working even if one machine (or several machines, or the network, or an entire datacenter) goes down, you can use multiple machines to give you redundancy. When one fails, another one can take over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you have users around the world, you might want to have servers at various locations worldwide so that each user can be served from a datacenter that is geographically close to them. That avoids the users having to wait for network packets to travel halfway around the world.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Replication means keeping a copy of the same data on multiple machines that are connected via a network.&lt;/strong&gt; There are several reasons why you might want to replicate data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To keep data geographically close to your users&lt;/strong&gt; (and thus reduce latency)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To allow the system to continue working even if some of its parts have failed&lt;/strong&gt; (and thus increase availability)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To scale out the number of machines that can serve read queries&lt;/strong&gt; (and thus increase read throughput)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will discuss three popular algorithms for replicating changes between nodes: &lt;strong&gt;single-leader, multi-leader, and leaderless replication&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Leaders and Followers&lt;/h2&gt;

&lt;p&gt;Each node that stores a copy of the database is called a replica. Every write to the database needs to be processed by every replica. &lt;strong&gt;The most common solution for this is called leader-based replication&lt;/strong&gt; (also known as active/passive or master–slave replication):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;One of the replicas is designated the leader&lt;/strong&gt; (also known as master or primary). When clients want to write to the database, they must send their requests to the leader, which first writes the new data to its local storage.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The other replicas are known as followers (read replicas, slaves, secondaries, or hot standbys). Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of a replication log or change stream. &lt;strong&gt;Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader (the followers are read-only from the client’s point of view).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Synchronous Versus Asynchronous Replication&lt;/h3&gt;

&lt;p&gt;Often, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable.&lt;/p&gt;

&lt;h3&gt;Setting Up New Followers&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Take a consistent snapshot of the leader’s database at some point in time—if possible, without taking a lock on the entire database. Most databases have this feature, as it is also required for backups.&lt;/li&gt;
&lt;li&gt;Copy the snapshot to the new follower node.&lt;/li&gt;
&lt;li&gt;The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in the leader’s replication log.&lt;/li&gt;
&lt;li&gt;When the follower has processed the backlog of data changes since the snapshot, we say it has caught up. It can now continue to process data changes from the leader as they happen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Handling Node Outages&lt;/h3&gt;

&lt;p&gt;How do you achieve high availability with leader-based replication?&lt;/p&gt;

&lt;h4&gt;Follower failure: Catch-up recovery&lt;/h4&gt;

&lt;p&gt;The follower can recover quite easily: from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. &lt;/p&gt;

&lt;h4&gt;Leader failure: Failover&lt;/h4&gt;

&lt;p&gt;Handling a failure of the leader is trickier: &lt;strong&gt;one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader&lt;/strong&gt;. This process is called failover.&lt;/p&gt;

&lt;p&gt;Failover can happen manually or automatically. An automatic failover process usually consists of the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Determining that the leader has failed.&lt;/li&gt;
&lt;li&gt;Choosing a new leader.&lt;/li&gt;
&lt;li&gt;Reconfiguring the system to use the new leader.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Implementation of Replication Logs&lt;/h3&gt;

&lt;h4&gt;Statement-based replication&lt;/h4&gt;

&lt;p&gt;In the simplest case, the leader logs every write request (statement) that it executes and sends that statement log to its followers.&lt;/p&gt;

&lt;p&gt;There are various ways in which this approach to replication can break down:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Any statement that calls a nondeterministic function&lt;/strong&gt;, such as NOW() to get the current date and time or RAND() to get a random number, is likely to generate a different value on each replica.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;If statements use an autoincrementing column&lt;/strong&gt;, or if they depend on the existing data in the database (e.g., UPDATE ... WHERE &lt;some condition&gt;), they must be executed in exactly the same order on each replica, or else they may have a different effect. This can be limiting when there are multiple concurrently executing transactions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statements that have side effects&lt;/strong&gt; (e.g., triggers, stored procedures, user-defined functions) may result in different side effects occurring on each replica, unless the side effects are absolutely deterministic.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Statement-based replication was used in MySQL before version 5.1.&lt;/p&gt;

&lt;h4&gt;Write-ahead log (WAL) shipping&lt;/h4&gt;

&lt;p&gt;In Chapter 3 we discussed how storage engines represent data on disk, and we found that usually every write is appended to a log. &lt;strong&gt;In either case, the log is an append-only sequence of bytes containing all writes to the database.&lt;/strong&gt; We can use the exact same log to build a replica on another node: besides writing the log to disk, the leader also sends it across the network to its followers. When the follower processes this log, it builds a copy of the exact same data structures as found on the leader.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The main disadvantage is that the log describes the data on a very low level&lt;/strong&gt;: a WAL contains details of which bytes were changed in which disk blocks. &lt;strong&gt;This makes replication closely coupled to the storage engine.&lt;/strong&gt; If the database changes its storage format from one version to another, it is typically not possible to run different versions of the database software on the leader and the followers.&lt;/p&gt;

&lt;h4&gt;Logical (row-based) log replication&lt;/h4&gt;

&lt;p&gt;An alternative is to use different log formats for replication and for the storage engine, which allows the replication log to be &lt;strong&gt;decoupled from the storage engine internals&lt;/strong&gt;. This kind of replication log is called a logical log, to distinguish it from the storage engine’s (physical) data representation.&lt;/p&gt;

&lt;p&gt;A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For an inserted row, the log contains the new values of all columns.&lt;/li&gt;
&lt;li&gt;For a deleted row, the log contains enough information to uniquely identify the row that was deleted. Typically this would be the primary key, but if there is no primary key on the table, the old values of all columns need to be logged.&lt;/li&gt;
&lt;li&gt;For an updated row, the log contains enough information to uniquely identify the updated row, and the new values of all columns (or at least the new values of all columns that changed).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A transaction that modifies several rows generates several such log records, followed by a record indicating that the transaction was committed. MySQL’s binlog (when configured to use row-based replication) uses this approach.&lt;/p&gt;

&lt;h4&gt;Trigger-based replication&lt;/h4&gt;

&lt;p&gt;The replication approaches described so far are implemented by the database system, without involving any application code. &lt;strong&gt;If you want to only replicate a subset of the data, or want to replicate from one kind of database to another, or if you need conflict resolution logic, then you may need to move replication up to the application layer.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;An alternative is to use features that are available in many relational databases: &lt;strong&gt;triggers and stored procedures&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A trigger lets you register custom application code that is automatically executed when a data change (write transaction) occurs in a database system. &lt;/p&gt;

&lt;h2&gt;Problems with Replication Lag&lt;/h2&gt;

&lt;p&gt;Being able to tolerate node failures is just one reason for wanting replication. Other reasons are &lt;strong&gt;scalability&lt;/strong&gt; (processing more requests than a single machine can handle) and &lt;strong&gt;latency&lt;/strong&gt; (placing replicas geo‐graphically closer to users).&lt;/p&gt;

&lt;p&gt;Leader-based replication requires all writes to go through a single node, but read-only queries can go to any replica. &lt;/p&gt;

&lt;p&gt;However, this approach only realistically works with asynchronous replication—if you tried to synchronously replicate to all followers, a single node failure or network outage would make the entire system unavailable for writing.&lt;/p&gt;

&lt;p&gt;Unfortunately, &lt;strong&gt;if an application reads from an asynchronous follower, it may see outdated information if the follower has fallen behind&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This inconsistency is just a temporary state—if you stop writing to the database and wait a while, the followers will eventually catch up and become consistent with the leader. For that reason, this effect is known as &lt;strong&gt;eventual consistency&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Reading Your Own Writes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Read-after-write consistency&lt;/strong&gt;, also known as &lt;strong&gt;read-your-writes consistency&lt;/strong&gt;, is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves.&lt;/p&gt;

&lt;p&gt;There are various possible techniques to implement read-after-write consistency in a system with leader-based replication:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;When reading something that the user may have modified, read it from the leader&lt;/strong&gt;; otherwise, read it from a follower. This requires that you have some way of knowing whether something might have been modified, without actually querying it. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If most things in the application are potentially editable by the user, that approach won’t be effective, as most things would have to be read from the leader.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The client can remember the timestamp of its most recent write—then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp.&lt;/strong&gt; If a replica is not sufficiently up to date, either the read can be handled by another replica or the query can wait until the replica has caught up. The timestamp could be a logical timestamp (something that indicates ordering of writes, such as the log sequence number) or the actual system clock.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If your replicas are distributed across multiple datacenters (for geographical proximity to users or for availability), there is additional complexity. Any request that needs to be served by the leader must be routed to the datacenter that contains the leader.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another complication arises when the same user is accessing your service from multiple devices, in this case, there are some additional issues to consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Approaches that require remembering the timestamp of the user’s last update become more difficult, because the code running on one device doesn’t know what updates have happened on the other device. This metadata will need to be centralized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If your replicas are distributed across different datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. If your approach requires reading from the leader, you may first need to route requests from all of a user’s devices to the same datacenter.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Monotonic Reads&lt;/h3&gt;

&lt;p&gt;Our second example of an anomaly that can occur when reading from asynchronous followers is that it’s possible for a user to see things &lt;strong&gt;moving backward in time&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monotonic reads&lt;/strong&gt; is a guarantee that this kind of anomaly does not happen. &lt;strong&gt;It’s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency.&lt;/strong&gt; When you read data, you may see an old value; &lt;strong&gt;monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One way of achieving monotonic reads is to make sure that each user always makes their reads from the same replica.&lt;/strong&gt; However, if that replica fails, the user’s queries will need to be rerouted to another replica.&lt;/p&gt;

&lt;h3&gt;Consistent Prefix Reads&lt;/h3&gt;

&lt;p&gt;Our third example of replication lag anomalies concerns violation of causality. Preventing this kind of anomaly requires another type of guarantee: &lt;strong&gt;consistent prefix reads&lt;/strong&gt;. &lt;strong&gt;This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a particular problem in partitioned (sharded) databases. If the database always applies writes in the same order, reads always see a consistent prefix, so this anomaly cannot happen. However, &lt;strong&gt;in many distributed databases, different partitions operate independently, so there is no global ordering of writes&lt;/strong&gt;: when a user reads from the database, they may see some parts of the database in an older state and some in a newer state.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One solution is to make sure that any writes that are causally related to each other are written to the same partition&lt;/strong&gt;—but in some applications that cannot be done efficiently. There are also algorithms that explicitly keep track of causal dependencies.&lt;/p&gt;

&lt;h3&gt;Solutions for Replication Lag&lt;/h3&gt;

&lt;p&gt;As discussed earlier, there are ways in which an application can provide a stronger guarantee than the underlying database—for example, by performing certain kinds of reads on the leader. However, dealing with these issues in application code is complex and easy to get wrong.&lt;/p&gt;

&lt;p&gt;It would be better if application developers didn’t have to worry about subtle replication issues and could just trust their databases to “do the right thing.” &lt;strong&gt;This is why transactions exist: they are a way for a database to provide stronger guarantees so that the application can be simpler.&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Multi-Leader Replication&lt;/h2&gt;

&lt;p&gt;Leader-based replication has one major downside: there is only one leader, and all writes must go through it. If you can’t connect to the leader for any reason, you can’t write to the database. &lt;strong&gt;A natural extension of the leader-based replication model is to allow more than one node to accept writes. We call this a multi-leader configuration.&lt;/strong&gt; In this setup, each leader simultaneously acts as a follower to the other leaders.&lt;/p&gt;

&lt;h3&gt;Use Cases for Multi-Leader Replication&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;It rarely makes sense to use a multi-leader setup within a single datacenter, because the benefits rarely outweigh the added complexity.&lt;/strong&gt; However, there are some situations in which this configuration is reasonable.&lt;/p&gt;

&lt;h4&gt;Multi-datacenter operation&lt;/h4&gt;

&lt;p&gt;Imagine you have a database with replicas in several different datacenters (perhaps so that you can tolerate failure of an entire datacenter, or perhaps in order to be closer to your users). With a normal leader-based replication setup, the leader has to be in one of the datacenters, and all writes must go through that datacenter.&lt;/p&gt;

&lt;p&gt;In a multi-leader configuration, you can have a leader in each datacenter. &lt;strong&gt;Within each datacenter, regular leader–follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although multi-leader replication has advantages, it also has a big downside: &lt;strong&gt;the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As multi-leader replication is a somewhat retrofitted feature in many databases, there are often subtle configuration pitfalls and surprising interactions with other database features. For example, autoincrementing keys, triggers, and integrity constraints can be problematic.&lt;/p&gt;

&lt;h4&gt;Clients with offline operation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Another situation in which multi-leader replication is appropriate is if you have an application that needs to continue to work while it is disconnected from the internet.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example, consider the calendar apps on your mobile phone, your laptop, and other devices. If you make any changes while you are offline, they need to be synced with a server and your other devices when the device is next online.&lt;/p&gt;

&lt;p&gt;In this case, &lt;strong&gt;every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas of your calendar on all of your devices&lt;/strong&gt;. The replication lag may be hours or even days, depending on when you have internet access available.&lt;/p&gt;

&lt;h4&gt;Collaborative editing&lt;/h4&gt;

&lt;p&gt;Real-time collaborative editing applications allow several people to edit a document simultaneously. When one user edits a document, the changes are instantly applied to their local replica (the state of the document in their web browser or client application) and asynchronously replicated to the server and any other users who are editing the same document.&lt;/p&gt;

&lt;p&gt;If you want to guarantee that there will be no editing conflicts, the application must obtain a lock on the document before a user can edit it. This collaboration model is equivalent to single-leader
replication with transactions on the leader.&lt;/p&gt;

&lt;h3&gt;Handling Write Conflicts&lt;/h3&gt;

&lt;p&gt;The biggest problem with multi-leader replication is that write conflicts can occur, which means that conflict resolution is required.&lt;/p&gt;

&lt;h4&gt;Synchronous versus asynchronous conflict detection&lt;/h4&gt;

&lt;p&gt;In principle, you could make the conflict detection synchronous—i.e., wait for the write to be replicated to all replicas before telling the user that the write was successful. However, by doing so, you would lose the main advantage of multi-leader replication: allowing each replica to accept writes independently. If you want synchronous conflict detection, you might as well just use single-leader replication.&lt;/p&gt;

&lt;h4&gt;Conflict avoidance&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;The simplest strategy for dealing with conflicts is to avoid them: if the application can ensure that all writes for a particular record go through the same leader&lt;/strong&gt;, then conflicts cannot occur. Since many implementations of multi-leader replication handle conflicts quite poorly, &lt;strong&gt;avoiding conflicts is a frequently recommended approach&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;Converging toward a consistent state&lt;/h4&gt;

&lt;p&gt;There are various ways of achieving convergent conflict resolution:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Give each write a unique ID&lt;/strong&gt; (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner, and throw away the other writes. If a timestamp is used, this technique is known as last write wins (LWW). Although this approach is popular, it is dangerously prone to data loss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Give each replica a unique ID&lt;/strong&gt;, and let writes that originated at a higher-numbered replica always take precedence over writes that originated at a lower-numbered replica. This approach also implies data loss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Somehow merge the values together&lt;/strong&gt;—e.g., order them alphabetically and then concatenate them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Record the conflict in an explicit data structure&lt;/strong&gt; that preserves all information, and write application code that resolves the conflict at some later time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Custom conflict resolution logic&lt;/h4&gt;

&lt;p&gt;As the most appropriate way of resolving a conflict may depend on the application, most multi-leader replication tools let you write conflict resolution logic using application code. That code may be executed on write or on read:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On write&lt;/p&gt;

&lt;p&gt;As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On read&lt;/p&gt;

&lt;p&gt;When a conflict is detected, all the conflicting writes are stored. The next time the data is read, these multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict, and write the result back to the database. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There has been some interesting research into automatically resolving conflicts caused by concurrent data modifications. A few lines of research are worth mentioning:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Conflict-free replicated datatypes (CRDTs) are a family of data structures for sets, maps, ordered lists, counters, etc. that can be concurrently edited by multiple users, and which automatically resolve conflicts in sensible ways.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mergeable persistent data structures track history explicitly, similarly to the Git version control system, and use a three-way merge function (whereas CRDTs use two-way merges).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Operational transformation is the conflict resolution algorithm behind collaborative editing applications such as Etherpad and Google Docs. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute a text document.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Multi-Leader Replication Topologies&lt;/h2&gt;

&lt;p&gt;A replication topology describes the communication paths along which writes are propagated from one node to another. If you have two leaders, there is only one plausible topology: leader 1 must send all of its writes to leader 2, and vice versa. With more than two leaders, various different topologies are possible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The most general topology is all-to-all (Figure [c]), in which every leader sends its writes to every other leader.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A problem with circular and star topologies is that if just one node fails, it can interrupt the flow of replication messages between other nodes&lt;/strong&gt;, causing them to be unable to communicate until the node is fixed. The fault tolerance of a more densely connected topology (such as all-to-all) is better because it allows messages to travel along different paths, avoiding a single point of failure.&lt;/p&gt;

&lt;p&gt;On the other hand, all-to-all topologies can have issues too. In particular, &lt;strong&gt;some network links may be faster than others (e.g., due to network congestion), with the result that some replication messages may “overtake” others.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;To order these events correctly, a technique called version vectors can be used, which we will discuss later in this chapter. However, conflict detection techniques are poorly implemented in many multi-leader replication systems. &lt;/p&gt;

&lt;h2&gt;Leaderless Replication&lt;/h2&gt;

&lt;p&gt;In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes. &lt;/p&gt;

&lt;h3&gt;Writing to the Database When a Node Is Down&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;When a client reads from the database, it doesn’t just send its request to one replica: read requests are also sent to several nodes in parallel. The client may get different responses from different nodes; i.e., the up-to-date value from one node and a stale value from another. &lt;strong&gt;Version numbers are used to determine which value is newer.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Read repair and anti-entropy&lt;/h4&gt;

&lt;p&gt;Two mechanisms are often used in Dynamo-style datastores:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Read repair&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When a client makes a read from several nodes in parallel, it can detect any stale responses. The client sees that replica has a stale value and writes the newer value back to that replica. This approach works well for values that are frequently read.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Anti-entropy process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In addition, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Quorums for reading and writing&lt;/h4&gt;

&lt;p&gt;If there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. &lt;strong&gt;As long as w + r &amp;gt; n, we expect to get an up-to-date value when reading&lt;/strong&gt;, because at least one of the r nodes we’re reading from must be up to date. Reads and writes that obey these r and w values are called &lt;strong&gt;quorum reads and writes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A workload with few writes and many reads may benefit from setting w = n and r = 1. This makes reads faster, but has the disadvantage that just one failed node causes all database writes to fail.&lt;/p&gt;

&lt;h3&gt;Limitations of Quorum Consistency&lt;/h3&gt;

&lt;p&gt;However, even with w + r &amp;gt; n, there are likely to be edge cases where stale values are returned. These depend on the implementation, but possible scenarios include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If a sloppy quorum is used, the w writes may end up on different nodes than the r reads, so there is no longer a guaranteed overlap between the r nodes and the w nodes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If two writes occur concurrently, it is not clear which one happened first. In this case, the only safe solution is to merge the concurrent writes. If a winner is picked based on a timestamp (last write wins), writes can be lost due to clock skew.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a write happens concurrently with a read, the write may be reflected on only some of the replicas. In this case, it’s undetermined whether the read returns the old or the new value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a node carrying a new value fails, and its data is restored from a replica carry‐ ing an old value, the number of replicas storing the new value may fall below w, breaking the quorum condition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even if everything is working correctly, there are edge cases in which you can get unlucky with the timing. See “Linearizability and quorums”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Monitoring staleness&lt;/h4&gt;

&lt;p&gt;For leader-based replication, the database typically exposes metrics for the replication lag, which you can feed into a monitoring system. &lt;strong&gt;By subtracting a follower’s current position from the leader’s current position, you can measure the amount of replication lag.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, in systems with leaderless replication, there is no fixed order in which writes are applied, which makes monitoring more difficult. &lt;/p&gt;

&lt;h3&gt;Detecting Concurrent Writes&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h4&gt;Last write wins (discarding concurrent writes)&lt;/h4&gt;

&lt;p&gt;We say the writes are concurrent, so their order is undefined. Even though the writes don’t have a natural ordering, we can force an arbitrary order on them. For example, we can attach a timestamp to each write, pick the biggest timestamp as the most “recent,” and discard any writes with an earlier timestamp. &lt;strong&gt;This conflict resolution algorithm, called last write wins (LWW).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LWW achieves the goal of eventual convergence, but at the cost of durability: &lt;strong&gt;if there are several concurrent writes to the same key, even if they were all reported as successful to the client (because they were written to w replicas), only one of the writes will survive and the others will be silently discarded&lt;/strong&gt;. Moreover, LWW may even drop writes that are not concurrent, as we shall discuss in “Timestamps for ordering events”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If losing data is not acceptable, LWW is a poor choice for conflict resolution.&lt;/strong&gt; The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key.&lt;/p&gt;

&lt;h4&gt;The “happens-before” relationship and concurrency&lt;/h4&gt;

&lt;p&gt;An operation A happens before another operation B if B knows about A, or depends on A, or builds upon A in some way. In fact, we can simply say that two operations are concurrent if neither happens before the other.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If one operation happened before another, the later operation should overwrite the earlier operation, but if the operations are concurrent, we have a conflict that needs to be resolved.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For defining concurrency, exact time doesn’t matter: we simply call two operations concurrent if they are both unaware of each other, regardless of the physical time at which they occurred. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4&gt;Capturing the happens-before relationship&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-5.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When a client reads a key, the server returns all values that have not been over‐ written, as well as the latest version number. A client must read a key before writing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. (The response from a write request can be like a read, returning all current values, which allows us to chain several writes like in the shopping cart example.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the server receives a write with a particular version number, &lt;strong&gt;it can overwrite all values with that version number or below&lt;/strong&gt; (since it knows that they have been merged into the new value), &lt;strong&gt;but it must keep all values with a higher version number&lt;/strong&gt; (because those values are concurrent with the incoming write).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;When a write includes the version number from a prior read, that tells us which previous state the write is based on.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Version vectors&lt;/h4&gt;

&lt;p&gt;How does the algorithm change when there are multiple replicas, but no leader?&lt;/p&gt;

&lt;p&gt;Previous algorithm uses a single version number to capture dependencies between operations, but that is not sufficient when there are multiple replicas accepting writes concurrently. &lt;strong&gt;Instead, we need to use a version number per replica as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas.&lt;/strong&gt; This information indicates which values to overwrite and which values to keep as siblings.&lt;/p&gt;

&lt;p&gt;The collection of version numbers from all the replicas is called a &lt;strong&gt;version vector&lt;/strong&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 12 Sep 2018 22:03:19 +0800</pubDate>
        <link>http://masutangu.com/2018/09/data-intensive-note-2/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/09/data-intensive-note-2/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Designing Data-Intensive Applications 读书笔记（一）</title>
        <description>&lt;h1&gt;Chapter 1. Reliable, Scalable, and Maintainable Applications&lt;/h1&gt;

&lt;h2&gt;Thinking About Data Systems&lt;/h2&gt;

&lt;p&gt;Increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and storage needs. Instead, &lt;strong&gt;the work is broken down into tasks that can be performed efficiently on a single tool, and those different tools are stitched together using application code.（在应用层代码整合多种不同的工具来实现需求）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In this book, we focus on three concerns that are important in most software systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reliability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Maintainability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Over time, many different people will work on the system (engineering and operations, both maintaining current behavior and adapting the system to new use cases), and they should all be able to work on it productively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Reliability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Continuing to work correctly, even when things go wrong.&lt;/strong&gt; A fault is not the same as a failure. A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. &lt;strong&gt;It is impossible to reduce the probability of a fault to zero; therefore it is usually best to design fault-tolerance mechanisms that prevent faults from causing failures.（实现 fault－tolerance 以避免 faults 引发 failures）&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Scalability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Scalability is the term we use to describe a system’s ability to cope with increased load.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Describing Load&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Load can be described with a few numbers which we call load parameters.&lt;/strong&gt; The best choice of parameters depends on the architecture of your system: it may be requests per second to a web server, the ratio of reads to writes in a database, the number of simultaneously active users in a chat room, the hit rate on a cache, or something else.&lt;/p&gt;

&lt;h3&gt;Approaches for Coping with Load&lt;/h3&gt;

&lt;p&gt;People often talk of a dichotomy between &lt;strong&gt;scaling up (vertical scaling, moving to a more powerful machine)&lt;/strong&gt; and &lt;strong&gt;scaling out (horizontal scaling, distributing the load across multiple smaller machines)&lt;/strong&gt;. Distributing load across multiple machines is also known as a &lt;strong&gt;shared-nothing architecture&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An architecture that scales well for a particular application is built around &lt;strong&gt;assumptions of which operations will be common and which will be rare—the load parameters（扩展性良好的系统是基于 load parameter 来构建的）&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Maintainability&lt;/h2&gt;

&lt;h3&gt;Operability: Making Life Easy for Operations&lt;/h3&gt;

&lt;p&gt;Make it easy for operations teams to keep the system running smoothly.&lt;/p&gt;

&lt;h3&gt;Simplicity:Managing Complexity&lt;/h3&gt;

&lt;p&gt;Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system. (Note this is not the same as simplicity of the user interface.) &lt;strong&gt;One of the best tools we have for removing accidental complexity is abstraction.&lt;/strong&gt; A good abstraction can hide a great deal of implementation detail behind a clean, simple-to-understand façade. &lt;strong&gt;SQL is an abstraction that hides complex on-disk and in-memory data structures.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Evolvability: Making Change Easy&lt;/h3&gt;

&lt;p&gt;Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity.&lt;/p&gt;

&lt;h1&gt;Chapter 2. Data Models and Query Languages&lt;/h1&gt;

&lt;p&gt;Data models are perhaps the most important part of developing software, because they have such a profound effect: not only on how the software is written, &lt;strong&gt;but also on how we think about the problem that we are solving&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Most applications are built by layering one data model on top of another. For each layer, &lt;strong&gt;the key question is: how is it represented in terms of the next-lower layer?&lt;/strong&gt; In a complex application there may be more intermediary levels, such as APIs built upon APIs, but the basic idea is still the same: &lt;strong&gt;each layer hides the complexity of the layers below it by providing a clean data model&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Relational Model Versus Document Model&lt;/h2&gt;

&lt;p&gt;The best-known data model today is probably that of SQL, data is organized into relations (called tables in SQL), where each relation is an unordered collection of tuples (rows in SQL).&lt;/p&gt;

&lt;h3&gt;The Birth of NoSQL&lt;/h3&gt;

&lt;p&gt;There are several driving forces behind the adoption of NoSQL databases, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A need for greater scalability than relational databases can easily achieve, including very large datasets or very high write throughput&lt;/li&gt;
&lt;li&gt;A widespread preference for free and open source software over commercial database products&lt;/li&gt;
&lt;li&gt;Specialized query operations that are not well supported by the relational model&lt;/li&gt;
&lt;li&gt;Frustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Relational Versus Document Databases Today&lt;/h3&gt;

&lt;p&gt;The main arguments in favor of the document data model are &lt;strong&gt;schema flexibility, better performance due to locality, and that for some applications it is closer to the data structures used by the application&lt;/strong&gt;.（&lt;em&gt;不需要和关系型数据库一样使用 ORM 做转换：If data is stored in relational tables, an awkward translation layer is required between the objects in the application code and the database model of tables, rows, and columns. The disconnect between the models is sometimes called an&lt;/em&gt; &lt;strong&gt;impedance mismatch&lt;/strong&gt;）The relational model counters by providing better support for &lt;strong&gt;joins, and many-to-one and many-to-many relationships&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;Which data model leads to simpler application code?&lt;/h4&gt;

&lt;p&gt;If the data in your application has a document-like structure, then it’s probably a good idea to use a document model. &lt;strong&gt;However, if your application does use many-to-many relationships, the document model becomes less appealing. It’s possible to reduce the need for joins by denormalizing, but then the application code needs to do additional work to keep the denormalized data consistent.&lt;/strong&gt; Joins can be emulated in application code by making multiple requests to the database, but that also moves complexity into the application and is usually slower than a join performed by specialized code inside the database. &lt;strong&gt;In such cases, using a document model can lead to significantly more complex application
code and worse performance.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Schema flexibility in the document model&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Document databases are sometimes called schemaless, there is an implicit schema, but it is not enforced by the database.&lt;/strong&gt; A more accurate term is &lt;strong&gt;schema-on-read&lt;/strong&gt; (the structure of the data is implicit, and only interpreted when the data is read), in contrast with &lt;strong&gt;schema-on-write&lt;/strong&gt; (the traditional approach of relational databases, where the schema is explicit and the database ensures all written data conforms to it).&lt;/p&gt;

&lt;h4&gt;Data locality for queries&lt;/h4&gt;

&lt;p&gt;A document is usually stored as a single continuous string, encoded as JSON, XML, or a binary variant thereof. If your application often needs to access the entire document, there is a performance advantage to this &lt;strong&gt;storage locality&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The locality advantage only applies if you need large parts of the document at the same time. On updates to a document, the entire document usually needs to be rewritten—only modifications that don’t change the encoded size of a document can easily be performed in place. For these reasons, it is generally recommended that you keep documents fairly small and &lt;strong&gt;avoid writes that increase the size of a document&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Query Languages for Data&lt;/h2&gt;

&lt;p&gt;When the relational model was introduced, it included a new way of querying data: &lt;strong&gt;SQL is a declarative query language, whereas IMS and CODASYL queried the database using imperative code&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In a declarative query language, like SQL or relational algebra, you just specify the pattern of the data you want, but not how to achieve that goal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Declarative languages often lend themselves to parallel execution. Imperative code is very hard to parallelize across multiple cores and multiple machines&lt;/strong&gt;, because it specifies instructions that must be performed in a particular order.&lt;/p&gt;

&lt;h2&gt;Graph-Like Data Models&lt;/h2&gt;

&lt;p&gt;The relational model can handle simple cases of many-to-many relationships, but as the connections within your data become more complex, it becomes more natural to start modeling your data as a graph.&lt;/p&gt;

&lt;h3&gt;Property Graphs&lt;/h3&gt;

&lt;p&gt;In the property graph model, each vertex consists of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A unique identifier&lt;/li&gt;
&lt;li&gt;A set of outgoing edges&lt;/li&gt;
&lt;li&gt;A set of incoming edges&lt;/li&gt;
&lt;li&gt;A collection of properties (key-value pairs)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each edge consists of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A unique identifier&lt;/li&gt;
&lt;li&gt;The vertex at which the edge starts (the tail vertex)&lt;/li&gt;
&lt;li&gt;The vertex at which the edge ends (the head vertex)&lt;/li&gt;
&lt;li&gt;A label to describe the kind of relationship between the two vertices&lt;/li&gt;
&lt;li&gt;A collection of properties (key-value pairs)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Graphs are good for evolvability: as you add features to your application, a graph can easily be extended to accommodate changes in your application’s data structures.&lt;/strong&gt;&lt;/p&gt;

&lt;h1&gt;Chapter 3. Storage and Retrieval&lt;/h1&gt;

&lt;p&gt;We will examine two families of storage engines: &lt;strong&gt;log-structured storage engines, and page-oriented storage engines such as B-trees&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Data Structures That Power Your Database&lt;/h2&gt;

&lt;p&gt;In order to efficiently find the value for a particular key in the database, we need a different data structure: an index. &lt;/p&gt;

&lt;h3&gt;Hash Indexes&lt;/h3&gt;

&lt;p&gt;Let’s say our data storage consists only of appending to a file. Then the simplest possible indexing strategy is this: &lt;strong&gt;keep an in-memory hash map where every key is mapped to a byte offset in the data file&lt;/strong&gt;—the location at which the value can be found. This is essentially what Bitcask (the default storage engine in Riak) does&lt;/p&gt;

&lt;p&gt;A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. For example, the key might be the URL of a cat video, and the value might be the number of times it has been played. &lt;strong&gt;In this kind of workload, there are a lot of writes, but there are not too many distinct keys—you have a large number of writes per key, but it’s feasible to keep all keys in memory.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;How do we avoid eventually running out of disk space? &lt;strong&gt;A good solution is to break the log into segments of a certain size by closing a segment file when it reaches a certain size, and making subsequent writes to a new segment file.&lt;/strong&gt; We can then perform compaction on these segments. &lt;strong&gt;Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Each segment now has its own in-memory hash table, mapping keys to file offsets.&lt;/strong&gt; In order to find the value for a key, we first check the most recent segment’s hash map; if the key is not present we check the second-most-recent segment, and so on.&lt;/p&gt;

&lt;p&gt;An append-only design turns out to be good for several reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Concurrency and crash recovery are much simpler if segment files are append-only or immutable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Merging old segments avoids the problem of data files getting fragmented over time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the hash table index also has limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The hash table must fit in memory, so if you have a very large number of keys, you’re out of luck.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Range queries are not efficient. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;SSTables and LSM-Trees&lt;/h3&gt;

&lt;p&gt;Now we can make a simple change to the format of our segment files: we require that the sequence of key-value pairs is sorted by key. &lt;/p&gt;

&lt;p&gt;We call this format &lt;strong&gt;Sorted String Table, or SSTable&lt;/strong&gt; for short. We also require that each key only appears once within each merged segment file (the compaction process already ensures that). SSTables have several big advantages over log segments with hash indexes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Merging segments is simple and efficient&lt;/strong&gt;, even if the files are bigger than the available memory. The approach is like the one used in the mergesort algorithm.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to find a particular key in the file, you no longer need to keep an index of all the keys in memory. You still need an in-memory index to tell you the offsets for some of the keys, but &lt;strong&gt;it can be sparse&lt;/strong&gt;: one key for every few kilobytes of segment file is sufficient, because a few kilobytes can be scanned very quickly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Since read requests need to scan over several key-value pairs in the requested range anyway, &lt;strong&gt;it is possible to group those records into a block and compress it before writing it to disk&lt;/strong&gt;. Each entry of the sparse in-memory index then points at the start of a compressed block. Besides saving disk space, compression also reduces the I/O bandwidth use.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Constructing and maintaining SSTables&lt;/h4&gt;

&lt;p&gt;How do you get your data to be sorted by key in the first place? We can now make our storage engine work as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When a write comes in, add it to an in-memory balanced tree data structure.&lt;/li&gt;
&lt;li&gt;When the memtable gets bigger than some threshold—typically a few megabytes—write it out to disk as an SSTable file.&lt;/li&gt;
&lt;li&gt;In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.&lt;/li&gt;
&lt;li&gt;From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It only suffers from one problem: if the database crashes, the most recent writes (which are in the memtable but not yet written out to disk) are lost. In order to avoid that problem, we can &lt;strong&gt;keep a separate log on disk to which every write is immediately appended&lt;/strong&gt;, just like in the previous section. That log is not in sorted order, but that doesn’t matter, because &lt;strong&gt;its only purpose is to restore the memtable after a crash&lt;/strong&gt;. &lt;/p&gt;

&lt;h4&gt;Performance optimizations&lt;/h4&gt;

&lt;p&gt;The LSM-tree algorithm can be slow when looking up keys that do not exist in the database: you have to check the memtable, then the segments all the way back to the oldest before you can be sure that the key does not exist. &lt;strong&gt;In order to optimize this kind of access, storage engines often use additional Bloom filters.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are also different strategies to determine the order and timing of how SSTables are compacted and merged. The most common options are &lt;strong&gt;size-tiered and leveled compaction&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;B-Trees&lt;/h3&gt;

&lt;p&gt;The most widely used indexing structure is quite different: the B-tree.&lt;/p&gt;

&lt;p&gt;Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key- value lookups and range queries. &lt;/p&gt;

&lt;p&gt;The log-structured indexes we saw earlier break the database down into variable-size segments, typically several megabytes or more in size, and always write a segment sequentially. &lt;strong&gt;By contrast, B-trees break the database down into fixed-size blocks or pages&lt;/strong&gt;, traditionally 4 KB in size (sometimes bigger), and &lt;strong&gt;read or write one page at a time&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Each page can be identified using an address or location&lt;/strong&gt;, which allows one page to refer to another—similar to a pointer, but on disk instead of in memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;One page is designated as the root of the B-tree; whenever you want to look up a key in the index, you start here. The page contains several keys and references to child pages. Each child is responsible for a continuous range of keys, and &lt;strong&gt;the keys between the references indicate where the boundaries between those ranges lie&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The number of references to child pages in one page of the B-tree is called the &lt;strong&gt;branching factor&lt;/strong&gt;. If you want to add a new key, you need to find the page whose range encompasses the new key and add it to that page. If there isn’t enough free space in the page to accommodate the new key, it is split into two half-full pages, and the parent page is updated to account for the new subdivision of key ranges.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;This algorithm ensures that the tree remains balanced: a B-tree with n keys always has a depth of O(log n). Most databases can fit into a B-tree that is three or four levels deep, so you don’t need to follow many page references to find the page you are look‐ ing for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.)&lt;/p&gt;

&lt;h4&gt;Making B-trees reliable&lt;/h4&gt;

&lt;p&gt;If you split a page because an insertion caused it to be overfull, you need to write the two pages that were split, and also overwrite their parent page to update the references to the two child pages. &lt;strong&gt;This is a dangerous operation, because if the database crashes after only some of the pages have been written, you end up with a corrupted index.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to make the database resilient to crashes, &lt;strong&gt;it is common for B-tree implementations to include an additional data structure on disk: a write-ahead log (WAL, also known as a redo log)&lt;/strong&gt;.  This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself. When the database comes back up after a crash, this log is used to restore the B-tree back to a consistent state.&lt;/p&gt;

&lt;p&gt;An additional complication of updating pages in place is that careful concurrency control is required if multiple threads are going to access the B-tree at the same time. &lt;strong&gt;This is typically done by protecting the tree’s data structures with latches (lightweight locks).&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;B-tree optimizations&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Instead of overwriting pages and maintaining a WAL for crash recovery, some databases (like LMDB) use a copy-on-write scheme. A modified page is written to a different location, and a new version of the parent pages in the tree is created, pointing at the new location.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can save space in pages by not storing the entire key, but abbreviating it. Especially in pages on the interior of the tree, keys only need to provide enough information to act as boundaries between key ranges. Packing more keys into a page allows the tree to have a higher branching factor, and thus fewer levels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additional pointers have been added to the tree. For example, each leaf page may have references to its sibling pages to the left and right, which allows scanning keys in order without jumping back to parent pages.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Comparing B-Trees and LSM-Trees&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;As a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Advantages of LSM-trees&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;A B-tree index must write every piece of data at least twice: once to the write-ahead log, and once to the tree page itself (and perhaps again as pages are split).&lt;/strong&gt; There is also overhead from having to write an entire page at a time, even if only a few bytes in that page changed.&lt;/p&gt;

&lt;p&gt;Log-structured indexes also rewrite data multiple times due to repeated compaction and merging of SSTables. &lt;/p&gt;

&lt;p&gt;In write-heavy applications, the performance bottleneck might be the rate at which the database can write to disk.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moreover, LSM-trees are typically able to sustain higher write throughput than B-trees, partly because they sometimes have lower write amplification, and partly because they sequentially write compact SSTable files rather than having to overwrite several pages in the tree.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since LSM-trees are not page-oriented and periodically rewrite SSTables to remove fragmentation, they have lower storage overheads, especially when using leveled compaction.&lt;/p&gt;

&lt;h4&gt;Downsides of LSM-trees&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;A downside of log-structured storage is that the compaction process can sometimes interfere with the performance of ongoing reads and writes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An advantage of B-trees is that each key exists in exactly one place in the index&lt;/strong&gt;, whereas a log-structured storage engine may have multiple copies of the same key in different segments. This aspect makes B-trees attractive in databases that want to offer strong transactional semantics: &lt;strong&gt;in many relational databases, transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Other Indexing Structures&lt;/h3&gt;

&lt;p&gt;A secondary index can easily be constructed from a key-value index. The main difference is that keys are not unique. This can be solved in two ways: &lt;strong&gt;either by making each value in the index a list of matching row identifiers (like a postings list in a full-text index) or by making each key unique by appending a row identifier to it.&lt;/strong&gt; &lt;/p&gt;

&lt;h4&gt;Storing values within the index&lt;/h4&gt;

&lt;p&gt;The key in an index is the thing that queries search for, but the value can be one of two things: it could be the actual row (document, vertex) in question, or it could be a reference to the row stored elsewhere. In the latter case, the place where rows are stored is known as a &lt;strong&gt;heap file&lt;/strong&gt;. &lt;strong&gt;The heap file approach is common because it avoids duplicating data when multiple secondary indexes are present: each index just references a location in the heap file, and the actual data is kept in one place.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In some situations, the extra hop from the index to the heap file is too much of a performance penalty for reads, so it can be desirable to store the indexed row directly within an index. This is known as a &lt;strong&gt;clustered index&lt;/strong&gt;. For example, in MySQL’s InnoDB storage engine, the primary key of a table is always a clustered index, and secondary indexes refer to the primary key (rather than a heap file location)（每一 个 Secondary Index 包含主健，再根据主键索引找到相应的数据）&lt;/p&gt;

&lt;h4&gt;Multi-column indexes&lt;/h4&gt;

&lt;p&gt;The most common type of multi-column index is called a &lt;strong&gt;concatenated index&lt;/strong&gt;, which simply combines several fields into one key by appending one column to another(the index definition specifies in which order the fields are concatenated).&lt;/p&gt;

&lt;p&gt;Multi-dimensional indexes are a more general way of querying several columns at once, which is particularly important for geospatial data. One option is to translate a two-dimensional location into a single number using a space-filling curve, and then to use a regular B-tree index. &lt;strong&gt;More commonly, specialized spatial indexes such as R-trees are used.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Full-text search and fuzzy indexes&lt;/h4&gt;

&lt;p&gt;To cope with typos in documents or queries, Lucene is able to search text for words within a certain edit distance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lucene uses a SSTable-like structure for its term dictionary.&lt;/strong&gt; This structure requires a small in- memory index that tells queries at which offset in the sorted file they need to look for a key. In LevelDB, this in-memory index is a sparse collection of some of the keys, but &lt;strong&gt;in Lucene, the in-memory index is a finite state automaton over the characters in the keys, similar to a trie&lt;/strong&gt;. This automaton can be transformed into a Levenshtein automaton, which supports efficient search for words within a given edit distance.&lt;/p&gt;

&lt;h4&gt;Keeping everything in memory&lt;/h4&gt;

&lt;p&gt;Counterintuitively, the performance advantage of in-memory databases is not due to the fact that they don’t need to read from disk. Rather, &lt;strong&gt;they can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk.&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Transaction Processing or Analytics?&lt;/h2&gt;

&lt;h3&gt;Data Warehousing&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;A data warehouse is a separate database that analysts can query to their hearts’ content, without affecting OLTP operations.&lt;/strong&gt; The data warehouse contains a read-only copy of the data in all the various OLTP systems in the company. Data is extracted from OLTP databases (using either a periodic data dump or a continuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse. This process of getting data into the warehouse is known as Extract–Transform–Load (ETL).&lt;/p&gt;

&lt;p&gt;A big advantage of using a separate data warehouse, rather than querying OLTP systems directly for analytics, is that the data warehouse can be optimized for analytic access patterns. It turns out that the indexing algorithms discussed in the first half of this chapter work well for OLTP, but are not very good at answering analytic queries.&lt;/p&gt;

&lt;h4&gt;The divergence between OLTP databases and data warehouses&lt;/h4&gt;

&lt;p&gt;The data model of a data warehouse is most commonly relational, because SQL is generally a good fit for analytic queries. &lt;/p&gt;

&lt;h3&gt;Stars and Snowflakes: Schemas for Analytics&lt;/h3&gt;

&lt;p&gt;A wide range of different data models are used in the realm of transaction processing, depending on the needs of the application. On the other hand, &lt;strong&gt;in analytics, there is much less diversity of data models. Many data warehouses are used in a fairly formulaic style, known as a star schema&lt;/strong&gt; (also known as dimensional modeling).&lt;/p&gt;

&lt;h2&gt;Column-Oriented Storage&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;In most OLTP databases, storage is laid out in a row-oriented fashion&lt;/strong&gt;: all the values from one row of a table are stored next to each other. Document databases are similar: an entire document is typically stored as one contiguous sequence of bytes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-5.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h3&gt;Column Compression&lt;/h3&gt;

&lt;p&gt;Depending on the data in the column, different compression techniques can be used. One technique that is particularly effective in data warehouses is bitmap encoding.&lt;/p&gt;

&lt;h3&gt;Sort Order in Column Storage&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Another advantage of sorted order is that it can help with compression of columns.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Writing to Column-Oriented Storage&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Column-oriented storage, compression, and sorting all help to make those read queries faster.&lt;/strong&gt; However, they have the downside of making writes more difficult.&lt;/p&gt;

&lt;p&gt;An update-in-place approach, like B-trees use, is not possible with compressed columns. If you wanted to insert a row in the middle of a sorted table, you would most likely have to rewrite all the column files.&lt;/p&gt;

&lt;p&gt;Fortunately, we have already seen a good solution earlier in this chapter: LSM-trees. All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn’t matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written to new files in bulk. &lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;On a high level, we saw that storage engines fall into two broad categories: those optimized for transaction processing (OLTP), and those optimized for analytics (OLAP). There are big differences between the access patterns in those use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;OLTP systems are typically user-facing, which means that they may see a huge volume of requests. &lt;strong&gt;Disk seek time is often the bottleneck here.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data warehouses and similar analytic systems are less well known, because they are primarily used by business analysts, not by end users. &lt;strong&gt;Disk bandwidth (not seek time) is often the bottleneck here&lt;/strong&gt;, and column-oriented storage is an increasingly popular solution for this kind of workload.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the OLTP side, we saw storage engines from two main schools of thought:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The log-structured school, which only permits appending to files and deleting obsolete files, but never updates a file that has been written. Bitcask, SSTables, LSM-trees, LevelDB, Cassandra, HBase, Lucene, and others belong to this group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The update-in-place school, which treats the disk as a set of fixed-size pages that can be overwritten. B-trees are the biggest example of this philosophy, being used in all major relational databases and also many nonrelational ones.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Analytic workloads are so different from OLTP: when your queries require sequentially scanning across a large number of rows, indexes are much less relevant. Instead it becomes important to encode data very compactly, to minimize the amount of data that the query needs to read from disk.&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jul 2018 18:57:23 +0800</pubDate>
        <link>http://masutangu.com/2018/07/data-intensive-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/data-intensive-note-1/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>TiKV 源码阅读（未完成）</title>
        <description>&lt;p&gt;TiKV 使用 RocksDB 做持久化存储引擎。将 key 分 range，每一段称为 Region。Region 分散在多台机器上以实现存储的水平扩展。每个 Region 会存放多个副本在不同机器上，使用 raft 算法管理：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tikv-note-1/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;PD 负责整个 TiKV 集群的调度。&lt;/p&gt;

&lt;p&gt;TiKV 使用 version 的方式进行多版本控制（MVCC）：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Key1-Version3 -&amp;gt; Value
Key1-Version2 -&amp;gt; Value
Key1-Version1 -&amp;gt; Value
...
Key2-Version2 -&amp;gt; Value
Key2-Version1 -&amp;gt; Value
...
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;TiKV 事务使用 &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36726.pdf&quot;&gt;Percolator&lt;/a&gt; 模型。采用乐观锁，事务提交才进行冲突检测。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24564094&quot;&gt;https://zhuanlan.zhihu.com/p/24564094&lt;/a&gt;
&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-internal-1/&quot;&gt;https://pingcap.com/blog-cn/tidb-internal-1/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Jul 2018 21:59:16 +0800</pubDate>
        <link>http://masutangu.com/2018/07/tikv-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/tikv-note-1/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>raft-rust 初体验</title>
        <description>&lt;p&gt;之前分析了使用 golang 实现的 etcd-raft，这几天再读了下 rust 实现的 &lt;a href=&quot;https://github.com/pingcap/raft-rs&quot;&gt;raft-rs&lt;/a&gt;，简单说下对比。rust 版应该是基于 golang 版来实现的，所有的类、方法基本上是一致的。&lt;/p&gt;

&lt;p&gt;从样例看起，&lt;code&gt;let (sender, receiver) = mpsc::channel();&lt;/code&gt; 创建了 channel 用于线程之间数据传递（类似 golang 的 channel）。调用 &lt;code&gt;send_propose&lt;/code&gt; 创建一个线程，通过 &lt;code&gt;sender&lt;/code&gt; 发送 propose 请求。&lt;code&gt;main&lt;/code&gt; 主线程则在 loop 循环中监听 &lt;code&gt;receiver&lt;/code&gt; channel 的请求。如果是 propose 请求，调用 RawNode 的 &lt;code&gt;propose&lt;/code&gt; 方法处理（其内部调用 &lt;code&gt;self.raft.step&lt;/code&gt; 方法），如果是其他请求，直接调用 RawNode 的 &lt;code&gt;step&lt;/code&gt; 方法处理（其内部也是调用 &lt;code&gt;self.raft.step&lt;/code&gt; 方法）。loop 循环最后调用 &lt;code&gt;on_ready&lt;/code&gt;，处理 raft 层返回的 ready 对象，这个逻辑和之前 golang 实现的 etcd-raft 是很类似的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;// A simple example about how to use the Raft library in Rust.
fn main() {
    // Create a storage for Raft, and here we just use a simple memory storage.
    // You need to build your own persistent storage in your production.
    // Please check the Storage trait in src/storage.rs to see how to implement one.
    let storage = MemStorage::new();

    // Create the configuration for the Raft node.
    let cfg = Config {
        ..Default::default()
    };

    // Create the Raft node.
    let mut r = RawNode::new(&amp;amp;cfg, storage, vec![]).unwrap();

    let (sender, receiver) = mpsc::channel();

    // Use another thread to propose a Raft request.
    send_propose(sender);

    // Loop forever to drive the Raft.
    let mut t = Instant::now();
    let mut timeout = Duration::from_millis(100);

    // Use a HashMap to hold the `propose` callbacks.
    let mut cbs = HashMap::new();

    loop {
        match receiver.recv_timeout(timeout) {
            Ok(Msg::Propose { id, cb }) =&amp;gt; {
                cbs.insert(id, cb);
                r.propose(vec![], vec![id]).unwrap();
            }
            Ok(Msg::Raft(m)) =&amp;gt; r.step(m).unwrap(),
            Err(RecvTimeoutError::Timeout) =&amp;gt; (),
            Err(RecvTimeoutError::Disconnected) =&amp;gt; return,
        }

        let d = t.elapsed();
        if d &amp;gt;= timeout {
            t = Instant::now();
            timeout = Duration::from_millis(100);
            // We drive Raft every 100ms.
            r.tick();
        } else {
            timeout -= d;
        }

        on_ready(&amp;amp;mut r, &amp;amp;mut cbs);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;留意这里，往 sender 发了个包在 Box 里的闭包 &lt;code&gt;s1.send(0).unwrap()&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;fn send_propose(sender: mpsc::Sender&amp;lt;Msg&amp;gt;) {
    thread::spawn(move || {
        // Wait some time and send the request to the Raft.
        thread::sleep(Duration::from_secs(10));

        let (s1, r1) = mpsc::channel::&amp;lt;u8&amp;gt;();

        println!(&amp;quot;propose a request&amp;quot;);

        // Send a command to the Raft, wait for the Raft to apply it
        // and get the result.
        sender
            .send(Msg::Propose {
                id: 1,
                // cb 为 closure
                cb: Box::new(move || {
                    s1.send(0).unwrap();
                }),
            })
            .unwrap();

        // 当该 propose 请求被处理时，会调用 cb，往 s1 send 值，于是 r1 的 recv 会返回
        let n = r1.recv().unwrap();
        assert_eq!(n, 0);

        println!(&amp;quot;receive the propose callback&amp;quot;);
    });
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;fn on_ready(r: &amp;amp;mut RawNode&amp;lt;MemStorage&amp;gt;, cbs: &amp;amp;mut HashMap&amp;lt;u8, ProposeCallback&amp;gt;) {
    let mut ready = r.ready(); // 调用 ready 方法，拿到 ready 对象
    // 一波处理 忽略，处理完后也是调用了 advance 方法
    ...
    // Advance the Raft
    r.advance(ready);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;和 golang 版最大的区别，是 golang 用了 channel 来做进行模块之间的数据通信：&lt;code&gt;recvc&lt;/code&gt; channel 收发请求，&lt;code&gt;advancec&lt;/code&gt; channel 收发 advance 消息，&lt;code&gt;readyc&lt;/code&gt; channel 收发 ready 对象。而 rust 中则直接调用 &lt;code&gt;step&lt;/code&gt;、&lt;code&gt;propose&lt;/code&gt;、&lt;code&gt;ready&lt;/code&gt; 和 &lt;code&gt;advance&lt;/code&gt; 方法来驱动状态机，没有通过 rust 的 channel 机制做消息传递。也许是考虑到效率？由于 raft-rs 提供的例子有点简单，等之后读 tikv 的代码，再来做下一步的对比。&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Jul 2018 21:59:16 +0800</pubDate>
        <link>http://masutangu.com/2018/07/raft-rust/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/raft-rust/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（PreVote）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 的 PreVote 机制，避免由于网络分区导致 candidate 的 term 不断增大。&lt;/p&gt;

&lt;p&gt;Election timeout 之后，发送 type 为 pb.MsgHup 的请求，进入选举阶段：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// tickElection is run by followers and candidates after r.electionTimeout.
func (r *raft) tickElection() {
    r.electionElapsed++

    if r.promotable() &amp;amp;&amp;amp; r.pastElectionTimeout() {
        r.electionElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgHup})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;如果设置了 preVote 为 true，则先进入 prevote 阶段。调用 &lt;code&gt;r.campaign&lt;/code&gt; 传入 type &lt;code&gt;campaignPreElection&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) Step(m pb.Message) error {
    switch m.Type {
    case pb.MsgHup:
        if r.state != StateLeader {
            r.logger.Infof(&amp;quot;%x is starting a new election at term %d&amp;quot;, r.id, r.Term)
            if r.preVote {
                // 如果 preVote 设置为 true，先发起 campaignPreElection
                r.campaign(campaignPreElection)
            } else {
                r.campaign(campaignElection)
            }
        } else {
            r.logger.Debugf(&amp;quot;%x ignoring MsgHup because already leader&amp;quot;, r.id)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;campaign&lt;/code&gt; 方法处理选举逻辑。如果是 &lt;code&gt;campaignPreElection&lt;/code&gt;，设置节点状态为 &lt;code&gt;StatePreCandidate&lt;/code&gt;，此时不会递增节点的 Term（避免 term 增长过快）。然后向其他 peers 发送 type 为 &lt;code&gt;pb.MsgPreVote&lt;/code&gt; 的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) campaign(t CampaignType) {
    var term uint64
    var voteMsg pb.MessageType
    if t == campaignPreElection {
        r.becomePreCandidate()  // state 设置为 StatePreCandidate
        voteMsg = pb.MsgPreVote  // msg type 设置为 preVote
        // PreVote RPCs are sent for the next term before we&amp;#39;ve incremented r.Term.
        term = r.Term + 1  // preVote 不会递增 r.Term
    } else {
        ...
    }
    if r.quorum() == r.poll(r.id, voteRespMsgType(voteMsg), true) {
        // We won the election after voting for ourselves (which must mean that
        // this is a single-node cluster). Advance to the next state.
        if t == campaignPreElection {
            r.campaign(campaignElection)  // prevote 成功，可以发起 campaignElection 了
        } else {
            ... 
        }
        return
    }

    // 广播 pb.MsgPreVote
    for id := range r.prs {
        if id == r.id {
            continue
        }
        var ctx []byte
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看看 &lt;code&gt;stepCandidate&lt;/code&gt; 如何处理 &lt;code&gt;pb.MsgPreVote&lt;/code&gt; 请求的回包。检查选票是否达到 quorum 数量，如果已经达到，prevote 成功，可以发起真正的选举了，调用 &lt;code&gt;r.campaign&lt;/code&gt; 传入 type &lt;code&gt;campaignElection&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// stepCandidate is shared by StateCandidate and StatePreCandidate; the difference is
// whether they respond to MsgVoteResp or MsgPreVoteResp.
func stepCandidate(r *raft, m pb.Message) error {
    // Only handle vote responses corresponding to our candidacy (while in
    // StateCandidate, we may get stale MsgPreVoteResp messages in this term from
    // our pre-candidate state).
    var myVoteRespType pb.MessageType
    if r.state == StatePreCandidate {
        myVoteRespType = pb.MsgPreVoteResp
    } else {
        myVoteRespType = pb.MsgVoteResp
    }
    switch m.Type {
    case myVoteRespType:
        gr := r.poll(m.From, m.Type, !m.Reject)
        switch r.quorum() {
        case gr:
            if r.state == StatePreCandidate {
                r.campaign(campaignElection)  // prevote 成功，可以发起 campaignElection
            } else {
                ...
            }
        case len(r.votes) - gr:  // prevote 失败（m.Reject 为 true，此时 m.Term &amp;gt; r.Term），转为 follower 角色
            // pb.MsgPreVoteResp contains future term of pre-candidate
            // m.Term &amp;gt; r.Term; reuse r.Term
            r.becomeFollower(r.Term, None)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;如果 campaign 类型为 &lt;code&gt;campaignElection&lt;/code&gt;，则调用 &lt;code&gt;r.becomeCandidate&lt;/code&gt;，此时设置节点状态为 &lt;code&gt;StateCandidate&lt;/code&gt;，递增节点的 Term，并向其他 peers 发送 &lt;code&gt;pb.MsgVote&lt;/code&gt; 请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) campaign(t CampaignType) {
    var term uint64
    var voteMsg pb.MessageType
    if t == campaignPreElection {
        ...
    } else {
        r.becomeCandidate()  // become cancdidate，term 递增
        voteMsg = pb.MsgVote
        term = r.Term
    }
    if r.quorum() == r.poll(r.id, voteRespMsgType(voteMsg), true) {
        // We won the election after voting for ourselves (which must mean that
        // this is a single-node cluster). Advance to the next state.
        if t == campaignPreElection {
            ...
        } else {
            r.becomeLeader()  // 得到 quorum 的选票，选举 leader 成功，become leader
        }
        return
    }

    // 广播 pb.MsgVote，进行选举
    for id := range r.prs {
        if id == r.id {
            continue
        }
        var ctx []byte
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;收到 &lt;code&gt;pb.MsgVote&lt;/code&gt; 的回包后，同样检查是否选票数量是否达到 quorum，成功则该节点当选 leader：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepCandidate(r *raft, m pb.Message) error {
    // Only handle vote responses corresponding to our candidacy (while in
    // StateCandidate, we may get stale MsgPreVoteResp messages in this term from
    // our pre-candidate state).
    var myVoteRespType pb.MessageType
    if r.state == StatePreCandidate {
        myVoteRespType = pb.MsgPreVoteResp
    } else {
        myVoteRespType = pb.MsgVoteResp
    }
    switch m.Type {
    case myVoteRespType:
        gr := r.poll(m.From, m.Type, !m.Reject)
        r.logger.Infof(&amp;quot;%x [quorum:%d] has received %d %s votes and %d vote rejections&amp;quot;, r.id, r.quorum(), gr, m.Type, len(r.votes)-gr)
        switch r.quorum() {
        case gr:
            if r.state == StatePreCandidate {
                ...
            } else {
                r.becomeLeader() // 收到 quorum 选票，选举成功
                r.bcastAppend()  // 广播 append 消息
            }
        case len(r.votes) - gr:
            // pb.MsgPreVoteResp contains future term of pre-candidate
            // m.Term &amp;gt; r.Term; reuse r.Term
            r.becomeFollower(r.Term, None)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Sun, 08 Jul 2018 10:19:08 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-6/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-6/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（Leader Transfer）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 如何实现 leadership transfer，把 leader 身份转移给某个 follower。&lt;/p&gt;

&lt;p&gt;应用层调用 &lt;code&gt;TransferLeadership&lt;/code&gt; 方法，发送一个 type 为 pb.MsgTransferLeader 的请求给 raft 处理。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (n *node) TransferLeadership(ctx context.Context, lead, transferee uint64) {
    select {
    // manually set &amp;#39;from&amp;#39; and &amp;#39;to&amp;#39;, so that leader can voluntarily transfers its leadership
    case n.recvc &amp;lt;- pb.Message{Type: pb.MsgTransferLeader, From: transferee, To: lead}:
    case &amp;lt;-n.done:
    case &amp;lt;-ctx.Done():
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;stepLeader&lt;/code&gt; 收到 pb.MsgTransferLeader 后，检查下是否有正在进行的 leader transfer，并检查 tranferee 的 log 是否是最新的，如果是，调用 &lt;code&gt;sendTimeoutNow&lt;/code&gt;，如果不是最新日志，则发送 appendEntriesReq，收到 MsgAppResp 后，如果条件符合，再调用 &lt;code&gt;sendTimeoutNow&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // All other message types require a progress for m.From (pr).
    pr := r.getProgress(m.From)
    // These message types do not require any progress for m.From.
    switch m.Type {
        case pb.MsgTransferLeader:
        leadTransferee := m.From
        lastLeadTransferee := r.leadTransferee
        if lastLeadTransferee != None {
            if lastLeadTransferee == leadTransferee {
                return nil
            }
            // 取消之前的
            r.abortLeaderTransfer()
        }
        if leadTransferee == r.id {
            // leadTransferee 已经是 leader 了
            return nil
        }
        // Transfer leadership should be finished in one electionTimeout, so reset r.electionElapsed.
        r.electionElapsed = 0
        r.leadTransferee = leadTransferee
        // 如果 leadTransferee 的 log 已经是最新的了 则马上调用 sendTimeoutNow，开始 transfer
        if pr.Match == r.raftLog.lastIndex() {
            r.sendTimeoutNow(leadTransferee)
        } else {
            // 否则先往 leadTransferee append 日志
            r.sendAppend(leadTransferee)
        }

        ...
        // 收到 append 回包后，检查是不是有 in progress 的 leader transfer，并且 log 也是最新了的话，则调用 sendTimeoutNow
        case pb.MsgAppResp:
        pr.RecentActive = true

        ...
        // Transfer leadership is in progress.
        if m.From == r.leadTransferee &amp;amp;&amp;amp; pr.Match == r.raftLog.lastIndex() {
            r.sendTimeoutNow(m.From)
        }
        ...
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;Leader transfer 过程中不处理 pb.MsgProp 类型的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgProp:
        ...
        if r.leadTransferee != None {
            // 正在 leader tranfer，不处理 Propose 请求
            return ErrProposalDropped
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;sendTimeoutNow&lt;/code&gt; 发送 pb.MsgTimeoutNow 的请求，看看 follower 如何处理：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepFollower(r *raft, m pb.Message) error {
    switch m.Type {
        case pb.MsgTimeoutNow:
        if r.promotable() {
            // Leadership transfers never use pre-vote even if r.preVote is true; we
            // know we are not recovering from a partition so there is no need for the
            // extra round trip.
            r.campaign(campaignTransfer)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;campain&lt;/code&gt; 会发送 voteMsg 给 peers 进行选举：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;
func (r *raft) campaign(t CampaignType) {
    var term uint64
    var voteMsg pb.MessageType
    if t == campaignPreElection {
        r.becomePreCandidate()
        voteMsg = pb.MsgPreVote
        // PreVote RPCs are sent for the next term before we&amp;#39;ve incremented r.Term.
        term = r.Term + 1
    } else {
        r.becomeCandidate()  // 变成 Candidate term + 1，此时该节点 term 最大，所以该节点将成为新的 leader
        voteMsg = pb.MsgVote
        term = r.Term
    }
    if r.quorum() == r.poll(r.id, voteRespMsgType(voteMsg), true) {
        // We won the election after voting for ourselves (which must mean that
        // this is a single-node cluster). Advance to the next state.
        if t == campaignPreElection {
            r.campaign(campaignElection)
        } else {
            r.becomeLeader()
        }
        return
    }

    // 发送 voteMsg
    for id := range r.prs {
        if id == r.id {
            continue
        }
        var ctx []byte
        if t == campaignTransfer {
            ctx = []byte(t)
        }
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;当前 leader 变成 follower 之后，会调用 &lt;code&gt;reset&lt;/code&gt;，&lt;code&gt;reset&lt;/code&gt; 将调用 &lt;code&gt;abortLeaderTransfer&lt;/code&gt; 把 &lt;code&gt;r.leadTransferee&lt;/code&gt; 设置为 None，leader transfer 完成。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) reset(term uint64) {
    if r.Term != term {
        r.Term = term
        r.Vote = None
    }
    r.lead = None

    r.electionElapsed = 0
    r.heartbeatElapsed = 0
    r.resetRandomizedElectionTimeout()
    r.abortLeaderTransfer()
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Fri, 06 Jul 2018 22:56:37 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-5/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-5/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（Linearizable Read 之 Lease）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 如何实现 linearizable read（linearizable read 简单的说就是不返回 stale 数据，具体可以看这篇文章 &lt;a href=&quot;https://aphyr.com/posts/313-strong-consistency-models&quot;&gt;《Strong consistency models》&lt;/a&gt;）。&lt;/p&gt;

&lt;p&gt;除了基于 &lt;a href=&quot;http://masutangu.com/2018/07/etcd-raft-note-3/&quot;&gt;ReadIndex&lt;/a&gt; 之外，raft 论文第 8 节还阐述了另一种基于 heartbeat 的 lease 思路：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease, but this would rely on timing for safety (it
assumes bounded clock skew).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;raft 中，follower 至少会在 election timeout 之后才重新进行选举。leader 定期发送 heartbeat，在收到 quonum 节点的回包后的 election timeout 这段时间间隔内，不会有新一轮的选举（因为各个机器的 cpu 时钟有误差，所以这个方案有风险）。&lt;/p&gt;

&lt;p&gt;lease 模式对应用层提供的接口还是 &lt;code&gt;ReadIndex&lt;/code&gt;，应用层处理的方式也和基于 ReadIndex 模式相同。只是 raft 内部逻辑不同。&lt;/p&gt;

&lt;p&gt;如果指定了 leaseBase 的模式，那要求 &lt;code&gt;CheckQuorum&lt;/code&gt; 为 true，&lt;code&gt;validate&lt;/code&gt; 方法做了这个检查：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (c *Config) validate() error {
    ...
    if c.ReadOnlyOption == ReadOnlyLeaseBased &amp;amp;&amp;amp; !c.CheckQuorum {
        return errors.New(&amp;quot;CheckQuorum must be enabled when ReadOnlyOption is ReadOnlyLeaseBased&amp;quot;)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;指定了 &lt;code&gt;checkQuorum&lt;/code&gt; 为 true 之后，每次 tick 都会看是否应该检查 Quorum（间隔 electionTimeout），通过发送 pb.MsgCheckQuorum 类型的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// tickHeartbeat is run by leaders to send a MsgBeat after r.heartbeatTimeout.
func (r *raft) tickHeartbeat() {
    r.heartbeatElapsed++
    r.electionElapsed++

    if r.electionElapsed &amp;gt;= r.electionTimeout {  // 每隔 electionTimeout 检查一次
        r.electionElapsed = 0
        if r.checkQuorum {
            r.Step(pb.Message{From: r.id, Type: pb.MsgCheckQuorum})  // 检查 Quorum
        }
        // If current leader cannot transfer leadership in electionTimeout, it becomes leader again.
        if r.state == StateLeader &amp;amp;&amp;amp; r.leadTransferee != None {
            r.abortLeaderTransfer()
        }
    }

    if r.state != StateLeader {
        return
    }

    if r.heartbeatElapsed &amp;gt;= r.heartbeatTimeout {
        r.heartbeatElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgBeat})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;stepLeader&lt;/code&gt; 收到 pb.MsgCheckQuorum 后调用 &lt;code&gt;checkQuorumActive&lt;/code&gt; 进行检查，如果返回 false，此时把节点变更为 follower：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgCheckQuorum:
        if !r.checkQuorumActive() {  
            r.becomeFollower(r.Term, None)  // checkQuorumActive 失败，变成 follower
        }
        return nil
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;checkQuorumActive&lt;/code&gt; 即统计 active 的 peers 数量是否超过 quonum：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// checkQuorumActive also resets all RecentActive to false.
func (r *raft) checkQuorumActive() bool {
    var act int

    r.forEachProgress(func(id uint64, pr *Progress) {
        if id == r.id { // self is always active
            act++
            return
        }

        if pr.RecentActive &amp;amp;&amp;amp; !pr.IsLearner {
            act++
        }

        pr.RecentActive = false
    })

    return act &amp;gt;= r.quorum()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;RecentActive&lt;/code&gt; 是 leader 收到 peers 的心跳回包或者 appendEntriesReq 的回包时设置的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // All other message types require a progress for m.From (pr).
    pr := r.getProgress(m.From)

    switch m.Type {
    case pb.MsgAppResp:
        pr.RecentActive = true
        ...
    case pb.MsgHeartbeatResp:
        pr.RecentActive = true
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;文章开头提到， leaseBase 模式下 etcd-raft 对应用层暴露的也是 &lt;code&gt;ReadIndex&lt;/code&gt; 接口。在收到 pb.MsgReadIndex 类型的请求时，由于 CheckQuonum 保证了我们 leader 有效，就可以直接 append 到 r.readStates 中。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgReadIndex:
        // 5.4 safety
        if r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(r.raftLog.committed)) != r.Term {
            // Reject read only request when this leader has not committed any log entry at its term.
            return nil
        }

        // thinking: use an interally defined context instead of the user given context.
        // We can express this in terms of the term and index instead of a user-supplied value.
        // This would allow multiple reads to piggyback on the same message.
        switch r.readOnly.option {
        case ReadOnlyLeaseBased: // leaseBase 模式
            ri := r.raftLog.committed
            if m.From == None || m.From == r.id { // from local member
                r.readStates = append(r.readStates, ReadState{Index: r.raftLog.committed, RequestCtx: m.Entries[0].Data})
            } else {
                r.send(pb.Message{To: m.From, Type: pb.MsgReadIndexResp, Index: ri, Entries: m.Entries})
            }
        }

        return nil
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Fri, 06 Jul 2018 08:46:43 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-4/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-4/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（Linearizable Read 之 ReadIndx）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 如何实现 linearizable read（linearizable read 简单的说就是不返回 stale 数据，具体可以看这篇文章 &lt;a href=&quot;https://aphyr.com/posts/313-strong-consistency-models&quot;&gt;《Strong consistency models》&lt;/a&gt;）。&lt;/p&gt;

&lt;p&gt;raft 论文第 8 节阐述了思路：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Read-only operations can be handled without writing anything into the log. However, with no additional measures, this would run the risk of returning stale data, since the leader responding to the request might have been superseded by a newer leader of which it is unaware. Linearizable reads must not return stale data, and Raft needs two extra precautions to guarantee this without using the log. First, a leader must have the latest information on which entries are committed. The Leader Completeness Property guarantees that a leader has all committed entries, but at the start of its term, it may not know which those are. To find out, it needs to commit an entry from its term. Raft handles this by having each leader commit a blank no-op entry into the log at the start of its term. Second, a leader must check whether it has been deposed before processing a read-only request (its information may be stale if a more recent leader has been elected). Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在收到读请求时，leader 节点保存下当前的 commit index，并往 peers 发送心跳。如果确定该节点依然是 leader，则只需要等到该 commit index 的 log entry 被 apply 到状态机时就可以返回客户端结果。&lt;/p&gt;

&lt;p&gt;我们先通过位于 etcd/etcdserver 目录下的样例来看看应用层是如何使用 ReadIndex 来保证 linearizable read 的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// v3_server.go

type RaftKV interface {
    Range(ctx context.Context, r *pb.RangeRequest) (*pb.RangeResponse, error)
    Put(ctx context.Context, r *pb.PutRequest) (*pb.PutResponse, error)
    DeleteRange(ctx context.Context, r *pb.DeleteRangeRequest) (*pb.DeleteRangeResponse, error)
    Txn(ctx context.Context, r *pb.TxnRequest) (*pb.TxnResponse, error)
    Compact(ctx context.Context, r *pb.CompactionRequest) (*pb.CompactionResponse, error)
}

func (s *EtcdServer) Range(ctx context.Context, r *pb.RangeRequest) (*pb.RangeResponse, error) {
    var resp *pb.RangeResponse
    var err error

    if !r.Serializable {
        err = s.linearizableReadNotify(ctx)  // 等待 linearizableReadNotify 返回 才能继续往下走
        if err != nil {
            return nil, err
        }
    }
    // 读取数据逻辑 省略..
    ...
    return resp, err
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;在读请求 &lt;code&gt;Range&lt;/code&gt; 执行前，调用了 &lt;code&gt;linearizableReadNotify&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (s *EtcdServer) linearizableReadNotify(ctx context.Context) error {
    s.readMu.RLock()
    nc := s.readNotifier
    s.readMu.RUnlock()

    // signal linearizable loop for current notify if it hasn&amp;#39;t been already
    select {
    case s.readwaitc &amp;lt;- struct{}{}:
    default:
    }

    // wait for read state notification
    select {
    case &amp;lt;-nc.c:
        return nc.err
    case &amp;lt;-ctx.Done():
        return ctx.Err()
    case &amp;lt;-s.done:
        return ErrStopped
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;linearizableReadNotify&lt;/code&gt; 往 &lt;code&gt;readwaitc&lt;/code&gt; 发送个空的结构体，并且等待 &lt;code&gt;nc.c&lt;/code&gt; 的返回。&lt;code&gt;readwaitc&lt;/code&gt; 是在另外的 goroutine &lt;code&gt;linearizableReadLoop&lt;/code&gt; 里监听的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;
func (s *EtcdServer) linearizableReadLoop() {
    var rs raft.ReadState

    for {
        ctx := make([]byte, 8)
        binary.BigEndian.PutUint64(ctx, s.reqIDGen.Next())  // ctx 即请求唯一标识 reqId

        select {
        case &amp;lt;-s.readwaitc:  // 监听 readwaitc
        case &amp;lt;-s.stopping:
            return
        }

        nextnr := newNotifier()
        nr := s.readNotifier
        s.readNotifier = nextnr

        s.r.ReadIndex(cctx, ctx)  // 调用 ReadIndex 接口，往 recvc channel 发送 type 为 pb.MsgReadIndex 的请求

        var (
            timeout bool
            done    bool
        )
        for !timeout &amp;amp;&amp;amp; !done {
            select {
            case rs = &amp;lt;-s.r.readStateC:  // 收到 ready 对象时，会往 readStateC channel 传回来 readState，见 etcd/etcdserver/raft.go 文件的 func (r *raftNode) start(rh *raftReadyHandler)
                done = bytes.Equal(rs.RequestCtx, ctx)  // 比较下 reqId 是否一致
            case &amp;lt;-time.After(s.Cfg.ReqTimeout()):
                nr.notify(ErrTimeout)
                timeout = true
            case &amp;lt;-s.stopping:
                return
            }
        }
        if !done {
            continue
        }

        // 等待 readState 里的 index，也就是收到 pb.MsgReadIndex 请求时，leader 节点当前的 commit index 被 apply 到状态机时，此时调用 nr.notify(nil) 通知应用层可以读取状态机里的数据了，确保读到的不是 stale 数据
        if ai := s.getAppliedIndex(); ai &amp;lt; rs.Index {
            select {
            case &amp;lt;-s.applyWait.Wait(rs.Index):
            case &amp;lt;-s.stopping:
                return
            }
        }
        // unblock all l-reads requested at indices before rs.Index
        nr.notify(nil)
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;在 &lt;code&gt;linearizableReadLoop&lt;/code&gt; 调用 &lt;code&gt;nr.notify&lt;/code&gt; 后，&lt;code&gt;linearizableReadNotify&lt;/code&gt; 从 select 阻塞中返回，此时就可以继续走 &lt;code&gt;Range&lt;/code&gt; 的逻辑，读取数据，返回给客户端。&lt;/p&gt;

&lt;p&gt;从上面的例子，我们了解了应用层如何使用 Node 的 &lt;code&gt;ReadIndex&lt;/code&gt; 接口来实现 linearizable read。下面我们来介绍 &lt;code&gt;ReadIndex&lt;/code&gt; 这个新接口：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// Node represents a node in a raft cluster.
type Node interface {
    // Propose proposes that data be appended to the log.
    Propose(ctx context.Context, data []byte) error

    // Ready returns a channel that returns the current point-in-time state.
    // Users of the Node must call Advance after retrieving the state returned by Ready.
    //
    // NOTE: No committed entries from the next Ready may be applied until all committed entries
    // and snapshots from the previous one have finished.
    Ready() &amp;lt;-chan Ready

    // Advance notifies the Node that the application has saved progress up to the last Ready.
    // It prepares the node to return the next available Ready.
    //
    // The application should generally call Advance after it applies the entries in last Ready.
    //
    // However, as an optimization, the application may call Advance while it is applying the
    // commands. For example. when the last Ready contains a snapshot, the application might take
    // a long time to apply the snapshot data. To continue receiving Ready without blocking raft
    // progress, it can call Advance before finishing applying the last ready.
    Advance()

    // ReadIndex request a read state. The read state will be set in the ready.
    // Read state has a read index. Once the application advances further than the read
    // index, any linearizable read requests issued before the read request can be
    // processed safely. The read state will have the same rctx attached.
    ReadIndex(ctx context.Context, rctx []byte) error
}

func (n *node) ReadIndex(ctx context.Context, rctx []byte) error {
    return n.step(ctx, pb.Message{Type: pb.MsgReadIndex, Entries: []pb.Entry})
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;上篇文章 &lt;a href=&quot;http://masutangu.com/2018/07/etcd-raft-note-2/&quot;&gt;《etcd-raft 源码学习笔记（概览篇）》&lt;/a&gt; 提到当节点为 leader 时，&lt;code&gt;step&lt;/code&gt; 被设置为 &lt;code&gt;stepLeader&lt;/code&gt; 。我们来看看 &lt;code&gt;stepLeader&lt;/code&gt; 是如何处理 type 为 pb.MsgReadIndex 的 readIndexReq 的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgReadIndex:
        // raft 5.4 safty 检查
        if r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(r.raftLog.committed)) != r.Term {
            // Reject read only request when this leader has not committed any log entry at its term.
            return nil
        }

        // thinking: use an interally defined context instead of the user given context.
        // We can express this in terms of the term and index instead of a user-supplied value.
        // This would allow multiple reads to piggyback on the same message.
        switch r.readOnly.option {
        case ReadOnlySafe:
            r.readOnly.addRequest(r.raftLog.committed, m)  // r.raftLog.committed 为 当前 commit index
            r.bcastHeartbeatWithCtx(m.Entries[0].Data)  // 广播心跳包
        }
        return nil
    }
    return nil
}


&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;收到 readIndexReq 后，首先调用 &lt;code&gt;r.readOnly.addRequest&lt;/code&gt; 保存下，然后调用 &lt;code&gt;bcastHeartbeatWithCtx&lt;/code&gt; 广播心跳包， ctx 即唯一标识 readIndexReq 的 reqId。&lt;/p&gt;

&lt;p&gt;来看看 raft 是如何管理 readIndexReq 的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// addRequest adds a read only reuqest into readonly struct.
// `index` is the commit index of the raft state machine when it received
// the read only request.
// `m` is the original read only request message from the local or remote node.
func (ro *readOnly) addRequest(index uint64, m pb.Message) {
    ctx := string(m.Entries[0].Data)  // ctx 即 reqId
    if _, ok := ro.pendingReadIndex[ctx]; ok {
        return
    }
    ro.pendingReadIndex[ctx] = &amp;amp;readIndexStatus{index: index, req: m, acks: make(map[uint64]struct{})}  // acks 用于记录哪些 peer 已经 ack 确认。之后用于统计是否大于 quonum
    ro.readIndexQueue = append(ro.readIndexQueue, ctx)  // append 进 readIndexQueue
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;再看看 &lt;code&gt;stepLeader&lt;/code&gt; 如何处理心跳回包：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgHeartbeatResp:
        pr.RecentActive = true
        pr.resume()

        if r.readOnly.option != ReadOnlySafe || len(m.Context) == 0 {
            return nil
        }

        ackCount := r.readOnly.recvAck(m)
        if ackCount &amp;lt; r.quorum() {  // 判断是否收到 quorum 的心跳回包
            return nil
        }

        // 收到 quorum 的心跳回包了，把 readIndexReq 依次 append r.readStates 中，返回 ready 对象时会包含 r.readStates
        rss := r.readOnly.advance(m)
        for _, rs := range rss {
            req := rs.req
            if req.From == None || req.From == r.id { // from local member
                r.readStates = append(r.readStates, ReadState{Index: rs.index, RequestCtx: req.Entries[0].Data})
            } else {
                r.send(pb.Message{To: req.From, Type: pb.MsgReadIndexResp, Index: rs.index, Entries: req.Entries})
            }
        }
    return nil
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;调用 &lt;code&gt;r.readOnly.recvAck&lt;/code&gt;，根据 readIndeReq 的 reqId 统计收到心跳回包的数量：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// recvAck notifies the readonly struct that the raft state machine received
// an acknowledgment of the heartbeat that attached with the read only request
// context.
func (ro *readOnly) recvAck(m pb.Message) int {
    rs, ok := ro.pendingReadIndex[string(m.Context)]
    if !ok {
        return 0
    }

    rs.acks[m.From] = struct{}{}  // 记录下收到 m.From 这个节点的 ack
    // add one to include an ack from local node
    return len(rs.acks) + 1
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;如果超过 quonum 表示该节点依然是 leader，此时从 &lt;code&gt;r.readOnly.advance&lt;/code&gt; 拿到保存的 readIndexReq，append 到 &lt;code&gt;r.readStates&lt;/code&gt; 中：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// advance advances the read only request queue kept by the readonly struct.
// It dequeues the requests until it finds the read only request that has
// the same context as the given `m`.
func (ro *readOnly) advance(m pb.Message) []*readIndexStatus {
    var (
        i     int
        found bool
    )

    ctx := string(m.Context)
    rss := []*readIndexStatus{}

    for _, okctx := range ro.readIndexQueue {
        i++
        rs, ok := ro.pendingReadIndex[okctx]
        if !ok {
            panic(&amp;quot;cannot find corresponding read state from pending map&amp;quot;)
        }
        rss = append(rss, rs)
        if okctx == ctx {
            // 取出 reqId 相同的 ReadState 和其前面的所有 ReadState 
            found = true
            break
        }
    }

    if found {
        ro.readIndexQueue = ro.readIndexQueue[i:]
        for _, rs := range rss {
            delete(ro.pendingReadIndex, string(rs.req.Entries[0].Data))
        }
        return rss
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;之后调用 &lt;code&gt;newReady&lt;/code&gt; 会把 &lt;code&gt;r.readStates&lt;/code&gt; 返回给应用层，应用层取出 readIndexReq 中的 commit index，等到其被 apply 到状态机就可以允许读操作了。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func newReady(r *raft, prevSoftSt *SoftState, prevHardSt pb.HardState) Ready {
    rd := Ready{
        Entries:          r.raftLog.unstableEntries(),
        CommittedEntries: r.raftLog.nextEnts(),
        Messages:         r.msgs,
    }
    ...

    if len(r.readStates) != 0 {
        rd.ReadStates = r.readStates  // 附上 r.readStates
    }
    ...
    return rd
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Thu, 05 Jul 2018 13:46:43 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-3/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-3/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（概览篇）</title>
        <description>&lt;p&gt;这篇文章主要整体上介绍 etcd-raft 库，包括各个类的作用，类之间的串联。不涉及 raft 算法。先来看看 etcd-raft 几个结构体的定义：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;type raft struct {
    id uint64

    Term uint64
    Vote uint64

    // the log
    raftLog *raftLog

    state StateType

    // isLearner is true if the local raft node is a learner.
    isLearner bool

    votes map[uint64]bool

    msgs []pb.Message

    // the leader id
    lead uint64

    tick func()
    step stepFunc
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;type raftLog struct {
    // storage contains all stable entries since the last snapshot.
    storage Storage

    // unstable contains all unstable entries and snapshot.
    // they will be saved into storage.
    unstable unstable

    // committed is the highest log position that is known to be in
    // stable storage on a quorum of nodes.
    committed uint64
    // applied is the highest log position that the application has
    // been instructed to apply to its state machine.
    // Invariant: applied &amp;lt;= committed
    applied uint64
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// unstable.entries[i] has raft log position i+unstable.offset.
// Note that unstable.offset may be less than the highest log
// position in storage; this means that the next write to storage
// might need to truncate the log before persisting unstable.entries.
type unstable struct {
    // the incoming unstable snapshot, if any.
    snapshot *pb.Snapshot
    // all entries that have not yet been written to storage.
    entries []pb.Entry
    offset  uint64
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// node is the canonical implementation of the Node interface
type node struct {
    propc      chan msgWithResult
    recvc      chan pb.Message
    readyc     chan Ready
    advancec   chan struct{}
    tickc      chan struct{}
    done       chan struct{}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// Ready encapsulates the entries and messages that are ready to read,
// be saved to stable storage, committed or sent to other peers.
// All fields in Ready are read-only.
type Ready struct {
    // The current volatile state of a Node.
    // SoftState will be nil if there is no update.
    // It is not required to consume or store SoftState.
    *SoftState

    // The current state of a Node to be saved to stable storage BEFORE
    // Messages are sent.
    // HardState will be equal to empty state if there is no update.
    pb.HardState

    // ReadStates can be used for node to serve linearizable read requests locally
    // when its applied index is greater than the index in ReadState.
    // Note that the readState will be returned when raft receives msgReadIndex.
    // The returned is only valid for the request that requested to read.
    ReadStates []ReadState

    // Entries specifies entries to be saved to stable storage BEFORE
    // Messages are sent.
    Entries []pb.Entry

    // Snapshot specifies the snapshot to be saved to stable storage.
    Snapshot pb.Snapshot

    // CommittedEntries specifies entries to be committed to a
    // store/state-machine. These have previously been committed to stable
    // store.
    CommittedEntries []pb.Entry

    // Messages specifies outbound messages to be sent AFTER Entries are
    // committed to stable storage.
    // If it contains a MsgSnap message, the application MUST report back to raft
    // when the snapshot has been received or has failed by calling ReportSnapshot.
    Messages []pb.Message

    // MustSync indicates whether the HardState and Entries must be synchronously
    // written to disk or if an asynchronous write is permissible.
    MustSync bool
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;这几个结构体的关系如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/etcd-raft-node-2/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;RaftLog 的 &lt;code&gt;Storage&lt;/code&gt; 和 RaftNode 的 &lt;code&gt;raftStorage&lt;/code&gt; 都是指向同一个 Storage 对象（虚线表示指针）。Storage 在 kvstore 的示例中的实现为 MemoryStorage，可以理解为 WAL 的一个内存缓存。重启时会从 WAL 恢复 MemoryStorage 的数据。整个逻辑由 Node 的 &lt;code&gt;run&lt;/code&gt; 方法的 for loop 驱动，从 &lt;code&gt;recvc&lt;/code&gt; channel 接收请求，调用 raft 的 &lt;code&gt;Step&lt;/code&gt; 函数进行处理。&lt;code&gt;Step&lt;/code&gt; 函数会调用 &lt;code&gt;step&lt;/code&gt;，&lt;code&gt;step&lt;/code&gt; 是函数指针，在节点成为 leader 时将其设置为 &lt;code&gt;stepLeader&lt;/code&gt;，节点变成 follower 时设置为 &lt;code&gt;stepFollower&lt;/code&gt;。&lt;code&gt;step&lt;/code&gt; 处理 append 请求时，会调用 raftLog 的 &lt;code&gt;maybeAppend&lt;/code&gt; 方法，最终会把 entries append 到 &lt;code&gt;unstable&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;在 Node &lt;code&gt;run&lt;/code&gt; 方法的 for loop 中，会定期通过 &lt;code&gt;newReady&lt;/code&gt; 函数构造 Ready 对象。Ready 包括如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HardState 即 raft 节点的 persistent state &lt;/li&gt;
&lt;li&gt;SoftState 即 raft 节点的 volatile state &lt;/li&gt;
&lt;li&gt;CommittedEntries 即已经 commit 的 log entries，需要应用层 apply 到状态机&lt;/li&gt;
&lt;li&gt;Entries 即 unstable 中的 log entries（未落盘的 log entries）&lt;/li&gt;
&lt;li&gt;Snapshot 即需要持久化的 snapshot&lt;/li&gt;
&lt;li&gt;Messages 即 mailbox，所有还未发送的消息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;构造好的 &lt;code&gt;Ready&lt;/code&gt; 对象发送到 &lt;code&gt;readyc&lt;/code&gt; channel，RaftNode 取出后会做如下处理：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持久化 HardState、Entries、Snapshot 到 Storage 和 WAL (&lt;code&gt;raftStorage.ApplySnapshot()&lt;/code&gt;、&lt;code&gt;raftStorage.Append()&lt;/code&gt; 和 &lt;code&gt;wal.Save(rd.HardState, rd.Entries)&lt;/code&gt; 可以看出 memoryStorage 是 wal 的缓存，写 wal 的同时也写 memoryStorage)&lt;/li&gt;
&lt;li&gt;apply CommittedEntries 到状态机&lt;/li&gt;
&lt;li&gt;广播 Messages &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;处理完后调用 Node.Advance() 通知 Node Ready 对象处理完毕，准备好接收下一个。&lt;/p&gt;

&lt;p&gt;最后看看驱动整个逻辑的 &lt;code&gt;run&lt;/code&gt; 方法：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (n *node) run(r *raft) {
    var propc chan msgWithResult
    var readyc chan Ready
    var advancec chan struct{}
    var prevLastUnstablei, prevLastUnstablet uint64
    var havePrevLastUnstablei bool
    var prevSnapi uint64
    var rd Ready

    lead := None
    prevSoftSt := r.softState()
    prevHardSt := emptyState

    for {
        if advancec != nil {
            readyc = nil
        } else {
            // 应用层通知上一个 ready 对象已经处理完毕了 此时 advancec 为 nil 
            rd = newReady(r, prevSoftSt, prevHardSt)
            if rd.containsUpdates() { // 有更新才把 readyc 设为 非空
                readyc = n.readyc
            } else {
                readyc = nil
            }
        }

        select {
        case m := &amp;lt;-n.recvc:
            // filter out response message from unknown From.
            if pr := r.getProgress(m.From); pr != nil || !IsResponseMsg(m.Type) {
                r.Step(m)
            }
        case &amp;lt;-n.tickc:
            r.tick()
        case readyc &amp;lt;- rd:
            if rd.SoftState != nil {
                prevSoftSt = rd.SoftState
            }
            if len(rd.Entries) &amp;gt; 0 {
                prevLastUnstablei = rd.Entries[len(rd.Entries)-1].Index
                prevLastUnstablet = rd.Entries[len(rd.Entries)-1].Term
                havePrevLastUnstablei = true
            }
            if !IsEmptyHardState(rd.HardState) {
                prevHardSt = rd.HardState
            }
            if !IsEmptySnap(rd.Snapshot) {
                prevSnapi = rd.Snapshot.Metadata.Index
            }

            r.msgs = nil
            r.readStates = nil
            advancec = n.advancec
        case &amp;lt;-advancec:
            if prevHardSt.Commit != 0 {
                r.raftLog.appliedTo(prevHardSt.Commit)
            }
            // 应用层处理完了 表示 unstable 的东西不需要了 该清理就清理
            if havePrevLastUnstablei {
                r.raftLog.stableTo(prevLastUnstablei, prevLastUnstablet)
                havePrevLastUnstablei = false
            }
            r.raftLog.stableSnapTo(prevSnapi)
            advancec = nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;还有构造 Ready 对象的 &lt;code&gt;newReady&lt;/code&gt; 函数：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func newReady(r *raft, prevSoftSt *SoftState, prevHardSt pb.HardState) Ready {
    rd := Ready{
        Entries:          r.raftLog.unstableEntries(),
        CommittedEntries: r.raftLog.nextEnts(),
        Messages:         r.msgs,
    }
    if softSt := r.softState(); !softSt.equal(prevSoftSt) {
        rd.SoftState = softSt
    }
    if hardSt := r.hardState(); !isHardStateEqual(hardSt, prevHardSt) {
        rd.HardState = hardSt
    }
    if r.raftLog.unstable.snapshot != nil {
        rd.Snapshot = *r.raftLog.unstable.snapshot
    }
    if len(r.readStates) != 0 {
        rd.ReadStates = r.readStates
    }
    rd.MustSync = MustSync(rd.HardState, prevHardSt, len(rd.Entries))
    return rd
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Wed, 04 Jul 2018 13:33:35 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-2/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-2/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
  </channel>
</rss>
