<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Masutangu</title>
    <description>也許我這一生　始終在追逐那顆九號球</description>
    <link>http://masutangu.com/</link>
    <atom:link href="http://masutangu.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 11 Jan 2018 09:44:48 +0800</pubDate>
    <lastBuildDate>Thu, 11 Jan 2018 09:44:48 +0800</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>MIT 6.824 学习笔记（四）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;ZooKeeper&lt;/h1&gt;

&lt;h2&gt;Abstract&lt;/h2&gt;

&lt;p&gt;ZooKeeper 旨在提供简单高效的内核以供客户端实现更复杂的 coordination primitives。In addition to the wait-free
property, ZooKeeper provides a &lt;strong&gt;per client guarantee of FIFO execution of requests&lt;/strong&gt; and &lt;strong&gt;linearizability for all requests that change the ZooKeeper state&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;为了保证状态更新操作（写请求）的 linearizability，ZooKeeper 实现了 &lt;strong&gt;Zab，一个基于 leader 的原子广播协议&lt;/strong&gt;。In ZooKeeper, &lt;strong&gt;servers process read operations locally, and we do not use Zab to totally order them&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在客户端缓存数据是提高读性能的重要技术，&lt;strong&gt;ZooKeeper 提供了 watch 机制&lt;/strong&gt;，不直接管理客户端缓存。&lt;/p&gt;

&lt;h2&gt;Service overview&lt;/h2&gt;

&lt;p&gt;ZooKeeper 提供给客户端 znode 的抽象，znode 有下列两种类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regular&lt;/strong&gt; : 由客户端显式创建和删除&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ephemeral&lt;/strong&gt;: 由客户端创建，可以由客户端显式删除，当会话终止时系统也会自动删除&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ZooKeeper 实现了 watch 机制，当数据发生改变时通知客户端，而不必通过让客户端轮询服务器的方式。&lt;/p&gt;

&lt;p&gt;客户端连接到 ZooKeeper 时会初始化一个 session。Session 有超时机制，当 ZooKeeper 在超时时间内没有收到来自客户端 session的任何信息时，会判定该客户端已经挂掉。&lt;/p&gt;

&lt;h2&gt;ZooKeeper guarantees&lt;/h2&gt;

&lt;p&gt;ZooKeeper 具备以下两个基础保证：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linearizable writes&lt;/strong&gt;: all requests that update the state of ZooKeeper are serializable and respect precedence;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;FIFO client order&lt;/strong&gt;: all requests from a given client are executed in the order that they were sent by the client.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Examples of primitives&lt;/h2&gt;

&lt;h3&gt;Simple Locks without Herd Effect&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Lock
1 n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if n is lowest znode in C, exit
4 p = znode in C ordered just before n
5 if exists(p, true) wait for watch event
6 goto 2

Unlock
1 delete(n)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Write Lock
1 n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if n is lowest znode in C, exit
4 p = znode in C ordered just before n
5 if exists(p, true) wait for event
6 goto 2

Read Lock
1 n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if no write znodes lower than n in C, exit
4 p = write znode in C ordered just before n
5 if exists(p, true) wait for event
6 goto 3
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h2&gt;ZooKeeper Implementation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-4/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;收到请求时，由 request processor 处理。如果是写请求，则使用 zab 协议，最终提交到 ZooKeeper 数据库的修改将会被复制到该系统上所有的服务器。&lt;/p&gt;

&lt;p&gt;复制数据库是一个内存数据库，&lt;strong&gt;在变更被应到到内存数据库之前，我们强制将更新记录刷到磁盘上以实现 recoverability&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;每个 ZooKeeper 服务器都接收处理来自客户端的请求。 读操作由每个服务器的本地数据库来处理，写请求则由 zab 协议处理。&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Request Processor&lt;/h3&gt;

&lt;p&gt;与客户端发送的请求不同，&lt;strong&gt;事务是幂等的&lt;/strong&gt;。When the leader receives a write request, it calculates what the state of the system will be when the write is applied and transforms it into a transaction that captures this new state. The future state must be calculated because there may be outstanding transactions that have not yet been applied to the database. （尽管 transaction 可能执行多次，但通过计算 state 的方式，且保证多次执行的 transaction 也按照原先的顺序，就能保证幂等性）&lt;/p&gt;

&lt;h3&gt;Atomic Broadcast&lt;/h3&gt;

&lt;p&gt;所有更新 ZooKeeper 状态的请求都被转发到 leader。 Leader 执行请求并通过 Zab，一个原子广播协议广播该变更。Zab 采用majority quorums 来决定一个建议，因此 ZooKeeper 在 2F + 1 服务器的场景下最多可以容忍 F 台服务器故障。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Because state changes depend on the application of previous state changes, Zab provides stronger order guarantees than regular atomic broadcast.&lt;/strong&gt; More specifically, Zab guarantees that &lt;strong&gt;changes broadcast by a leader are delivered in the order they were sent&lt;/strong&gt; and &lt;strong&gt;all changes from previous leaders are delivered to an established leader before it broadcasts its own changes&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Replicated Database&lt;/h3&gt;

&lt;p&gt;重放消息以恢复状态非常耗时，因此 ZooKeeper 定期进行快照，恢复时只需要重放快照后的消息。 为了不锁住状态，ZooKeeper 采用&lt;strong&gt;模糊快照&lt;/strong&gt;的方式。对树进行深度优先扫描，原子读取每个 znode 的数据和元数据并写入磁盘。Since the resulting fuzzy snapshot may have applied some subset of the state changes delivered during the generation of the snapshot, the result may not correspond to the state of ZooKeeper at any point in time. However, since state changes are idempotent, we can apply them twice as long as we apply the state changes in order.&lt;/p&gt;

&lt;h3&gt;Client-Server Interactions&lt;/h3&gt;

&lt;p&gt;读请求由服务器本地处理。每个读请求都被一个 zxid 标记，表示服务器看到的最后一个事务。本地读取的方式提高了性能，但可能返回旧数据。客户端可以在读操作后调用 sync 来保证返回的是最新的值。In our implementation, we do not need to atomically broadcast sync as we use a leader-based algorithm, and we simply place the sync operation at the end of the queue of requests between the leader and the server executing the call to sync. In order for this to work, the follower must be sure that the leader is still the leader. If there are pending transactions that commit, then the server does not suspect the leader. If the pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction.&lt;/p&gt;

&lt;p&gt;ZooKeeper 服务器以 FIFO 的顺序来处理客户端的请求。Response 包含了 zxid。Even heartbeat messages during intervals of no activity include the last zxid seen by the server that the client is connected to. If the client connects to a new server, that new server ensures that its view of the ZooKeeper data is at least as recent as the view of the client by checking the last zxid of the client against its last zxid. If the client has a more recent view than the server, the server does not reestablish the session with the client until the server has caught up.&lt;/p&gt;

&lt;h1&gt;Linearizability versus Serializability&lt;/h1&gt;

&lt;p&gt;注：摘自 &lt;a href=&quot;http://www.bailis.org/blog/linearizability-versus-serializability/&quot;&gt;Linearizability versus Serializability&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Linearizability: single-operation, single-object, real-time order&lt;/h2&gt;

&lt;p&gt;Linearizability is a guarantee about single operations on single objects. It provides a real-time  guarantee on the behavior of a set of single operations.&lt;/p&gt;

&lt;p&gt;In plain English, under linearizability, writes should appear to be instantaneous. Imprecisely, once a write completes, all later reads (where “later” is defined by wall-clock start time) should return the value of that write or the value of a later write. Once a read returns a particular value, all later reads should return that value or the value of a later write.&lt;/p&gt;

&lt;p&gt;Linearizability for read and write operations is synonymous with the term “atomic consistency” and is the “C,” or “consistency,” in Gilbert and Lynch’s proof of the CAP Theorem. We say linearizability is composable (or “local”) because, if operations on each object in a system are linearizable, then all operations in the system are linearizable.&lt;/p&gt;

&lt;h2&gt;Serializability: multi-operation, multi-object, arbitrary total order&lt;/h2&gt;

&lt;p&gt;Serializability is a guarantee about transactions, or groups of one or more operations over one or more objects. It guarantees that the execution of a set of transactions (usually containing read and write operations) over multiple items is equivalent to some serial execution (total ordering) of the transactions.&lt;/p&gt;

&lt;p&gt;Serializability is the traditional “I,” or isolation, in ACID. If users’ transactions each preserve application correctness (“C,” or consistency, in ACID), a serializable execution also preserves correctness. Therefore, serializability is a mechanism for guaranteeing database correctness.&lt;/p&gt;

&lt;p&gt;Serializability is not composable. Serializability does not imply any kind of deterministic order—it simply requires that some equivalent serial execution exists.&lt;/p&gt;

&lt;p&gt;关于 Serializability 和 Linearizability，可以读读&lt;a href=&quot;https://36kr.com/p/5037166.html&quot;&gt;分布式系统一致性的发展历史&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Transaction Management in the R* Distributed Database Management System&lt;/h1&gt;

&lt;p&gt;这篇论文的主题是关于分布式数据库系统的事务管理，着重描述了 R* 提交协议：Presumed Abort（PA）和 Presumed Commit（PC）。PA 和 PC 是著名的 two-phase（2P）提交协议的扩展。&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;R* is an experimental, distributed database management system (DDBMS). In a distributed database
system, the actions of a transaction (an atomic unit of consistency and recovery) may occur at more than one site. &lt;strong&gt;A commit protocol is needed to guarantee the uniform commitment of distributed transaction executions.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Some of the desirable characteristics in a commit protocol are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;guaranteed transaction atomicity always&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ability to “forget” outcome of commit processing after a short amount of time（不需要一直记录结果）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;minimal overhead in terms of log writes and message traffic&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;optimized performance in the no-failure case&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;exploitation of completely or partially read-only transactions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;maximizing the ability to perform unilateral aborts&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Multilevel hierarchical commit protocols are suggested to be more natural than the conventional two-level (one coordinator and a set of subordinates) protocols.&lt;/strong&gt; With these goals in mind, we extended the conventional 2P commit protocol to support a tree of processes and defined the &lt;strong&gt;Presumed Abort (PA)&lt;/strong&gt; and the &lt;strong&gt;Presumed Commit (PC)&lt;/strong&gt; protocols to improve the performance of distributed transaction commit. &lt;/p&gt;

&lt;h2&gt;The Two-Phase Commit Protocol&lt;/h2&gt;

&lt;p&gt;在 2P 提交协议中，分布式事务执行模型包括了一个 coordinator 进程，它连接了客户端应用程序和其他称为 subordinates 的进程。Subordinates 进程彼此之间不交互，只和 coordinator 交互。每个事务分配一个全局唯一的名字。&lt;/p&gt;

&lt;h3&gt;2P Under Normal Operation&lt;/h3&gt;

&lt;p&gt;当 coordinator 接收到来自用户的事务提交请求时，并行的发送 PREPARE 消息给 subordinates，此时进入提交协议的第一阶段。如果该事务可以提交，subordinate 先 force-write 一条 prepare log record，之后发送 YES VOTE 给 coordinator，再等待 coordinator 返回最后的决策（commit 还是 abort），之后 subordinate 进入 &lt;strong&gt;prepare&lt;/strong&gt; 状态，无法单方面终止或提交事务。如果 subordinate 决定终止事务，先 force-write 一条 abort record，之后发送 NO VOTE 给 coordinator。因为 NO VOTE 表示否决，subordinate 不需要等待 coordinator，直接终止该事务，释放锁并“忘记”该事务（不需要记录任何信息了）。&lt;/p&gt;

&lt;p&gt;当 coordinator 接收到所有 subordinates 的投票时，进入提交协议的第二阶段。如果所有投票都是 YES，则 coordinator 进入 &lt;strong&gt;committing&lt;/strong&gt; 状态：首先 force-write 一条 commit record，发送 COMMIT 消息给所有 subordinates. 在 force-write 成功执行后，事务即可提交，且通知客户端事务已经成功提交。如果 coordinator 有收到 NO VOTE，则进入 &lt;strong&gt;aborting&lt;/strong&gt; 状态：force-write 一条 abort record，发送 ABORT 消息给处于准备状态的或者没有回 PREPARE 消息的 subordinates。&lt;/p&gt;

&lt;p&gt;如果 subordinate 收到 COMMIT 消息，则进入 &lt;strong&gt;committing&lt;/strong&gt; 状态：先 force-write 一条 commit record，发送 ACK 消息给 coordinator，然后提交事务并且“忘记”该事务。如果收到的是 ABORT 消息，进入 &lt;strong&gt;aborting&lt;/strong&gt; 状态：force-write 一条 abort record，发送 ACK 消息给 coordinator，然后终止事务并且“忘记”该事务。当 coordinator 收到所有 subordinates 的 ACK 消息（不包括 NO VOTE 的 subordinates），写一条 end record 然后“忘记”该事务。&lt;/p&gt;

&lt;p&gt;The general principle on which the protocols described in this paper are based is that if a subordinate acknowledges the receipt of any particular message, then it should make sure (by forcing a log record with the information in that message before sending the ACK) that it will never ask the coordinator about that piece of information. &lt;/p&gt;

&lt;p&gt;log 包含了如下内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;the type (prepare, end, etc.) of the record&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the identity of the process that writes the record&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the name of the transaction&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the identity of the coordinator&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the names of the exclusive locks held by the writer in the case of prepare records&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the identities of the subordinates in the case of the commit/abort records written by the coordinator&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结下，提交事务总共需要：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;subordinate 写两条记录：prepare record 和 commit record，且发送两条消息：YES VOTE 和 ACK&lt;/li&gt;
&lt;li&gt;coordinator 发送两条消息：PREPARE 和 COMMIT，以及写两条记录：commit record（force-write）和 end record（非 force-write） &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;2P and Failures&lt;/h3&gt;

&lt;p&gt;每个站点都存在一个 recovery 进程，处理来自其他站点的 recovery 进程的信息。当从 crash 中恢复时，recovery 进程读取 stable storage 的 log，并且在 virtual storage 中重建。在 virtual storage 的这部分信息有下列用途：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应答其他站点发来的事务查询请求，这些事务的 coordinators 是运行在该挂掉的站点上（answer queries from other sites about transactions that had their coordinators at this site）&lt;/li&gt;
&lt;li&gt;发送事务的查询信息给其他站点的 subordinates，这些事务的 coordinators 是运行在该挂掉的站点上（send unsolicited information to other sites that had subordinates for transactions that had their coordinators at this site）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 virtual storage 构建的好处在于可以快速回应其他站点的查询，而不需要从 stable storage 查询日志。&lt;/p&gt;

&lt;p&gt;当 recovery 进程发现有事务处于 &lt;strong&gt;prepare&lt;/strong&gt; 状态，则会定期向 coordinator 询问该事务应该如何处理（commit 还是 abort）。如果 recovery 进程发现有事务在执行过程中崩溃，没有留下任何日志，则直接回滚操作，写一条 abort record，然后“忘记”。&lt;/p&gt;

&lt;p&gt;如果 recovery 进程发现事务处于 &lt;strong&gt;committing/aborting&lt;/strong&gt; 状态，它将尝试定期发送 COMMIT/ABORT 给所有还没 ACK 过的 subordinates。一旦收到所有 ACK，则写一条 end record 并且“忘记”该事务。&lt;/p&gt;

&lt;p&gt;如果 recovery 进程收到一条事务查询，但 virtual storage 中没有该事务的信息，则直接返回 ABORT。&lt;/p&gt;

&lt;h2&gt;The Presumed Abort Protocol&lt;/h2&gt;

&lt;p&gt;上一节提到，在找不到关于事务的信息时，recovery 进程将返回 ABORT。这意味着当 coordinator 决定要 abort 事务时，可以直接“忘记”该事务，coordinator 和 subordinates 不再需要 force-write 一条 abort record，subordinates 也不需要 ACK 来自 coordinator 的 ABORT 消息。coordinator 也不在需要记录 end record 了。&lt;/p&gt;

&lt;p&gt;只读事务的情况下，leaf 进程直接返回 READ VOTE，释放锁，且“忘记”该事务。 A nonroot, nonleaf sends a READ VOTE only if its own vote and those of its subordinates’ are also READ VOTES. Otherwise, as long as none of the latter is a NO VOTE, it sends a YES VOTE.（存在部分只读事务）&lt;/p&gt;

&lt;p&gt;There will not be a second phase of the protocol if the root process is readonly and it gets only READ VOTES. In this case the root process, just like the other processes, writes no log records for the transaction. &lt;/p&gt;

&lt;p&gt;总结下，完全只读事务，所有进程都不需要写任何 log record。每个 nonleaf 进程发送一条 PREPARE 消息，每个 nonroot 进程发送一条信息 READ VOTE 消息。&lt;/p&gt;

&lt;h2&gt;Note&lt;/h2&gt;

&lt;p&gt;What about concurrent transactions?&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;x and y are bank balances
x and y start out as $10

T1 is doing a transfer of $1 from x to y
T1:
  add(x, 1)  -- server A
  add(y, -1) -- server B
T2:
  tmp1 = get(x)
  tmp2 = get(y)
  print tmp1, tmp2
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;Problem: &lt;strong&gt;What if T2 runs between the two add() RPCs?&lt;/strong&gt; Then T2 will print 11, 10 money will have been created!  T2 should print 10,10 or 9,11.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The traditional correctness definition is &amp;quot;serializability&amp;quot;.&lt;/strong&gt;  Results should be as if transactions ran one at a time in some order as if T1, then T2; or T2, then T1. The results for the two differ, either is OK.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&amp;quot;Two-phase locking&amp;quot; is one way to implement serializability&lt;/strong&gt;. Each database record has a lock. The lock is stored at the server that stores the record. Transaction must wait for and acquire a record&amp;#39;s lock before using it. Thus add() handler implicitly acquires lock when it uses record. x or y transaction holds its locks until &lt;em&gt;after&lt;/em&gt; commit or abort.&lt;/p&gt;

&lt;p&gt;What are locks really doing?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When transactions conflict, locks delay one to force serial execution.&lt;/li&gt;
&lt;li&gt;When transactions don&amp;#39;t conflict, locks allow fast parallel execution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft and two-phase commit solve different problems!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use Raft to get high availability by replicating&lt;/strong&gt;, i.e. to be able to operate when some servers are crashed. The servers all do the &lt;em&gt;same&lt;/em&gt; thing&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use 2PC when each subordinate does something different&lt;/strong&gt; and &lt;strong&gt;all of them must do their part&lt;/strong&gt;. &lt;strong&gt;2PC does not help availability&lt;/strong&gt; since all servers must be up to get anything done. &lt;strong&gt;Raft does not ensure that all servers do something&lt;/strong&gt; since only a majority have to be alive.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What if you want &lt;strong&gt;high availability and distributed commit&lt;/strong&gt;?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each &amp;quot;server&amp;quot; should be a Raft-replicated service&lt;/li&gt;
&lt;li&gt;And the TC should be Raft-replicated&lt;/li&gt;
&lt;li&gt;Run two-phase commit among the replicated services&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then you can tolerate failures and still make progress.&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Jan 2018 19:53:45 +0800</pubDate>
        <link>http://masutangu.com/2018/01/mit-6824-note-4/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/01/mit-6824-note-4/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>再见，2017</title>
        <description>&lt;p&gt;回看起去年的总结 &lt;a href=&quot;http://masutangu.com/2016/12/review/&quot;&gt;我的2016&lt;/a&gt;，心里挺有感慨的。17 年是出现转机的一年，在这最后一天，按照惯例，整理下这一年的历程和想法。&lt;/p&gt;

&lt;h1&gt;关于自己&lt;/h1&gt;

&lt;p&gt;在去年的时候，我总觉得自己在工作上挺失意的，各种原因下达不到自己的期望。还好，今年在工作方面对自己还算满意。&lt;strong&gt;最大的进步是对工作的心态上，不再有一些困惑和纠结。&lt;/strong&gt;工作上努力去争取有挑战的需求，空闲的时候则抽更多的时间在个人学习上，维持好工作和自学的平衡。我挺喜欢这样的节奏，忙的时候全心全意把工作做好，不忙的时候就把更多的时间投入到自己感兴趣的领域。&lt;/p&gt;

&lt;p&gt;在经历 16 年那段纠结的时间，如今自己努力一点点扭转，我也更加相信，&lt;strong&gt;在任何时候都要把主动权握在自己手里&lt;/strong&gt;。不管自己处在一个什么样的环境，对你是有利或者是不利的、有没有机遇、公不公平，&lt;strong&gt;把自己能控制的做好，在任何时候都会有选择权&lt;/strong&gt;。但另一方面，环境也是至关重要的一个因素，是金子总会发光，我觉得是悖论。或者说，&lt;strong&gt;是金子的话，应该主动寻找能让自己发光的机会&lt;/strong&gt;，而不是呆在沙堆中，等着别人来发掘你，该争取的东西就一定要去争取。&lt;/p&gt;

&lt;p&gt;个人学习上，下半年的主要精力都放在分布式系统上，跟着 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 的课程，读论文，做实验，记&lt;a href=&quot;http://masutangu.com/2017/11/mit-6824-note-1/&quot;&gt;笔记&lt;/a&gt;。这个课程受益匪浅，有些思路我也能顺利运用到工作上，这样的感觉挺不错的。&lt;/p&gt;

&lt;h1&gt;关于团队&lt;/h1&gt;

&lt;p&gt;平时喜欢篮球，也经常关注 NBA。在我看来，团队氛围和文化及其重要。勇士、马刺的成功，和他们的球队文化、管理层的理念是分不开的。&lt;strong&gt;球队的凝聚力、球员之间的化学反应，是无法体现在冷冰冰的数据上的。&lt;/strong&gt;读到这篇文章&lt;a href=&quot;https://zhuanlan.zhihu.com/p/31997176&quot;&gt;《你可能见证了勇士队的成功，却很少了解他们背后的球队文化》&lt;/a&gt;的时候，我内心有点被触动：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;即便克拉克离开了勇士队，勇士的主场球馆内依然挂着他身穿勇士球衣的照片，而克拉克自己也表示，虽然自己不在勇士，但是还是会和这些球员保持紧密的关系。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一支球队中，有不同的角色：全明星、先发、替补、饮水机管理员。但相信我，&lt;strong&gt;无论你在球队中扮演什么样的角色，每个人都希望自己是球队的一部分，是比赛的一部分，每个人都希望球队的胜利，也有自己贡献的那部分因素。&lt;/strong&gt;在这一年，我参与了项目组新项目的研发，大家一起对需求、讨论方案、前后台定协议、联调、改bug，没有什么撕逼、甩锅，我感觉到自己在团队中的角色，我知道自己能做些什么，这些都让我更加有动力去扮演好团队中的一份子。&lt;/p&gt;

&lt;h1&gt;一些不足&lt;/h1&gt;

&lt;p&gt;一直以来，我都比较难以保持稳定的状态。有时状态好的时候，就一头扎进去。前阵子有一段时间，每天都 6 点就自然醒，那段时间就天天很早到公司，吃完早餐后开始读论文，就这样每天都抽更多的时间去学习。但持续了大概一个月之后就累垮了，低迷了一两周。而且状态低迷的时候，自己也经常会焦虑，导致有些恶性循环。新的一年，希望自己能维持稳定些的状态，对自己也更有耐心些。&lt;strong&gt;细水长流&lt;/strong&gt;，做长远打算。&lt;/p&gt;

&lt;h1&gt;新年展望&lt;/h1&gt;

&lt;p&gt;新的一年，希望自己能在&lt;strong&gt;工作上能有更大的突破&lt;/strong&gt;，个人学习上继续按自己规划的路线走，&lt;strong&gt;扩展更多领域的知识，提升自己的核心竞争力&lt;/strong&gt;。还是相信那句话：&lt;strong&gt;Pursue excellence, and success will follow！&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 31 Dec 2017 16:28:17 +0800</pubDate>
        <link>http://masutangu.com/2017/12/2017-review/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/12/2017-review/</guid>
        
        
        <category>随笔</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（三）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;Spinnaker&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Spinnaker is an experimental datastore that is designed to run on a large cluster of commodity servers in a single datacenter. This paper describes Spinnaker’s Paxos-based replication protocol. The use of Paxos ensures that a data partition in Spinnaker will be available for reads and writes as long a majority of its replicas are alive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;实现持续可用性的一个解决方案是&lt;strong&gt;主从复制&lt;/strong&gt;，但主从复制存在以下缺陷：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在上图的例子中，主从节点都从 LSN=10 开始（a），之后 slave 节点挂了（b），master 节点继续接收写请求，一直到 LSN=20。之后 master 节点也挂了（c），之后 slave 节点恢复（d），然而，在此时 slave 节点不能接收任何读写请求因为它缺失了 LSN=11 到LSN＝20 之间的记录。如果要避免这种情况，只有在任意节点挂掉的时候，都阻塞写请求。但这样就降低了整个系统的 availability。&lt;/p&gt;

&lt;p&gt;分布式系统中，&lt;strong&gt;一致性模型描述了如何使不同的 relicas 保持同步&lt;/strong&gt;。&lt;strong&gt;强一致性&lt;/strong&gt;保证了所有的 replicas 都是一致的，但要实现强一致性需要牺牲 availability 或网络分区容忍性。CAP 理论提出 &lt;strong&gt;Consistency&lt;/strong&gt;、&lt;strong&gt;Availability&lt;/strong&gt; 和 &lt;strong&gt;Partition tolerance&lt;/strong&gt; 三者最多只能同时满足两项。&lt;/p&gt;

&lt;p&gt;比如 Dynamo 这样的系统，使用&lt;strong&gt;最终一致性&lt;/strong&gt;模型来提供高可用性和分区容忍性。Dynamo 是一个 AP 系统，牺牲了 Consistency。&lt;/p&gt;

&lt;p&gt;Spinnaker 使用基于 Paxos 的协议来实现日志提交和故障恢复。Paxos 确保了系统在大多数节点存活的情况下可以运作。Spinnaker 是一个 CA 系统，用于单一的 datacenter，并使用另外的 replication strategy 来保证跨 datacenter 的容错性。 &lt;/p&gt;

&lt;h2&gt;Related work&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Two-phase commit (2PC)&lt;/strong&gt; 是保持 replicas 一致的一种方式。但 2PC 更偏向于将每个 participant 当作一个独立的资源管理者，而不仅仅是 replica。使用 2PC 来实现 replication 有些 overkill，并且还有不少缺陷。首先单一节点失败会导致系统 abort。其次每个 transaction 都发起 2PC 会导致极差的性能。每次 2PC 都需要两次磁盘强制写和传输两条信息的时延。最后，2PC 在 coordinator 挂掉时无法运作。&lt;/p&gt;

&lt;p&gt;Amazon 的 Dynamo 通过&lt;strong&gt;分布式时钟&lt;/strong&gt;来解决最终一致性的问题。&lt;/p&gt;

&lt;p&gt;Google 的 Bigtable 提供了强一致性，和 spinnaker 不同的是，Bigtable 依赖 GFS 来存储数据和日志，还有实现 replication。这样每个 transaction 的 workload 就加重了（需要和 gfs 的 master 交互）。&lt;/p&gt;

&lt;h2&gt;Architecture&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Like Bigtable and PNUTS, Spinnaker distributes the rows of a table across its cluster using range partitioning. Each node is assigned a base key range, which is replicated on the next N − 1 nodes (N = 3 by default).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Each group of nodes involved in replicating a key range is denoted as a &lt;strong&gt;cohort&lt;/strong&gt;. Note that cohorts overlap.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个日志由一个 LSN 唯一的标记。Commit queue 是在内存的数据结构，用于存放 pending writes。写操作只有在接收到大多数 cohort 的 ack 之后才能提交。在此之前都存放在 commit queue 中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-3.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;已经提交的写操作存于 memtable 中，并被定期刷到被称为 SSTable 的 immutable disk structure。SSTable 会被定期合并以提升读性能并删除不需要的数据。&lt;/p&gt;

&lt;h2&gt;The replication protocol&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-4.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;提交一个写操作需要三次日志强制写和四条信息交互，不过大多数操作都是重叠的（可以并行）。&lt;/p&gt;

&lt;h2&gt;Recovery&lt;/h2&gt;

&lt;p&gt;Follower 的恢复需要两个阶段：&lt;strong&gt;local recovery&lt;/strong&gt; 和 &lt;strong&gt;catch up&lt;/strong&gt;。定义 f.cmt 表示 follower 的最后一个提交日志的 LSN，f.lst 表示 follower 的最后一个日志的 LSN。Local recovery 阶段，follower 从最近一次 checkpoint 开始重新执行日志直到 f.cmt，之后进入 catch up 阶段。Catch up 阶段，follower 通知 leader 自己的 f.cmt，leader 回复 f.cmt 之后所有的 commit writes。Leader 将会阻塞所有新的写请求直到 follower 已经跟上。&lt;/p&gt;

&lt;p&gt;当 leader 挂掉，新的 leader 将被选举，并且会确保新的 leader 会包含所有已提交的写操作。在老的 leader 挂掉时，有可能其提交的写操作在某些 followers 还处于 pending 的状态。新 leader 将使用下图的算法，继续提交所有 unresolved 写操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-5.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Leader election&lt;/h2&gt;

&lt;p&gt;选举算法如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-6.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Dynamo&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;一个系统的&lt;strong&gt;可靠性&lt;/strong&gt;和&lt;strong&gt;可扩展性&lt;/strong&gt;取决于如何管理应用状态。Amazon 采用一种高度去中心化，松耦合，由数百个服务组成的面向服务架构。在这种环境中尤其需要一个高可用的存储技术。&lt;/p&gt;

&lt;p&gt;Dynamo 被设计用于管理服务状态，要求具备非常高的可靠性，而且需要严格控制&lt;strong&gt;可用性、一致性、成本效益和性能&lt;/strong&gt;的之间的权衡。&lt;/p&gt;

&lt;p&gt;Amazon 服务平台中许多服务只需要主键访问数据存储。通常关系数据库模式会导致效率低下，且可扩展性和可用性也有限。Dynamo 提供了一个简单且唯一的主键接口，以满足这些应用的要求。&lt;/p&gt;

&lt;p&gt;Dynamo 综合一些著名的技术来实现可伸缩性和可用性：使用&lt;strong&gt;一致性哈希&lt;/strong&gt;划分和复制数据，一致性由&lt;strong&gt;对象版本&lt;/strong&gt;来实现。更新时副本之间的一致性是由&lt;strong&gt;类似仲裁（quorum-like）&lt;/strong&gt;的技术和&lt;strong&gt;去中心化的副本同步协议&lt;/strong&gt;来保证。Dynamo 采用了&lt;strong&gt;基于 gossip&lt;/strong&gt; 的分布式故障检测和 membership protocol。&lt;/p&gt;

&lt;h2&gt;System Assumptions and Requirements&lt;/h2&gt;

&lt;p&gt;Dynamo 的目标应用程序是通过较弱的一致性（ACID中的“C”）来达到高可用性。Dynamo 不提供任何数据隔离（ACID中的“I”）保证，只允许单一主键更新。&lt;/p&gt;

&lt;h2&gt;Design Considerations&lt;/h2&gt;

&lt;p&gt;对于容易出现服务器和网络故障的系统，可使用&lt;strong&gt;乐观复制&lt;/strong&gt;来提高系统的可用性，数据变化允许在后台发送到副本，同时容忍网络断开。这种做法带来的挑战是如何检测和协调由此引发的冲突。协调的过程引入了两个问题：&lt;strong&gt;何时协调&lt;/strong&gt;以及&lt;strong&gt;谁负责协调&lt;/strong&gt;。Dynamo 被设计成&lt;strong&gt;最终一致性（eventually consistent）&lt;/strong&gt;的数据存储，所有的更新操作最终会分发到所有副本。&lt;/p&gt;

&lt;p&gt;一个重要的设计考虑的因素是何时去协调更新操作冲突，例如是在读还是写过程中协调冲突。许多传统 data store 在写的过程中协调冲突，从而保持读的复杂度相对简单。另一方面，&lt;strong&gt;Dynamo 的目标是“永远可写”&lt;/strong&gt;。对于 Amazon 许多服务来讲，拒绝客户的更新操作可能导致糟糕的客户体验。这迫使我们&lt;strong&gt;将协调冲突的复杂性推给“读”&lt;/strong&gt;，以确保“写”永远不会拒绝。&lt;/p&gt;

&lt;p&gt;其他重要的设计原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;增量的可扩展性（Incremental scalability）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dynamo 需要能够水平扩展一台存储主机，而对系统操作者和系统本身的影响很小。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;对称性（Symmetry）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;每个 Dynamo 节点应该有一样的责任，不应该存在有区别或具备特殊的角色或额外的责任的节点。根据我们的经验，对称性(symmetry)简化了系统的配置和维护。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;去中心化（Decentralization）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;是对对称性的延伸，设计应采用有利于去中心化而不是集中控制的技术。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;异质性（Heterogeneity）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;系统必须能够利用其运行所在的基础设施的异质性。例如，负载的分配必须与各个独立的服务器的能力成比例。这样就可以一次只增加一个高处理能力的节点，而无需一次升级所有的主机。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Partitioning Algorithm&lt;/h2&gt;

&lt;p&gt;Dynamo 的关键设计要求之一是&lt;strong&gt;增量可扩展性&lt;/strong&gt;，这需要一个机制来将数据动态划分到系统中的节点上。Dynamo 的分区方案依赖于一致哈希。&lt;strong&gt;一致性哈希的主要优点是节点的新增或减少只影响其最直接的邻居&lt;/strong&gt;，而对其他节点没影响。&lt;/p&gt;

&lt;p&gt;基本的一致性哈希算法存在一些不足：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;环上​​的节点随机分配位置导致数据和负载的不均匀分布&lt;/li&gt;
&lt;li&gt;基本算法无法满足节点性能的&lt;strong&gt;异质性（Heterogeneity）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决这些问题，&lt;strong&gt;Dynamo 引入虚拟节点&lt;/strong&gt;：每个节点被分配到环上多点，而不是只映射到环上的一个单点。系统中每个节点对多个虚拟节点负责。&lt;/p&gt;

&lt;p&gt;使用虚拟节点具有以下优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果一个节点不可用，这个节点的负载将均匀地分散给其余的可用节点&lt;/li&gt;
&lt;li&gt;当一个节点再次可用，或者是新添加到系统中，the newly available node accepts a roughly equivalent amount of load from each of the other available nodes. &lt;/li&gt;
&lt;li&gt;一个节点负责的虚拟节点数目由其处理能力来决定（heterogeneity）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Replication&lt;/h2&gt;

&lt;p&gt;每个键 K 被分配到一个协调器节点。协调器节点负责其管辖范围内的数据的复制。除了在本地存储每个 key 外，协调器节点也将 key 复制到环上顺时针方向的 N-1 后继节点。系统中每个节点负责环上的从其自己到第 N 个前继节点间的区域。&lt;/p&gt;

&lt;p&gt;负责存储一个特定的键的节点列表被称为&lt;strong&gt;首选列表&lt;/strong&gt;。&lt;/p&gt;

&lt;h2&gt;Data Versioning&lt;/h2&gt;

&lt;p&gt;Dynamo 提供最终一致性，允许更新操作异步地传播到所有副本。put 调用可能在更新操作被所有的副本执行之前就返回给调用者，这会导致在随后的 get 操作可能会返回一个不是最新的对象。&lt;/p&gt;

&lt;p&gt;Dynamo 将每次数据修改的结果当作一个全新且不可修改的数据版本。它允许同一时间存在多个版本的对象。大多数情况，新版本包括了老的版本，系统自己可以决定权威版本（语法协调 syntactic reconciliation）。然而，在出现失败且伴随并发更新操作的时候，可能导致对象的版本冲突。在这种情况下，系统无法协调，需要由客户端必须执行协调，将多个分支演化后的数据折叠成一个合并的版本（语义协调 semantic reconciliation）。&lt;/p&gt;

&lt;p&gt;Dynamo 使用&lt;strong&gt;矢量时钟&lt;/strong&gt;来捕捉同一对象不同版本的因果关系。矢量时钟实际上是一系列（节点，计数器）对。一个矢量时钟和每个对象的每个版本相关联。通过审查向量时钟，可以判断一个对象的两个版本是平行或因果顺序。&lt;strong&gt;如果第一个时钟对象上的计数器小于或等于第二个时钟对象上的所有节点，那么第一个是第二个的祖先&lt;/strong&gt;，可以被忽略。否则这两个变化是冲突的，需要协调。&lt;/p&gt;

&lt;p&gt;在 Dynamo 中，当客户端更新一个对象时必须指定它正要更新哪个版本，这通过从早期的读操作中获得的上下文对象来指定。它包含了向量时钟信息。当处理一个读请求，如果 Dynamo 访问到多个不能语法协调的分支，它将返回处于分支叶子的所有对象。&lt;/p&gt;

&lt;h2&gt;Execution of get () and put () operations&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;处理读或写操作的节点被称为协调员。&lt;/strong&gt;通常，这是首选列表中前 N 个节点中的第一个。如果请求是通过 load balancer 收到，访问 key 的请求可能被路由到环上任一节点。在这种情况下，如果该节点不是请求的 key 的首选列表中前 N 个节点之一，该节点将请求转发到首选列表前 N 个中的第一个节点。&lt;/p&gt;

&lt;p&gt;为了保持副本的一致性，Dynamo 使用的一致性协议类似于仲裁（Quorum NRW 模型）。该协议有两个关键配置值：R 和 W。R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. N 则是数据的副本数量。设定 R 和 W，使得 R + W &amp;gt; N，即&lt;strong&gt;读操作副本加上写操作副本必须大于数据的副本量&lt;/strong&gt;。在此模型中，一个 get 或者 put 操作延时是由最慢的 R 或 W 副本决定的。因此 R 和 W 通常配置为小于 Ｎ，以减小时延。&lt;/p&gt;

&lt;p&gt;当收到对 key 的 put 请求时，协调员为新版本生成向量时钟并写入本地，然后将新版本（带上向量时钟）发送给首选列表中的前 Ｎ 个可达节点。如果至少 Ｗ-1 个节点返回了响应，那么这个写操作被认为是成功的。&lt;/p&gt;

&lt;p&gt;同样，对于 get 请求，协调员从首选列表中前 Ｎ 个可达节点处请求该 key 所有版本的数据，然后等待 Ｒ 个响应，然后返回结果给客户端。如果协调员收到多个版本的数据，它返回所有它认为没有因果关系的版本。&lt;/p&gt;

&lt;h2&gt;Handling Failures: Hinted Handoff&lt;/h2&gt;

&lt;p&gt;Dynamo 如果使用传统的仲裁方式，在服务器故障和网络分裂的情况下将不可用。因此 Dynamo 使用了&lt;strong&gt;“马虎仲裁”&lt;/strong&gt;机制，所有的读，写操作是由首选列表上的前 N 个健康的节点执行的，它们可能不总是在散列环上遇到的那前 N 个节点。&lt;/p&gt;

&lt;p&gt;举个例子，如果写操作过程中节点 A 暂时挂掉或不可达，本来发往 A 的副本现在被发送到节点 D。发送到 D 的副本将在其原数据中表明 A 节点才是该副本的预期接收者。接收到的暗示副本将被保存在单独的本地存储中定期扫描。在检测到了 A 已经复苏，D 会尝试发送副本到 A。一旦传送成功，D 可将数据从本地存储中删除，不会降低系统中的副本总数。&lt;/p&gt;

&lt;p&gt;数据中心有可能因为断电、冷却装置故障、网络故障或自然灾害发生故障。Dynamo 将每个对象跨多个数据中心地进行复制。从本质上讲，每个个 key 的首选列表的分布在多个数据中心。这些数据中心通过高速网络连接。这种&lt;strong&gt;跨多个数据中心的复制方案&lt;/strong&gt;使我们能够处理整个数据中心故障。&lt;/p&gt;

&lt;h2&gt;Handling permanent failures: Replica synchronization&lt;/h2&gt;

&lt;p&gt;Dynamo 实现了&lt;strong&gt;反熵（anti-entropy 或副本同步）协议&lt;/strong&gt;来保持副本同步。&lt;/p&gt;

&lt;p&gt;为了更快地检测副本之间的不一致性并减少传输的数据量，&lt;strong&gt;Dynamo 使用 MerkleTree&lt;/strong&gt;。MerkleTree 是一个哈希树，其叶子是各个 key 的哈希值，父节点为其各自孩子节点的哈希。MerkleTree 的主要优点是树的每个分支可以独立地检查。另外，MerkleTree 有助于减少为检查副本间不一致而传输的数据的大小。如果两树的根哈希值相等，那么树的叶节点值也相等，该节点不需要同步。&lt;/p&gt;

&lt;h2&gt;Membership and Failure Detection&lt;/h2&gt;

&lt;p&gt;去中心化的故障检测协议使用一个简单的 Gossip 式的协议，使系统中的每个节点可以了解其他节点加入或离开。详细可以参考论文《On
scalable and efficient distributed failure detectors》&lt;/p&gt;

&lt;h2&gt;Ensuring Uniform Load distribution&lt;/h2&gt;

&lt;p&gt;随着时间和负载分布的影响，Dynamo 的划分方案也逐渐演化：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-7.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;策略 1：每个节点随机分配 T 个 token ，且基于 token 值进行分割&lt;/p&gt;

&lt;p&gt;使用这一策略会出现以下问题。首先，当新的节点加入系统时，它需要从其他节点上“窃取”其负责的 key ranges。然而，这些需要移交key ranges 给新节点的节点必须扫描他们的本地持久化存储。第二，当一个节点加入/离开系统，许多节点的 key range 发生变化，MertkleTree 需要重新计算，在生产系统上，这不是一个简单的操作。最后，由于 key range 的随机性，没有简单的办法为整个 key space 做快照，这使得归档过程变得复杂。在这个方案中，归档整个 key space 需要分别检索每个节点的 key，这是非常低效的。&lt;/p&gt;

&lt;p&gt;这个策略的根本问题是，&lt;strong&gt;数据划分&lt;/strong&gt;和&lt;strong&gt;数据安置&lt;/strong&gt;交织在一起。 &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;策略 2：每个节点随机分配 T 个 token，每个分区同等大小&lt;/p&gt;

&lt;p&gt;在此策略中，哈希空间被划分为 Q 个同样大小的分区/范围，每个节点随机分配 T 个 token。通常设置 Q 使得 Q &amp;gt;&amp;gt; N 且 Q &amp;gt;&amp;gt; S * T，其中 S 为系统的节点个数。在这一策略中，token 只是用来构造映射函数，该函数将哈希空间的值映射到一个有序列的节点列表，而不决定分区。A partition is placed on the first N unique nodes that are encountered while walking the consistent hashing ring clockwise from the end of the partition. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;策略 3：每个节点分配 Q/S 个 token，每个分区同等大小&lt;/p&gt;

&lt;p&gt;Each node is assigned Q/S tokens where S is the number of nodes in the system. When a node leaves the system, its tokens are randomly distributed to the remaining nodes such that these properties are preserved. Similarly, when a node joins the system it &amp;quot;steals&amp;quot; tokens from nodes in the system in a way that preserves these properties. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相对于策略 1，策略 3 达到更好的效率，每个节点需要维持的信息的大小降低了三个数量级。虽然存储不是一个主要问题，但节点间周期地gossip 成员信息，因此尽可能简化信息。除此之外，策略 3 更易于部署，理由如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;更快的 bootstrapping/recovery&lt;/p&gt;

&lt;p&gt;由于分区范围是固定的，它们可以保存在单独的文件，这意味着转移分区仅需要简单地转移文件。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;易于归档&lt;/p&gt;

&lt;p&gt;对数据集定期归档是 Amazon 存储服务的强制性要求。策略 3 归档整个数据集很简单，因为分区的文件可以被分别归档。相反，策略1 中 token 是随机选取的，归档存储的数据需要分别检索各个节点的 key，这通常是低效和缓慢的。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 15:19:31 +0800</pubDate>
        <link>http://masutangu.com/2017/12/mit-6824-note-3/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/12/mit-6824-note-3/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（二）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;Raft&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Raft 是用于管理复制日志的一致性算法。为了方便理解，Raft 将一致性算法分解为几个关键模块：&lt;strong&gt;Leader 选举&lt;/strong&gt;、&lt;strong&gt;日志复制&lt;/strong&gt;和&lt;strong&gt;安全性&lt;/strong&gt;，同时&lt;strong&gt;通过更强的一致性来减少需要考虑的状态&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;一致性算法允许一组机器像一个整体一样工作，即使其中一些机器挂掉。一致性算法在构建大规模可信赖系统中扮演重要的角色。&lt;/p&gt;

&lt;p&gt;Raft 和许多一致性算法类似，但他也有自己的新特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strong leader&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用比其他一致性算法更强的 leadership 形式。举例来说，log entries 只能从 leader 发往其他 server。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leader election&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用随机 timer 来选举 leader，简洁地解决了冲突的问题。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Membership changes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用 joint consensus 的方式来处理成员变化的问题。在变迁的过程中，两个不同的配置可以共存，以此来实现在变迁过程中集群仍然能持续运转。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Replicated state machines 通常用 replicated log 实现。一致性算法起的作用是保证多个 replicas 上的 replicated log 一致。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;实际系统中使用的一致性算法通常有下列特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;安全性保证&lt;/strong&gt;（绝对不会返回一个错误的结果）&lt;/p&gt;

&lt;p&gt;在非拜占庭错误情况下，网络延迟、分区、丢包、冗余和乱序等错误都可以保证不返回错误的结果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;可用性&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;集群中只要大多数的机器可运行并且能够相互通信、和客户端通信，服务就可用。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;不依赖时序来保证一致性&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通常情况下，小部分 slow servers 不会影响系统整体的性能&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Raft basics&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft 在任何时候都保证以下的特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Election Safety&lt;/strong&gt;: at most one leader can be elected in a given term.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leader Append-Only&lt;/strong&gt;: a leader never overwrites or deletes entries in its log; it only appends new entries. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Log Matching&lt;/strong&gt;: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leader Completeness&lt;/strong&gt;: if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;State Machine Safety&lt;/strong&gt;: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft 中每个 server 在任意时刻都处于 &lt;strong&gt;leader&lt;/strong&gt;、&lt;strong&gt;follower&lt;/strong&gt;、&lt;strong&gt;candidate&lt;/strong&gt; 这三种状态之一。Followers 被动的接收 leader 或 candidate 的请求，leader 处理所有客户端请求，candidate 则起了选举 leader 的角色。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Raft 把时间划分为&lt;strong&gt;任期（Terms）&lt;/strong&gt;，并保证每个任期内只有一个 leader。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Terms act as a logical clock in Raft&lt;/strong&gt;, and they allow servers to detect obsolete information such as stale leaders. &lt;strong&gt;Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft 服务器之间使用 RPC 进行通信，&lt;strong&gt;RequestVote RPC 用于选举，AppendEntries 用于 leader 向 followers 复制日志和心跳。&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Log replication&lt;/h2&gt;

&lt;p&gt;Raft 的日志机制维护下面两个特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If two entries in different logs have the same index and term, then they store the same command.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Safty&lt;/h2&gt;

&lt;p&gt;在任何 leader-based 的一致性算法中，leader 都必须存储所有已经提交的日志条目。Raft 使用了一种简单的办法，保证之前任期中已经提交的日志都会出现在新的 leader 中，不需要将这些日志传给新的 leader。这意味着&lt;strong&gt;日志的传送是单向的从 leader 发往 followers，并且 leader 从不覆盖本地已经存在的日志。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 在投票过程中只允许&lt;strong&gt;包含全部已提交日志的 server 当选 leader&lt;/strong&gt;。Raft 通过比较日志中最后一条日志条目的索引值和任期来判断哪份日志更新。&lt;strong&gt;任期大的日志更加新，如果任期相同，那么日志索引值更大的新。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Leader 不能断定之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。&lt;/strong&gt;（请看论文的 figure 8）Raft 永远不会通过计算副本数目的方式提交一个之前任期内的日志条目。&lt;strong&gt;只有当前任期里的日志条目是通过计算副本数目的方式被提交。&lt;/strong&gt;一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。&lt;/p&gt;

&lt;h2&gt;Timing and availability&lt;/h2&gt;

&lt;p&gt;Raft 要求系统满足以下时间限制：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;broadcastTime ≪ electionTimeout ≪ MTBF
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;BroadcastTime 指的是并行发送 RPCs 给集群中的其他服务器并接收到响应的平均时间。MTBF 是单台服务器两次故障之间的平均时间。&lt;/p&gt;

&lt;p&gt;Raft 的 RPCs 通常耗时在 0.5ms 到 20ms 之间。因此 electionTimeout 最好处于 10ms 到 500ms 之间。&lt;/p&gt;

&lt;h2&gt;Cluster membership changes&lt;/h2&gt;

&lt;p&gt;为了保证配置修改机制的安全，在转换的过程中同一个任期中任何一个时间点都不能出现超过一个 leader。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-3.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用 two-phase 的方式来保证安全性。集群首先切换到过渡的配置，我们称其为 &lt;strong&gt;joint consensus&lt;/strong&gt;。一旦 joint consensus 被提交，集群再切换到新的配置。&lt;strong&gt;Joint consensus&lt;/strong&gt; 是新老配置的结合：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日志条目被复制给集群中新、老配置的所有服务器&lt;/li&gt;
&lt;li&gt;新、旧配置的服务器都可以成为领导人&lt;/li&gt;
&lt;li&gt;Agreement (for elections and entry commitment) 需要分别在两种配置上获得大多数的支持&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群配置以特殊的日志条目保存在 replicated log 中来存储和通信，下图展示了配置转换的过程。一旦一个服务器将新的配置日志条目添加到本地日志中，它就会用这个配置来做决定（服务器总是使用最新的配置，无论该配置是否已经被提交）。这意味着 leader 要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-4.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new 都不能单独做出决定，并且 Leader Completeness 的特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为 leader。这时 leader 创建一条 C-new 配置的日志条目并复制给集群就是安全的了，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，不在新的配置内的服务器就可以关闭了。&lt;/p&gt;

&lt;h2&gt;Client interaction&lt;/h2&gt;

&lt;p&gt;Raft 的目标是要实现 linearizable semantics(each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response) 但 Raft 是可能执行同一条命令多次的。解决方法是&lt;strong&gt;客户端对于每一条指令都赋予一个唯一的序列号，状态机记录每条指令最新的序列号和相应的响应。&lt;/strong&gt;如果接收到一条指令其序列号已经被执行了，那么就立即返回响应，无需重复执行指令。&lt;/p&gt;

&lt;p&gt;只读的操作可以直接处理而不需要写入日志，但可能返回脏数据。Raft 采用额外的机制在不写入日志的情况下保证读操作的正确性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader 必须知道最新的被提交日志。Raft 中通过 leader 在任期开始的时候提交一个空白的日志条目到日志中去来实现。&lt;/li&gt;
&lt;li&gt;Leader 在处理只读的请求之前需要检查自己是否已经被废黜了。Raft 中通过让 leader 在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Raft makes a few design choices that sacrifice performance for simplicity:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Raft follower rejects out-of-order AppendEntries RPCs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Rather than saving for use after hole is filled. Might be important if network re-orders packets a lot
And makes leader-&amp;gt;follower RPC pipelining harder&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Snapshotting is wasteful for big slowly-changing states&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A slow leader may hurt Raft, e.g. in geo-replication&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Experience suggests these have a big effect on performance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disk writes for persistence&lt;/li&gt;
&lt;li&gt;Message/packet/RPC overhead&lt;/li&gt;
&lt;li&gt;Need to execute logged commands sequentially&lt;/li&gt;
&lt;li&gt;Fast path for read-only operations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Appendix&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://thesecretlivesofdata.com/raft/&quot;&gt;http://thesecretlivesofdata.com/raft/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://thesquareplanet.com/blog/students-guide-to-raft/&quot;&gt;https://thesquareplanet.com/blog/students-guide-to-raft/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 15:19:31 +0800</pubDate>
        <link>http://masutangu.com/2017/12/mit-6824-note-2/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/12/mit-6824-note-2/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（一）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;Introduction&lt;/h1&gt;

&lt;p&gt;本课程涵盖以下主题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RPC、线程、并发控制&lt;/li&gt;
&lt;li&gt;性能 &lt;/li&gt;
&lt;li&gt;容灾：&lt;strong&gt;Availability&lt;/strong&gt;, &lt;strong&gt;Durability&lt;/strong&gt;. replicated servers 是不错的选择。&lt;/li&gt;
&lt;li&gt;一致性：replica如何保持一致？ &lt;strong&gt;Consistency&lt;/strong&gt; 和 &lt;strong&gt;Performance&lt;/strong&gt; 不可兼得&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;MapReduce&lt;/h1&gt;

&lt;p&gt;Map funciton 接收 input pair 处理生成一系列的 intermediate key/value pairs。MapReduce 库将相同 intermediate key 的 intermediate values 打包起来传给 Reduce function。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-1/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reduce function 接收 intermediate key 和与其关联的一系列 intermediate values。Reduce function 将这些 intermediate values 合并成更小的 values set。The intermediate values are supplied to the user’s reduce function via an iterator。&lt;/p&gt;

&lt;p&gt;Map Reduce function 的输入输出参数通常如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;map (k1,v1) → list(k2,v2)&lt;/li&gt;
&lt;li&gt;reduce (k2,list(v2)) → list(v2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;容错：worker 挂掉，重跑任务。master挂掉？直接暂停，由用户决定是否重跑。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MR re-runs just the failed Map()s and Reduce()s. MR requires them to be pure functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;they don&amp;#39;t keep state across calls,&lt;/li&gt;
&lt;li&gt;they don&amp;#39;t read or write files other than expected MR inputs/outputs,&lt;/li&gt;
&lt;li&gt;there&amp;#39;s no hidden communication among tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So re-execution yields the same output.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;重跑如何保持一致：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Rely on atomic commits of map and reduce task outputs 细节？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不确定性？&lt;/p&gt;

&lt;p&gt;实现一个精简map reduce&lt;/p&gt;

&lt;h1&gt;RPC&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Better RPC behavior: &amp;quot;at most once&amp;quot;&lt;/p&gt;

&lt;p&gt;idea: server RPC code detects duplicate requests, returns previous reply instead of re-running handler&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;如何检测重复的请求？&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Client 在每个请求都带上唯一的 Request ID，重试时使用相同的 Request ID：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;server:
    if seen[xid]:
      r = old[xid]
    else
      r = handler()
      old[xid] = r
      seen[xid] = true
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Request ID 如何保证唯一？&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求 id 包含客户端 ip，以区分不同客户端&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Server 端何时可以删掉保存的请求 id？&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;unique client IDs, per-client RPC sequence numbers, client includes &amp;quot;seen all replies &amp;lt;= X&amp;quot; with every RPC, much like TCP sequence #s and acks （客户端带上 ack_req_id，服务端可以把 &amp;lt;= ack_req_id 的请求 id 都删掉）&lt;/li&gt;
&lt;li&gt;only allow client one outstanding RPC at a time arrival of seq + 1 allows server to discard all &amp;lt;= seq&lt;/li&gt;
&lt;li&gt;client agrees to keep retrying for &amp;lt; 5 minutes, server discards after 5+ minutes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Server 应该把 Dupliate Request 信息写入磁盘，也应该同步到 replica。万一 server crash 或者重启，才不会丢失。&lt;/p&gt;

&lt;h1&gt;GFS&lt;/h1&gt;

&lt;p&gt;GFS 重新审视了传统文件系统的设计，并总结了以下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;组件挂掉是常态事件，因此文件系统必须集成监控、错误侦测、容灾以及自动恢复的机制&lt;/li&gt;
&lt;li&gt;几 GB 的大文件很普遍。&lt;/li&gt;
&lt;li&gt;随机写的场景非常少。一旦写入完成，文件通常只会被顺序读取&lt;/li&gt;
&lt;li&gt;co-designing the applications and the file system API benefits the overall system by increasing our flexibility（不理解，是说简化 API 的设计么？）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GFS 集群由单一 master 和多个 chunkserver 构成，可同时被多个 client 访问。文件被划为固定大小的 chunks。每个 chunk 在创建时由 master 分配一个不可修改且全局唯一的 chunk handle 标记。chunkservers 以 linux 文件形式保存 chunks，读写 chunks 时client 指定 chunk handle 和 byte range。每个 chunk 被复制多份以实现 reliability。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-1/illustration-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Master 维护整个文件系统的 metadata，包括命名空间，访问控制，文件到 chunks 的映射和 chunks 所在的位置。Master 还管理系统范围内的活动，例如 chunk 租用管理， orphaned chunk 的回收，以及 chunks 在 chunkservers 之间的迁移。Master 使用心跳信息周期地和每个 chunkserver 通讯，发送指令并收集 chunkserver 的状态信息。&lt;/p&gt;

&lt;p&gt;Client 和 master 的通信只涉及元数据，所有的数据操作都直接和 chunkservers 通信。&lt;/p&gt;

&lt;p&gt;Master 主要保存下面三种元数据：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文件和 chunk 的命名空间&lt;/li&gt;
&lt;li&gt;文件到 chunk 的映射关系&lt;/li&gt;
&lt;li&gt;chunk 复本的位置&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;前两种通过 logging mutations 持久化。chunk 复本的位置则是启动时 master 向每个 chunkserver 询问的。&lt;/p&gt;

&lt;p&gt;容灾：通过 &lt;strong&gt;snapshot&lt;/strong&gt; 和 &lt;strong&gt;logging mutations&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;一致性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A file region is &lt;strong&gt;consistent&lt;/strong&gt; if all clients will always see the same data, regardless of which replicas they read from. &lt;/li&gt;
&lt;li&gt;A region is &lt;strong&gt;defined&lt;/strong&gt; after a file data mutation if it is consistent and clients will see what the mutation writes in
its entirety.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GFS 通过以下措施确保被修改的文件 region 是已定义的，并且包含最后一次修改操作写入的数据：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk 的所有副本的修改操作顺序保持一致&lt;/li&gt;
&lt;li&gt;使用 chunk 的版本号来检测副本是否有效&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了应对记录追加的 at-least-once 特性，readers 可以使用下面的方法来处理 padding 和重复的 record。Writers （应用层 writer）在每条写入的记录中都包含了额外的信息例如 Checksum，用来验证它的有效性。Reader 使用 Checksum 识别和丢弃额外的 padding 和记录片段。应用程序还可以用记录的唯一标识符来过滤重复 record。 &lt;/p&gt;

&lt;p&gt;GFS 使用租约（lease）来保证多个副本间变更顺序的一致性。Master 节点给其中一个 replica 分配 chunk 租约，该 replica 变成 primary。Mutations 的顺序由 primary 决定。&lt;/p&gt;

&lt;p&gt;Master 维护了每个 chunk 的版本号，用以分辨出过时的 replicas。 每次 master 下发新的 lease 都会提升 chunk 的版本号。当 chunkserver 上报自身 chunk 集合和相关的版本号时，master 就可以检测出失效的 replica。 &lt;/p&gt;

&lt;h1&gt;Fault-Tolerant Virtual Machines&lt;/h1&gt;

&lt;p&gt;实现容错系统最常见的一种方法是主备方法，备机必须与主机保持接近一致。实现主备一致的一个普遍的做法是将主机状态的变化都同步给备机，包括 CPU、内存、I/O，但这种方式需要大量的带宽。另一种方式是将服务器建模成 deterministic state machine，如果输入相同，那最终的状态也会相同。通过同步输入序列的方式，对带宽的要求远低于同步状态的方式。&lt;/p&gt;

&lt;p&gt;Primary VM 的输入将通过 logging channel 同步给 backup VM。backup 的输出会被 hypervisor 丢弃，只有 primary VM 的输出会返回给客户端。&lt;/p&gt;

&lt;p&gt;Log all hardware events into a log: clock interrupts, network interrupts, i/o interrupts, etc.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;For non-deterministic instructions, record additional info&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;e.g., log the value of the time stamp register&lt;/p&gt;

&lt;p&gt;on replay: return the value from the log instead of the actual register&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Replay: delivery inputs in the same order at the same instructions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;if during recording delivered clock interrupt at nth instruction executed
during replay also delivers the clock interrupt at the nth instruction&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Output Requirement&lt;/strong&gt;: if the backup VM ever takes over after a failure of the primary, the backup VM will continu executing in a way that is entirely consistent with all outputs that the primary VM has sent to the external world.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;要满足 Output Requirement，所有的输出都必须延迟直到 backup VM 收到和该输出相关联的信息以便其可以回放。确保 Output Requirement 的一个简单的做法是给每个 output 操作创建一个特殊的log entry，当 backup VM 接收并 ack 该 log entry 时 primary VM 才会输出结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-1/illustration-3.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FT 使用 UDP 发送心跳包来检测 server 是否存活。另外 FT 还会通过监控从 primary 发往 backup 的流量的方式来检测。&lt;/p&gt;

&lt;p&gt;为了避免 split-brain 的问题，在切换主的时候，需要在 shared storage 上执行原子的 test-and-set 操作，执行成功的才能升为 primary。&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Nov 2017 15:40:29 +0800</pubDate>
        <link>http://masutangu.com/2017/11/mit-6824-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/11/mit-6824-note-1/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>游戏开发之状态机</title>
        <description>&lt;p&gt;这阵子工作的内容有用到状态机，感觉挺有意思。正好好久没写博客了，今天也来写一篇总结下。&lt;/p&gt;

&lt;h1&gt;前言&lt;/h1&gt;

&lt;p&gt;用状态机来实现业务模型，有以下几点好处：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不需要写一大坨 if-else 或 switch case。代码逻辑结构清晰，也更便于调试&lt;/li&gt;
&lt;li&gt;代码阅读起来更加友好，方便其他读者理解整个业务逻辑&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;状态机可以划分为下面三个模块：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;状态集&lt;/strong&gt;：总共包括哪些状态&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;事件（条件）&lt;/strong&gt;：事件会触发状态机的状态发生变化&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作&lt;/strong&gt;：事件发生后执行的动作，可以变迁到新状态，也可以维持当前状态&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;实现&lt;/h1&gt;

&lt;h2&gt;一个简单的状态机的实现&lt;/h2&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;// example 1. simple state machine
// 事件 interface
type Event interface {
  EventId() int64
}

type State int

// 事件处理函数 返回触发事件后的下一个状态 
type Handler func(prevState State, event Event) State

type StateMachine struct {
  currState    State
  handlers map[int64]Handler
}

// 添加事件处理方法 
func (s *StateMachine) AddHandler(eventId int64, handler Handler) {
  s.handlers[eventId] = handler
}

// 事件处理
func (s *StateMachine) Call(event Event)  {
  fmt.Println(&amp;quot;original state: &amp;quot;, s.currState)
  if handler, ok := s.handlers[event.EventId()]; ok {
    s.currState = handler(s.currState, event)  // 调用对应的 handler 更新状态机的状态
  }
  fmt.Println(&amp;quot;new state: &amp;quot;, s.currState)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;Handler&lt;/code&gt; 为事件处理函数，输入参数为当前状态和事件，返回处理后的新状态。状态机根据事件找到对应的&lt;code&gt;Handler&lt;/code&gt;，&lt;code&gt;Handler&lt;/code&gt; 根据当前状态和触发的事件，返回下一个新状态，由状态机更新。&lt;/p&gt;

&lt;p&gt;下面看看一个开关的例子，初始状态为关，按下按钮状态由关变为开，再次按下由开变为关。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;// example 1. simple state machine
const (
  EVENT_PRESS = iota
)

var (
  Off = State(0)  // 定义关闭状态
  On  = State(1)  // 定义开启状态 
)

// 定义 press 事件
type PressEvent struct {
}

func (event *PressEvent) EventId() int64 {
  return EVENT_PRESS
}

// 定义事件 Handler
func PressButton(prevState State, event Event) State {
  if prevState == Off {
    return On
  } else {
    return Off
  }
}

func main() {
  stateMachine := StateMachine{
    currState:    Off,  // 初始状态为关闭
    handlers: make(map[int64]Handler),
  }

  stateMachine.AddHandler(EVENT_PRESS, PressButton)
  stateMachine.Call(&amp;amp;PressEvent{}) // 按下后变成开
  stateMachine.Call(&amp;amp;PressEvent{}) // 再次按下变为关闭
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;程序输出：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;original state:  0
new state:  1
original state:  1
new state:  0
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;这个实现很简单，但不足之处在于状态变迁逻辑都放在&lt;code&gt;Handler&lt;/code&gt;去实现了。对于读代码的人来说，需要读每个Handler的代码，才能整理出整个状态变迁图。&lt;/p&gt;

&lt;h2&gt;一个稍微复杂点的状态机&lt;/h2&gt;

&lt;p&gt;我们希望可以把状态变迁以更直观的方式表现出来，让读者一看就知道状态是如何流转的。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;// example 2. a little more complicate state machine

type Event interface {
  EventId() int64
}

type State int

// 事件处理函数 返回 true 表示可以变迁到下一个状态 返回 false 表示维持当前状态
type Handler func(prevState State, event Event) bool

type StateMachine struct {
  currState    State
  handlers map[int64]Handler
  transitions map[State]map[int64]State
}

// 添加事件处理方法 
func (s *StateMachine) AddHandler(eventId int64, handler Handler) {
  s.handlers[eventId] = handler
}

// 添加状态变迁
func (s *StateMachine) AddTransition(originState State, eventId int64, destState State) {
  if trans, ok := s.transitions[originState]; !ok {
    s.transitions[originState] = map[int64]State{eventId: destState}
  } else {
    trans[eventId] = destState
  }
}

// 事件处理
func (s *StateMachine) Call(event Event) {
  fmt.Println(&amp;quot;original state: &amp;quot;, s.currState)  
  if handler, ok := s.handlers[event.EventId()]; ok { // 首先找到事件的handler
    if handler(s.currState, event) { // 如果事件Handler返回true 则执行状态变迁
      if trans, ok := s.transitions[s.currState]; ok {
        if newState, ok := trans[event.EventId()]; ok { 
          s.currState = newState  // 执行状态变迁
        }
      }
    }
  }
  fmt.Println(&amp;quot;new state: &amp;quot;, s.currState) 
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;这个状态机用&lt;code&gt;transitions&lt;/code&gt; 结构来记录状态流转的关系。初始化时调用方调用&lt;code&gt;AddTransition&lt;/code&gt;方法来添加状态变迁。请看下面的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;// example 2. a little more complicate state machine

const (
  EVENT_PRESS = iota
)

var (
  Off = State(0)  // 关闭
  On  = State(1)  // 开启 
  COUNT = 0
)


type PressEvent struct {
}

func (event *PressEvent) EventId() int64 {
  return EVENT_PRESS
}

// 定义事件 Handler
func PressButton(prevState State, event Event) bool {
  COUNT += 1
  if COUNT % 2 == 0 { // 按两下才切换到新状态
    return true
  }
  return false // 只按一次维持当前状态
}

func main() {
  stateMachine := StateMachine{
    currState:    Off,  // 初始状态为关闭
    handlers: make(map[int64]Handler),
    transitions: make(map[State]map[int64]State),
  }

  stateMachine.AddHandler(EVENT_PRESS, PressButton)
  stateMachine.AddTransition(Off, EVENT_PRESS, On)
  stateMachine.AddTransition(On, EVENT_PRESS, Off)
  stateMachine.Call(&amp;amp;PressEvent{}) // 按一次状态不变
  stateMachine.Call(&amp;amp;PressEvent{}) // 按两次变成 off 状态
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;通过&lt;code&gt;stateMachine.AddTransition(Off, EVENT_PRESS, On)&lt;/code&gt;就可以清晰的知道 Press 事件可能会让状态 Off 切换到 状态 On，尽管 Press 事件发生后还是有可能维持当前状态不变（当 Handler 返回 false 时）。&lt;/p&gt;

&lt;p&gt;程序输出：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;original state:  0
new state:  0
original state:  0
new state:  1
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h2&gt;更模块化的状态机&lt;/h2&gt;

&lt;p&gt;最后我们来实现一个更模块化的状态机，把事件和条件这两个逻辑彻底分开。当你在事件触发时，还需要做逻辑判断才能确定是否发生状态变迁时，建议将事件处理和条件判断剥离开来。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;// example 3. more modular state machine

type Event interface {
  EventId() int64
}

type State int

// 事件处理 Handler
type Handler interface {
  Process(prevState State, event Event)  // 只处理事件
  Check() bool                           // 处理完判断下是否应该做状态切换
}

type StateMachine struct {
  currState    State
  handlers          map[int64]Handler
  transitions   map[State]map[int64]State
}

// 添加事件处理方法 
func (s *StateMachine) AddHandler(eventId int64, handler Handler) {
  s.handlers[eventId] = handler
}

// 添加状态变迁
func (s *StateMachine) AddTransition(originState State, eventId int64, destState State) {
  if trans, ok := s.transitions[originState]; !ok {
    s.transitions[originState] = map[int64]State{eventId: destState}
  } else {
    trans[eventId] = destState
  }
}

// 事件处理
func (s *StateMachine) Call(event Event) {
  fmt.Println(&amp;quot;original state: &amp;quot;, s.currState)  
  if handler, ok := s.handlers[event.EventId()]; ok { // 首先找到事件的handler
    handler.Process(s.currState, event) 
    if handler.Check() {
      if trans, ok := s.transitions[s.currState]; ok {
        if newState, ok := trans[event.EventId()]; ok { 
          s.currState = newState  // 执行状态变迁
        }
      }
    }
  }
  fmt.Println(&amp;quot;new state: &amp;quot;, s.currState) 
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;定义&lt;code&gt;Handler&lt;/code&gt;结构体，有两个接口：&lt;code&gt;Process&lt;/code&gt;只负责事件引发的逻辑处理， &lt;code&gt;Check&lt;/code&gt;判断逻辑处理后是否应该做状态变迁。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// example 3. more modular state machine

const (
  EVENT_PRESS = iota
)

var (
  Off = State(0)  // 关闭
  On  = State(1)  // 开启 
  COUNT = 0
)

type PressEvent struct {
}

func (event *PressEvent) EventId() int64 {
  return EVENT_PRESS
}

type PressEventHandler struct {}

// Process 只处理事件带来的内部变量的变化
func (h *PressEventHandler) Process(prevState State, event Event) {
  COUNT += 1
}


// Check 判断是否应该做状态切换
func (h *PressEventHandler) Check() bool {
  if COUNT % 2 == 0 { // 按两下才有作用
    return true
  }
  return false
}

func main() {
  stateMachine := StateMachine{
    currState:    Off,  // 初始状态为关闭
    handlers: make(map[int64]Handler),
    transitions: make(map[State]map[int64]State),
  }

  stateMachine.AddHandler(EVENT_PRESS, &amp;amp;PressEventHandler{})
  stateMachine.AddTransition(Off, EVENT_PRESS, On)
  stateMachine.AddTransition(On, EVENT_PRESS, Off)
  stateMachine.Call(&amp;amp;PressEvent{}) // 按一次状态不变
  stateMachine.Call(&amp;amp;PressEvent{}) // 按两次变成 off 状态
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;程序输出：
&lt;code&gt;
original state:  0
new state:  0
original state:  0
new state:  1
&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;一些补充&lt;/h2&gt;

&lt;p&gt;可以把 &lt;code&gt;State&lt;/code&gt; 定义成 interface，提供 &lt;code&gt;Enter&lt;/code&gt; 和 &lt;code&gt;Leave&lt;/code&gt; 接口：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;type State interface {
  Enter() 
  Leave()
}

// 事件处理
func (s *StateMachine) Call(event Event) {
  fmt.Println(&amp;quot;original state: &amp;quot;, s.currState)  
  if handler, ok := s.handlers[event.EventId()]; ok { // 首先找到事件的handler
    handler.Process(s.currState, event) 

    if handler.Check() {
      if trans, ok := s.transitions[s.currState]; ok {
        if newState, ok := trans[event.EventId()]; ok { 
          s.currState.Leave()     // 离开当前状态 调用 Leave 
          s.currState = newState  // 执行状态变迁
          s.currState.Enter()     // 进入新状态 调用 Enter
        }
      }
    } 
  }
  fmt.Println(&amp;quot;new state: &amp;quot;, s.currState) 
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h2&gt;与 goroutine 的结合&lt;/h2&gt;

&lt;p&gt;涉及到多个 goroutine 时，总会面临数据竞争的问题。通过 channel 来传递事件，由状态机处理，可以让代码变得清晰，避免加锁。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-golang&quot; data-lang=&quot;golang&quot;&gt;
type Room struct {
  eventCh chan Event // 通过channel 传递事件给状态机
  st *StateMachine
}

func (room *Room) Process() {
  for e := range room.eventCh {
    room.st.Call(e)
  }
}

func (room *Room) DispatchEvent(event Event) {
  room.eventCh &amp;lt;- event
}

func main() {
  stateMachine := StateMachine{
    currState:    Off,  // 初始状态为关闭
    handlers: make(map[int64]Handler),
    transitions: make(map[State]map[int64]State),
  }

  stateMachine.AddHandler(EVENT_PRESS, &amp;amp;PressEventHandler{})
  stateMachine.AddTransition(Off, EVENT_PRESS, On)
  stateMachine.AddTransition(On, EVENT_PRESS, Off)

  room := Room{st: &amp;amp;stateMachine, eventCh: make(chan Event)}

  go room.DispatchEvent(&amp;amp;PressEvent{})
  go room.DispatchEvent(&amp;amp;PressEvent{})
  go room.DispatchEvent(&amp;amp;PressEvent{})

  room.Process()  // just sample code
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;即使是多个 goroutine 并发抛出事件，状态机只从&lt;code&gt;eventCh&lt;/code&gt;中串行的取出事件并处理，处理过程中不需要对数据加锁。 &lt;/p&gt;
</description>
        <pubDate>Wed, 01 Nov 2017 08:33:41 +0800</pubDate>
        <link>http://masutangu.com/2017/11/state-machine/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/11/state-machine/</guid>
        
        
        <category>工作</category>
        
      </item>
    
      <item>
        <title>LevelDB 源码阅读（一）</title>
        <description>&lt;p&gt;这篇文章主要记录 LevelDB 的重要模块、类以及方法。把读写操作和 Compaction 操作的代码串了一遍，并添加了小部分注释。&lt;/p&gt;

&lt;h1&gt;模块&lt;/h1&gt;

&lt;h3&gt;Log 文件&lt;/h3&gt;

&lt;p&gt;客户端的写请求会先 append 到 Log 文件，成功后再写入到 Memtable。如果宕机可以通过 Log 文件来恢复 Memtable。&lt;/p&gt;

&lt;h3&gt;Memtable 和 Immutable Memtable&lt;/h3&gt;

&lt;p&gt;内存数据结构，基于跳表。客户端的读写请求都会由 Memtable 处理。 当 Memtable 占用的内存达到一定阈值，重新生成新的 Memtable 处理客户端请求。原来的 Memtable 转成 Immutable Memtable，等待归并到 SST 文件中。&lt;/p&gt;

&lt;h3&gt;SST 文件&lt;/h3&gt;

&lt;p&gt;落地到磁盘的存储文件。SST 分为不同的 level，具体参考&lt;a href=&quot;https://github.com/google/leveldb/blob/master/doc/impl.md&quot;&gt;文档&lt;/a&gt;。&lt;/p&gt;

&lt;h3&gt;Manifest 文件&lt;/h3&gt;

&lt;p&gt;Manifest 记录不同 level 的 SST 文件，包括每个 SST 文件的 key range、大小等 metadata。&lt;/p&gt;

&lt;h3&gt;Current 文件&lt;/h3&gt;

&lt;p&gt;Current 记录了最新的 Manifest 文件。&lt;/p&gt;

&lt;h1&gt;类成员变量&lt;/h1&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;class DBImpl : public DB {
  private:
    TableCache* table_cache_;
    MemTable* mem_;
    MemTable* imm_;
    WritableFile* logfile_;
    log::Writer* log_;
    std::deque&amp;lt;Writer*&amp;gt; writers_;
    VersionSet* versions_;

    // Set of table files to protect from deletion because they are
    // part of ongoing compactions.
    std::set&amp;lt;uint64_t&amp;gt; pending_outputs_;
};

class MemTable {
  private:
    typedef SkipList&amp;lt;const char*, KeyComparator&amp;gt; Table;
    Arena arena_;  // 内存池
    Table table_;  // 跳表
};

struct FileMetaData {
  int refs;
  int allowed_seek;   // seeks allowed until compaction
  uint64_t number;    // ?? 
  uint64_t file_size;
  InternalKey smallest;
  InternalKey largest;
};

class VersionEdit {
  private:
    typedef std::set&amp;lt; std::pair&amp;lt;int, uint64_t&amp;gt; &amp;gt; DeletedFileSet;

    std::vector&amp;lt; std::pair&amp;lt;int, InternalKey&amp;gt; &amp;gt; compact_pointers_;
    DeletedFileSet deleted_files_;
    std::vector&amp;lt; std::pair&amp;lt;int, FileMetaData&amp;gt; &amp;gt; new_files_;
};

class Version {
  public:
    Status Get(const ReadOptions&amp;amp;, const LookupKey&amp;amp; key, std::string* val, 
               GetStats* stats);
  private:
    VersionSet* vset_;
    Version* next_;
    Version* prev_;

    // list of files per level
    std::vector&amp;lt;FileMetaData*&amp;gt; files_[config::kNumLevels];
};

class TableCache {
  public:
    Status Get(const ReadOptions&amp;amp; options, uint64_t file_number, 
               uint64_t file_size, const Slice&amp;amp; k, void *arg, 
               void (*handle_result)(void*, const Slice&amp;amp;, const Slice&amp;amp;));

  private:
    Cache* cache_;
};

class VersionSet {
  private:
    TableCache* const table_cache_;
    WritableFile* descriptor_file_;
    log::Writer* descriptor_log_;
    Version dummy_versions_;  // Head of circurlar doubly-linked list of versions  
    Version* current_;        // == dummy_versions_.prev_
};

class WriteBatch {
  public:
    class Handler {
    public:
        virtual ~Handler();
        virtual void Put(const Slice&amp;amp; key, const Slice&amp;amp; value) = 0;
        virtual void Delete(const Slice&amp;amp; key) = 0;
    };
  private:
    friend class WriteBatchInternal;
    std::string req_;
}

struct DBImpl::Writer {
  Status status;
  WriteBatch* batch;
  bool sync;
  bool done;
  port::CondVar cv;

  expplicit Writer(port::Mutex* mu) : cv(mu) { }
};

class Compaction {
  private:
    Version* input_version_;
    VersionEdit edit_;

    // Each compaction reads inputs from &amp;quot;level_&amp;quot; and &amp;quot;level_+1&amp;quot;
    std::vector&amp;lt;FileMetaData*&amp;gt; inputs_[2];      // The two sets of inputs

    // State used to check for number of of overlapping grandparent files
    // (parent == level_ + 1, grandparent == level_ + 2)
    std::vector&amp;lt;FileMetaData*&amp;gt; grandparents_;
    size_t grandparent_index_;  // Index in grandparent_starts_
    bool seen_key_;             // Some output key has been seen
    int64_t overlapped_bytes_;  // Bytes of overlap between current output
                                // and grandparent files

    // level_ptrs_ holds indices into input_version_-&amp;gt;levels_: our state
    // is that we are positioned at one of the file ranges for each
    // higher level than the ones involved in this compaction (i.e. for
    // all L &amp;gt;= level_ + 2).
    size_t level_ptrs_[config::kNumLevels];
};

&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h1&gt;主要操作&lt;/h1&gt;

&lt;h3&gt;读操作&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status DBImpl::Get(const ReadOptions&amp;amp; options,
                   const Slice&amp;amp; key,
                   std::string* value) {

  MutexLock l(&amp;amp;mutex_);
  MemTable* mem = mem_;
  MemTable* imm = imm_;
  Version* current = versions_-&amp;gt;current();

  bool have_stat_update = false;
  Version::GetStats stats;

  // Unlock while reading from files and memtables
  {
    mutex_.Unlock();
    // First look in the memtable, then in the immutable memtable (if any).
    LookupKey lkey(key, snapshot);
    if (mem-&amp;gt;Get(lkey, value, &amp;amp;s)) {  // 1）先在 MemTable 中查找
      // Done
    } else if (imm != NULL &amp;amp;&amp;amp; imm-&amp;gt;Get(lkey, value, &amp;amp;s)) {  // 2）再在 Imutable MemTable 中查找
      // Done
    } else {
      s = current-&amp;gt;Get(options, lkey, value, &amp;amp;stats);  // 3) 最后在当前 Version 中查找
      have_stat_update = true;
    }
    mutex_.Lock();
  }

  // UpdateStats 减去 allowed_seeks，如果小于等于 0，则设置 file_to_compact_，准备 compaction
  if (have_stat_update &amp;amp;&amp;amp; current-&amp;gt;UpdateStats(stats)) {
    MaybeScheduleCompaction();
  }

  return s;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;// Version 类的 Get 方法
Status Version::Get(const ReadOptions&amp;amp; options,
                    const LookupKey&amp;amp; k,
                    std::string* value,
                    GetStats* stats) {
  Slice ikey = k.internal_key();
  Slice user_key = k.user_key();
  const Comparator* ucmp = vset_-&amp;gt;icmp_.user_comparator();
  Status s;

  stats-&amp;gt;seek_file = NULL;
  stats-&amp;gt;seek_file_level = -1;
  FileMetaData* last_file_read = NULL;
  int last_file_read_level = -1;

  // We can search level-by-level since entries never hop across
  // levels.  Therefore we are guaranteed that if we find data
  // in an smaller level, later levels are irrelevant.
  std::vector&amp;lt;FileMetaData*&amp;gt; tmp;
  FileMetaData* tmp2;
  for (int level = 0; level &amp;lt; config::kNumLevels; level++) {
    size_t num_files = files_[level].size();
    if (num_files == 0) continue;

    // 这里省略一大段代码 files 指向候选文件列表，num_files 为列表的长度。具体实现看源码

    for (uint32_t i = 0; i &amp;lt; num_files; ++i) {
      if (last_file_read != NULL &amp;amp;&amp;amp; stats-&amp;gt;seek_file == NULL) {
        // We have had more than one seek for this read.  Charge the 1st file.
        // last_file_read 保存的其实就是第一个查找未命中的文件，函数返回后会调用 UpdateStats 来减去 allowed_seeks
        stats-&amp;gt;seek_file = last_file_read;
        stats-&amp;gt;seek_file_level = last_file_read_level;
      }

      FileMetaData* f = files[i];
      last_file_read = f;
      last_file_read_level = level;

      Saver saver;
      saver.state = kNotFound;
      saver.ucmp = ucmp;
      saver.user_key = user_key;
      saver.value = value;
      // 从 TableCache 中读取文件内容
      s = vset_-&amp;gt;table_cache_-&amp;gt;Get(options, f-&amp;gt;number, f-&amp;gt;file_size,
                                   ikey, &amp;amp;saver, SaveValue);
      if (!s.ok()) {
        return s;
      }
      switch (saver.state) {
        case kNotFound:
          break;      // Keep searching in other files
        case kFound:
          return s;
        case kDeleted:
          s = Status::NotFound(Slice());  // Use empty error message for speed
          return s;
        case kCorrupt:
          s = Status::Corruption(&amp;quot;corrupted key for &amp;quot;, user_key);
          return s;
      }
    }
  }

  return Status::NotFound(Slice());  // Use an empty error message for speed
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status TableCache::Get(const ReadOptions&amp;amp; options, uint64_t file_number, 
                      uint64_t file_size, const Slice&amp;amp; k, void *arg,
                      void (*saver)(void* const Slice&amp;amp;, const Slice&amp;amp;)) {
  Cache::Handle* handle = NULL;
  Status s = FindTable(file_number, file_size, &amp;amp;handle);
  if (s.ok()) {
    Table* t = reinterpret_cast&amp;lt;TableAndFile*&amp;gt;(cache_-&amp;gt;Value(handle))-&amp;gt;table;
    s = t-&amp;gt;InternalGet(options, k, arg, saver);
    cache_-&amp;gt;Release(handle);
  }
  return s;                        
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;查找顺序如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/leveldb/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;写操作&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status DB::Put(const WriteOptions&amp;amp; opt, const Slice&amp;amp; key, const Slice&amp;amp; value) {
  WriteBatch batch;
  batch.Put(key, value);
  return Write(opt, &amp;amp;batch);
}

Status DBImpl::Write(const WriteOptions&amp;amp; options, WriteBatch* my_batch) {
  Writer w(&amp;amp;mutex_);
  w.batch = my_batch;
  w.sync = options.sync;
  w.done = false;

  MutexLock l(&amp;amp;mutex_);
  writers_.push_back(&amp;amp;w);
  // 生产者消费者模型
  while (!w.done &amp;amp;&amp;amp; &amp;amp;w != writers_.front()) {
    w.cv.Wait();
  }
  // 写操作有可能被合并处理，因此有可能取到的时候写入已经完成。完成的话直接返回
  if (w.done) {
    return w.status;
  }

  // May temporarily unlock and wait.
  // MakeRoomForWrite 判断是非需要归并 memtable
  Status status = MakeRoomForWrite(my_batch == NULL);
  uint64_t last_sequence = versions_-&amp;gt;LastSequence();
  Writer* last_writer = &amp;amp;w;
  if (status.ok() &amp;amp;&amp;amp; my_batch != NULL) {  // NULL batch is for compactions
    WriteBatch* updates = BuildBatchGroup(&amp;amp;last_writer); // 合并写操作
    WriteBatchInternal::SetSequence(updates, last_sequence + 1);
    last_sequence += WriteBatchInternal::Count(updates);

    // Add to log and apply to memtable.  We can release the lock
    // during this phase since &amp;amp;w is currently responsible for logging
    // and protects against concurrent loggers and concurrent writes
    // into mem_.
    {
      mutex_.Unlock();
      status = log_-&amp;gt;AddRecord(WriteBatchInternal::Contents(updates));
      bool sync_error = false;
      if (status.ok() &amp;amp;&amp;amp; options.sync) {
        status = logfile_-&amp;gt;Sync();
        if (!status.ok()) {
          sync_error = true;
        }
      }
      if (status.ok()) {
        status = WriteBatchInternal::InsertInto(updates, mem_);
      }
      mutex_.Lock();
      if (sync_error) {
        // The state of the log file is indeterminate: the log record we
        // just added may or may not show up when the DB is re-opened.
        // So we force the DB into a mode where all future writes fail.
        RecordBackgroundError(status);
      }
    }
    if (updates == tmp_batch_) tmp_batch_-&amp;gt;Clear();

    versions_-&amp;gt;SetLastSequence(last_sequence);
  }

  while (true) {
    Writer* ready = writers_.front();
    writers_.pop_front();
    if (ready != &amp;amp;w) {
      ready-&amp;gt;status = status;
      ready-&amp;gt;done = true;
      ready-&amp;gt;cv.Signal();
    }
    if (ready == last_writer) break;
  }

  // Notify new head of write queue
  if (!writers_.empty()) {
    writers_.front()-&amp;gt;cv.Signal();
  }

  return status;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;// REQUIRES: Writer list must be non-empty
// REQUIRES: First writer must have a non-NULL batch
// 尝试合并写操作
WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {
  assert(!writers_.empty());
  Writer* first = writers_.front();
  WriteBatch* result = first-&amp;gt;batch;
  assert(result != NULL);

  size_t size = WriteBatchInternal::ByteSize(first-&amp;gt;batch);

  // Allow the group to grow up to a maximum size, but if the
  // original write is small, limit the growth so we do not slow
  // down the small write too much.
  size_t max_size = 1 &amp;lt;&amp;lt; 20;
  if (size &amp;lt;= (128&amp;lt;&amp;lt;10)) {
    max_size = size + (128&amp;lt;&amp;lt;10);
  }

  *last_writer = first;
  std::deque&amp;lt;Writer*&amp;gt;::iterator iter = writers_.begin();
  ++iter;  // Advance past &amp;quot;first&amp;quot;
  for (; iter != writers_.end(); ++iter) {
    Writer* w = *iter;
    if (w-&amp;gt;sync &amp;amp;&amp;amp; !first-&amp;gt;sync) {
      // Do not include a sync write into a batch handled by a non-sync write.
      break;
    }

    if (w-&amp;gt;batch != NULL) {
      size += WriteBatchInternal::ByteSize(w-&amp;gt;batch);
      if (size &amp;gt; max_size) {
        // Do not make batch too big
        break;
      }

      // Append to *result
      // 把合并的写请求保存在成员变量 tmp_batch_ 中，避免和调用者的写请求混淆在一起
      if (result == first-&amp;gt;batch) {
        // Switch to temporary batch instead of disturbing caller&amp;#39;s batch
        result = tmp_batch_;
        assert(WriteBatchInternal::Count(result) == 0);
        WriteBatchInternal::Append(result, first-&amp;gt;batch);
      }
      WriteBatchInternal::Append(result, w-&amp;gt;batch);
    }
    *last_writer = w;
  }
  return result;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status WriteBatchInternal::InsertInto(const WriteBatch* b,
                                      MemTable* memtable) {
  MemTableInserter inserter;
  inserter.sequence_ = WriteBatchInternal::Sequence(b);
  inserter.mem_ = memtable;
  return b-&amp;gt;Iterate(&amp;amp;inserter);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status WriteBatch::Iterate(Handler* handler) const {
  Slice input(rep_);
  if (input.size() &amp;lt; kHeader) {
    return Status::Corruption(&amp;quot;malformed WriteBatch (too small)&amp;quot;);
  }

  input.remove_prefix(kHeader);
  Slice key, value;
  int found = 0;
  while (!input.empty()) {
    found++;
    char tag = input[0];
    input.remove_prefix(1);
    switch (tag) {
      case kTypeValue:
        if (GetLengthPrefixedSlice(&amp;amp;input, &amp;amp;key) &amp;amp;&amp;amp;
            GetLengthPrefixedSlice(&amp;amp;input, &amp;amp;value)) {
          handler-&amp;gt;Put(key, value);
        } else {
          return Status::Corruption(&amp;quot;bad WriteBatch Put&amp;quot;);
        }
        break;
      case kTypeDeletion:
        if (GetLengthPrefixedSlice(&amp;amp;input, &amp;amp;key)) {
          handler-&amp;gt;Delete(key);
        } else {
          return Status::Corruption(&amp;quot;bad WriteBatch Delete&amp;quot;);
        }
        break;
      default:
        return Status::Corruption(&amp;quot;unknown WriteBatch tag&amp;quot;);
    }
  }
  if (found != WriteBatchInternal::Count(this)) {
    return Status::Corruption(&amp;quot;WriteBatch has wrong count&amp;quot;);
  } else {
    return Status::OK();
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h3&gt;Compaction&lt;/h3&gt;

&lt;p&gt;Compaction 触发时机：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Immutable MemTable 不为空&lt;/li&gt;
&lt;li&gt;指定了 Manual Compaction&lt;/li&gt;
&lt;li&gt;VersionSet NeedsCompaction 返回 True

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;compaction_score_&lt;/code&gt; 大于 1&lt;/li&gt;
&lt;li&gt;&lt;code&gt;file_to_compact_&lt;/code&gt; 不为空&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;void DBImpl::MaybeScheduleCompaction() {
  mutex_.AssertHeld();
  if (bg_compaction_scheduled_) {
    // Already scheduled
  } else if (shutting_down_.Acquire_Load()) {
    // DB is being deleted; no more background compactions
  } else if (!bg_error_.ok()) {
    // Already got an error; no more changes
  } else if (imm_ == NULL &amp;amp;&amp;amp;
             manual_compaction_ == NULL &amp;amp;&amp;amp;
             !versions_-&amp;gt;NeedsCompaction()) {
    // No work to be done
  } else {
    bg_compaction_scheduled_ = true;
    env_-&amp;gt;Schedule(&amp;amp;DBImpl::BGWork, this);
  }
}

bool VersionSet::NeedsCompaction() const {
  Version* v = current_;
  return (v-&amp;gt;compaction_score_ &amp;gt;= 1) || (v-&amp;gt;file_to_compact_ != NULL);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;compaction_score_ 的计算如下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;void VersionSet::Finalize(Version* v) {
  // Precomputed best level for next compaction
  int best_level = -1;
  double best_score = -1;

  for (int level = 0; level &amp;lt; config::kNumLevels-1; level++) {
    double score;
    if (level == 0) {
      // We treat level-0 specially by bounding the number of files
      // instead of number of bytes for two reasons:
      //
      // (1) With larger write-buffer sizes, it is nice not to do too
      // many level-0 compactions.
      //
      // (2) The files in level-0 are merged on every read and
      // therefore we wish to avoid too many files when the individual
      // file size is small (perhaps because of a small write-buffer
      // setting, or very high compression ratios, or lots of
      // overwrites/deletions).
      score = v-&amp;gt;files_[level].size() /
          static_cast&amp;lt;double&amp;gt;(config::kL0_CompactionTrigger);
    } else {
      // Compute the ratio of current size to size limit.
      const uint64_t level_bytes = TotalFileSize(v-&amp;gt;files_[level]);
      score = static_cast&amp;lt;double&amp;gt;(level_bytes) / MaxBytesForLevel(level);
    }

    if (score &amp;gt; best_score) {
      best_level = level;
      best_score = score;
    }
  }

  v-&amp;gt;compaction_level_ = best_level;
  v-&amp;gt;compaction_score_ = best_score;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;file_to_compact_ 则是由 allowed_seeks 来控制。从下面代码的注释可知 25 次 seek 的开销和一次 compaction 的开销差不多。allowed_seeks 可以理解为文件剩余查找次数，每次查找失败allowed_seeks 就会减 1。当 allowed_seeks 小于等于 0，意味着应该启动 compaction 来减少查找未命中带来的 seek 的开销了：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;bool Version::UpdateStats(const GetStats&amp;amp; stats) {
  FileMetaData* f = stats.seek_file;
  if (f != NULL) {
    f-&amp;gt;allowed_seeks--;
    if (f-&amp;gt;allowed_seeks &amp;lt;= 0 &amp;amp;&amp;amp; file_to_compact_ == NULL) {
      file_to_compact_ = f;
      file_to_compact_level_ = stats.seek_file_level;
      return true;
    }
  }
  return false;
}

// Apply all of the edits in *edit to the current state.
void Builder::Apply(VersionEdit* edit) {
  // Update compaction pointers
  for (size_t i = 0; i &amp;lt; edit-&amp;gt;compact_pointers_.size(); i++) {
    const int level = edit-&amp;gt;compact_pointers_[i].first;
    vset_-&amp;gt;compact_pointer_[level] =
        edit-&amp;gt;compact_pointers_[i].second.Encode().ToString();
  }

  // Delete files
  const VersionEdit::DeletedFileSet&amp;amp; del = edit-&amp;gt;deleted_files_;
  for (VersionEdit::DeletedFileSet::const_iterator iter = del.begin();
        iter != del.end();
        ++iter) {
    const int level = iter-&amp;gt;first;
    const uint64_t number = iter-&amp;gt;second;
    levels_[level].deleted_files.insert(number);
  }

  // Add new files
  for (size_t i = 0; i &amp;lt; edit-&amp;gt;new_files_.size(); i++) {
    const int level = edit-&amp;gt;new_files_[i].first;
    FileMetaData* f = new FileMetaData(edit-&amp;gt;new_files_[i].second);
    f-&amp;gt;refs = 1;

    // We arrange to automatically compact this file after
    // a certain number of seeks.  Let&amp;#39;s assume:
    //   (1) One seek costs 10ms
    //   (2) Writing or reading 1MB costs 10ms (100MB/s)
    //   (3) A compaction of 1MB does 25MB of IO:
    //         1MB read from this level
    //         10-12MB read from next level (boundaries may be misaligned)
    //         10-12MB written to next level
    // This implies that 25 seeks cost the same as the compaction
    // of 1MB of data.  I.e., one seek costs approximately the
    // same as the compaction of 40KB of data.  We are a little
    // conservative and allow approximately one seek for every 16KB
    // of data before triggering a compaction.
    f-&amp;gt;allowed_seeks = (f-&amp;gt;file_size / 16384);
    if (f-&amp;gt;allowed_seeks &amp;lt; 100) f-&amp;gt;allowed_seeks = 100;

    levels_[level].deleted_files.erase(f-&amp;gt;number);
    levels_[level].added_files-&amp;gt;insert(f);
  }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看看 Compaction 做了哪些工作：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;void DBImpl::BackgroundCompaction() {
  mutex_.AssertHeld();

  if (imm_ != NULL) {
    CompactMemTable();
    return;
  }
  // 这里去掉了 manual compaction 的代码 不关心
  Compaction* c = versions_-&amp;gt;PickCompaction();

  Status status;
  if (c == NULL) {
    // Nothing to do
  } else if (c-&amp;gt;IsTrivialMove()) {
    // Move file to next level
    // IsTrivialMove 返回 True 则直接将文件移入 level + 1 层即可
    assert(c-&amp;gt;num_input_files(0) == 1);
    FileMetaData* f = c-&amp;gt;input(0, 0);
    c-&amp;gt;edit()-&amp;gt;DeleteFile(c-&amp;gt;level(), f-&amp;gt;number);
    c-&amp;gt;edit()-&amp;gt;AddFile(c-&amp;gt;level() + 1, f-&amp;gt;number, f-&amp;gt;file_size,
                       f-&amp;gt;smallest, f-&amp;gt;largest);
    status = versions_-&amp;gt;LogAndApply(c-&amp;gt;edit(), &amp;amp;mutex_);
    if (!status.ok()) {
      RecordBackgroundError(status);
    }
  } else {
    CompactionState* compact = new CompactionState(c);
    status = DoCompactionWork(compact);
    if (!status.ok()) {
      RecordBackgroundError(status);
    }
    CleanupCompaction(compact);
    c-&amp;gt;ReleaseInputs();
    DeleteObsoleteFiles();
  }
  delete c;

  if (status.ok()) {
    // Done
  } else if (shutting_down_.Acquire_Load()) {
    // Ignore compaction errors found during shutting down
  } else {
    Log(options_.info_log,
        &amp;quot;Compaction error: %s&amp;quot;, status.ToString().c_str());
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Compaction* VersionSet::PickCompaction() {
  Compaction* c;
  int level;

  // We prefer compactions triggered by too much data in a level over
  // the compactions triggered by seeks.
  // 判断是 size_compaction 还是 seek_compaction
  const bool size_compaction = (current_-&amp;gt;compaction_score_ &amp;gt;= 1);
  const bool seek_compaction = (current_-&amp;gt;file_to_compact_ != NULL);
  if (size_compaction) {
    level = current_-&amp;gt;compaction_level_;
    assert(level &amp;gt;= 0);
    assert(level+1 &amp;lt; config::kNumLevels);
    c = new Compaction(level);

    // Pick the first file that comes after compact_pointer_[level]
    // compact_pointer_[level] 记录上次 compact 时最大的 key
    for (size_t i = 0; i &amp;lt; current_-&amp;gt;files_[level].size(); i++) {
      FileMetaData* f = current_-&amp;gt;files_[level][i];
      if (compact_pointer_[level].empty() ||
          icmp_.Compare(f-&amp;gt;largest.Encode(), compact_pointer_[level]) &amp;gt; 0) {
        c-&amp;gt;inputs_[0].push_back(f);
        break;
      }
    }
    if (c-&amp;gt;inputs_[0].empty()) {
      // Wrap-around to the beginning of the key space
      c-&amp;gt;inputs_[0].push_back(current_-&amp;gt;files_[level][0]);
    }
  } else if (seek_compaction) {
    level = current_-&amp;gt;file_to_compact_level_;
    c = new Compaction(level);
    c-&amp;gt;inputs_[0].push_back(current_-&amp;gt;file_to_compact_);
  } else {
    return NULL;
  }

  c-&amp;gt;input_version_ = current_;
  c-&amp;gt;input_version_-&amp;gt;Ref();

  // Files in level 0 may overlap each other, so pick up all overlapping ones
  if (level == 0) {
    InternalKey smallest, largest;
    GetRange(c-&amp;gt;inputs_[0], &amp;amp;smallest, &amp;amp;largest);
    // Note that the next call will discard the file we placed in
    // c-&amp;gt;inputs_[0] earlier and replace it with an overlapping set
    // which will include the picked file.
    current_-&amp;gt;GetOverlappingInputs(0, &amp;amp;smallest, &amp;amp;largest, &amp;amp;c-&amp;gt;inputs_[0]);
    assert(!c-&amp;gt;inputs_[0].empty());
  }

  // 填充 level + 1 的文件，更新 compact_pointer_ 
  SetupOtherInputs(c);

  return c;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;IsTrivialMove 判断能否直接将文件移入 level + 1 层：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;bool Compaction::IsTrivialMove() const {
  // Avoid a move if there is lots of overlapping grandparent data.
  // Otherwise, the move could create a parent file that will require
  // a very expensive merge later on.
  return (num_input_files(0) == 1 &amp;amp;&amp;amp;
          num_input_files(1) == 0 &amp;amp;&amp;amp;
          TotalFileSize(grandparents_) &amp;lt;= kMaxGrandParentOverlapBytes);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;具体的合并操作在 DoCompactionWork 方法：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status DBImpl::DoCompactionWork(CompactionState* compact) {
  if (snapshots_.empty()) {
    compact-&amp;gt;smallest_snapshot = versions_-&amp;gt;LastSequence();
  } else {
    compact-&amp;gt;smallest_snapshot = snapshots_.oldest()-&amp;gt;number_;
  }

  // Release mutex while we&amp;#39;re actually doing the compaction work
  mutex_.Unlock();

  Iterator* input = versions_-&amp;gt;MakeInputIterator(compact-&amp;gt;compaction);
  input-&amp;gt;SeekToFirst();
  Status status;
  ParsedInternalKey ikey;
  std::string current_user_key;
  bool has_current_user_key = false;
  SequenceNumber last_sequence_for_key = kMaxSequenceNumber;
  for (; input-&amp;gt;Valid() &amp;amp;&amp;amp; !shutting_down_.Acquire_Load(); ) {
    // Prioritize immutable compaction work
    if (has_imm_.NoBarrier_Load() != NULL) {
      mutex_.Lock();
      if (imm_ != NULL) {
        CompactMemTable();  // 总是优先处理 CompactMemTable 避免阻塞 MemTable 的写入
        bg_cv_.SignalAll();  // Wakeup MakeRoomForWrite() if necessary
      }
      mutex_.Unlock();
    }

    Slice key = input-&amp;gt;key();
    if (compact-&amp;gt;compaction-&amp;gt;ShouldStopBefore(key) &amp;amp;&amp;amp;
        compact-&amp;gt;builder != NULL) {
      status = FinishCompactionOutputFile(compact, input);
      if (!status.ok()) {
        break;
      }
    }

    // Handle key/value, add to state, etc.
    bool drop = false;
    if (!ParseInternalKey(key, &amp;amp;ikey)) {
      // Do not hide error keys
      current_user_key.clear();
      has_current_user_key = false;
      last_sequence_for_key = kMaxSequenceNumber;
    } else {
      if (!has_current_user_key ||
          user_comparator()-&amp;gt;Compare(ikey.user_key,
                                     Slice(current_user_key)) != 0) {
        // First occurrence of this user key
        current_user_key.assign(ikey.user_key.data(), ikey.user_key.size());
        has_current_user_key = true;
        last_sequence_for_key = kMaxSequenceNumber;
      }

      if (last_sequence_for_key &amp;lt;= compact-&amp;gt;smallest_snapshot) {
        // Hidden by an newer entry for same user key
        drop = true;    // (A)
      } else if (ikey.type == kTypeDeletion &amp;amp;&amp;amp;
                 ikey.sequence &amp;lt;= compact-&amp;gt;smallest_snapshot &amp;amp;&amp;amp;
                 compact-&amp;gt;compaction-&amp;gt;IsBaseLevelForKey(ikey.user_key)) {
        // For this user key:
        // (1) there is no data in higher levels
        // (2) data in lower levels will have larger sequence numbers
        // (3) data in layers that are being compacted here and have
        //     smaller sequence numbers will be dropped in the next
        //     few iterations of this loop (by rule (A) above).
        // Therefore this deletion marker is obsolete and can be dropped.
        // 如果高层还有记录，则 kTypeDeletion 标记不能丢掉。
        // smallest_snapshot 主要是为了快照功能服务
        // 但 ikey.sequence &amp;lt;= compact-&amp;gt;smallest_snapshot 这个判断没看懂
        drop = true;
      }

      last_sequence_for_key = ikey.sequence;
    }

    if (!drop) {
      // Open output file if necessary
      if (compact-&amp;gt;builder == NULL) {
        status = OpenCompactionOutputFile(compact);
        if (!status.ok()) {
          break;
        }
      }
      if (compact-&amp;gt;builder-&amp;gt;NumEntries() == 0) {
        compact-&amp;gt;current_output()-&amp;gt;smallest.DecodeFrom(key);
      }
      compact-&amp;gt;current_output()-&amp;gt;largest.DecodeFrom(key);
      compact-&amp;gt;builder-&amp;gt;Add(key, input-&amp;gt;value());

      // Close output file if it is big enough
      if (compact-&amp;gt;builder-&amp;gt;FileSize() &amp;gt;=
          compact-&amp;gt;compaction-&amp;gt;MaxOutputFileSize()) {
        status = FinishCompactionOutputFile(compact, input);  // 输出新的 SST 文件
        if (!status.ok()) {
          break;
        }
      }
    }

    input-&amp;gt;Next();
  }

  // 中间省略一坨代码

  mutex_.Lock();
  stats_[compact-&amp;gt;compaction-&amp;gt;level() + 1].Add(stats);

  if (status.ok()) {
    status = InstallCompactionResults(compact);
  }

  return status;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;最后调用 InstallCompactionResults，记录版本变化：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;Status DBImpl::InstallCompactionResults(CompactionState* compact) {
  mutex_.AssertHeld();
  Log(options_.info_log,  &amp;quot;Compacted %d@%d + %d@%d files =&amp;gt; %lld bytes&amp;quot;,
      compact-&amp;gt;compaction-&amp;gt;num_input_files(0),
      compact-&amp;gt;compaction-&amp;gt;level(),
      compact-&amp;gt;compaction-&amp;gt;num_input_files(1),
      compact-&amp;gt;compaction-&amp;gt;level() + 1,
      static_cast&amp;lt;long long&amp;gt;(compact-&amp;gt;total_bytes));

  // Add compaction outputs
  compact-&amp;gt;compaction-&amp;gt;AddInputDeletions(compact-&amp;gt;compaction-&amp;gt;edit());
  const int level = compact-&amp;gt;compaction-&amp;gt;level();
  for (size_t i = 0; i &amp;lt; compact-&amp;gt;outputs.size(); i++) {
    const CompactionState::Output&amp;amp; out = compact-&amp;gt;outputs[i];
    compact-&amp;gt;compaction-&amp;gt;edit()-&amp;gt;AddFile(
        level + 1,
        out.number, out.file_size, out.smallest, out.largest);
  }
  return versions_-&amp;gt;LogAndApply(compact-&amp;gt;compaction-&amp;gt;edit(), &amp;amp;mutex_);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Fri, 23 Jun 2017 15:53:23 +0800</pubDate>
        <link>http://masutangu.com/2017/06/leveldb_1/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/06/leveldb_1/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>链接和装载</title>
        <description>&lt;p&gt;本文是读《程序员的自我修养: 链接、装载与库》所整理的读书笔记。&lt;/p&gt;

&lt;h1&gt;概论&lt;/h1&gt;

&lt;p&gt;从源文件到可执行文件，可以分解为四个过程：&lt;strong&gt;预处理&lt;/strong&gt;，&lt;strong&gt;编译&lt;/strong&gt;，&lt;strong&gt;汇编&lt;/strong&gt;，&lt;strong&gt;链接&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;预处理主要完成以下工作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;展开所有宏定义，删除 #define&lt;/li&gt;
&lt;li&gt;处理所有条件预编译指令&lt;/li&gt;
&lt;li&gt;处理 #include 预编译指令。将被包含的文件插入到该预编译指令的位置&lt;/li&gt;
&lt;li&gt;删除所有注释&lt;/li&gt;
&lt;li&gt;添加行号和文件名标识，以便编译时编辑器产生调试用的行号信息以及编译产生错误或警告时的行号信息&lt;/li&gt;
&lt;li&gt;保留所有 #pragma 编译器指令&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;编译的过程即把预处理完的文件进行一系列的词法分析、语法分析、语义分析以及优化后产生相应的汇编代码文件。&lt;/p&gt;

&lt;p&gt;汇编器将汇编代码转变为机器可以执行的指令，即目标文件，再由链接器将多个目标文件输出成最后的可执行文件。链接器负责解决&lt;strong&gt;模块间符号引用&lt;/strong&gt;的问题，链接的过程包括了&lt;strong&gt;地址和空间分配&lt;/strong&gt;、&lt;strong&gt;符号决议&lt;/strong&gt;和&lt;strong&gt;重定位&lt;/strong&gt;。&lt;/p&gt;

&lt;h1&gt;目标文件&lt;/h1&gt;

&lt;p&gt;目标文件和可执行文件的格式很相似，在 Linux 下统称为 ELF 文件。ELF 文件格式的核心思想是按不同属性分段（section）存储。&lt;/p&gt;

&lt;p&gt;编译后的目标文件格式如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;程序源代码编译后的机器指令通常放在代码段（.text），已初始化的全局变量和静态局部变量放在数据段（.data），.bss 段用于为未初始化的全局变量和静态局部变量预留空间，因此该段在文件中不占据空间。.rodata 段存放的是只读数据，一般是只读变量和字符串常量。&lt;/p&gt;

&lt;p&gt;分段的好处在于：&lt;strong&gt;方便权限控制&lt;/strong&gt;、&lt;strong&gt;提高 CPU 缓存命中率&lt;/strong&gt;和&lt;strong&gt;节省内存&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;下面看个实际的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;/*
simplesection.c
linux: gcc -c simplesection.c -o simplesection.o
*/

int printf(const char* format, ...);

int global_init_var = 84;
int global_uninit_var;

void func1(int i) 
{
    printf(&amp;quot;%d\n&amp;quot;, i);
}

int main(void) 
{
    static int static_var = 85;
    static int static_var2;

    int a = 1;
    int b;

    func1(static_var + static_var2 + a + b);
    return a;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;使用 &lt;code&gt;objdump -h&lt;/code&gt; 查看段表的信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注意，&lt;code&gt;objdump -h&lt;/code&gt; 只会显示关键的段，可以用 &lt;code&gt;readelf -S&lt;/code&gt; 查看完整的段表信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-5.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注意这里的 .rel.text 段是重定位表，.symtab 段是符号表。下面会重点说明。&lt;/p&gt;

&lt;p&gt;可以使用 &lt;code&gt;objdump -s -d&lt;/code&gt; 查看各个段的内容：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-3.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到 .data 段前后四个字节分别是是 54000000 和 55000000，即十进制 84 和 85。正好对应 global_init_var 和 static_var 这两个变量。.rodata 段存放了 25640a00，即 %d\n 的 ASCII 字节序。&lt;/p&gt;

&lt;p&gt;接下来用 &lt;code&gt;readelf -h&lt;/code&gt; 看看 ELF 文件的头部：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-4.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;入口地址（entry point address）标识了 ELF 程序的入口虚拟地址，操作系统在加载完该程序后从这个入口地址开始执行进程的指令。可重定位文件一般没有入口地址，即值为 0。&lt;/p&gt;

&lt;h3&gt;符号表&lt;/h3&gt;

&lt;p&gt;用 &lt;code&gt;readelf -s&lt;/code&gt; 看看 ELF 文件的符号表：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-6.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Type 列含义如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NOTYPE：未知类型&lt;/li&gt;
&lt;li&gt;OBJECT：数据对象&lt;/li&gt;
&lt;li&gt;FUNC：函数或可执行代码&lt;/li&gt;
&lt;li&gt;SECTION：段&lt;/li&gt;
&lt;li&gt;FILE：文件名&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bind 列含义如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LOCAL：局部符号，对目标文件的外部不可见&lt;/li&gt;
&lt;li&gt;GLOBAL：全局符号&lt;/li&gt;
&lt;li&gt;WEAK：弱引用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果符号定义在本目标文件中，那 Ndx 列表示符号所在段在段表中的下标，如果不是定义在本目标文件中或是特殊符号，含义如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ABS：表明该符号包含了一个绝对值，例如文件名符号&lt;/li&gt;
&lt;li&gt;COMMON：表明该符号是一个 COMMON 块类型的符号，一般未初始化的全局变量就是这种类型&lt;/li&gt;
&lt;li&gt;UNDEF：表明该符号未定义，在本目标文件被引用但定义在其他目标文件中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Value 列含义如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果是符号的定义并且不是 COMMON 块，则表示符号在段中的偏移&lt;/li&gt;
&lt;li&gt;如果是 COMMON 块，则表示对齐属性&lt;/li&gt;
&lt;li&gt;可执行文件中，表示符号的虚拟地址，该虚拟地址对动态链接器来说非常有用&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;静态链接&lt;/h1&gt;

&lt;p&gt;链接器一般采用两步链接的方法来分配空间：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;扫描所有输入目标文件，收集各个段的长度、属性和未位置，并将输入目标文件符号表中的所有符号定义和符号引用汇总到全局符号表。&lt;/li&gt;
&lt;li&gt;读取输入文件中段的数据和重定位信息，进行符号解析与重定位。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;来看一个链接的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;/* a.c */

extern int shared;

int main() 
{
    int a = 100;
    swap(&amp;amp;a, &amp;amp;shared);
}

/* b.c */
int shared = 1;

void swap(int *a, int *b) 
{
    *a ^= *b ^= *a ^= *b;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到 a.c 中引用了 b.c 的两个符号，先用 objdump 看看 a.o 中代码段的反编译结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-11.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当 a.c 被编译成目标文件时，编译器并不知道 share 和 swap 的地址，因此编译器暂时把 地址 0 看成 shared 的地址：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;13: be 00 00 00 00          mov    $0x0,%esi
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;相似的，调用 swap 的时候使用了临时假地址：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;20: e8 00 00 00 00          callq  25 &amp;lt;main+0x25&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;使用 ld 将 a.o 和 b.o 链接起来：&lt;code&gt;ld a.o b.o -e main -o ab&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;再用 objdump 查看链接前后地址分配的情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-8.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-9.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-10.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;VMA 表示虚拟内存地址，LMA 表示加载地址，正常情况下这两个值是一样的。&lt;/p&gt;

&lt;p&gt;可以发现链接前目标文件所有段的 VMA 都是 0，因为虚拟空间还没分配。链接后可执行文件 ab 的各个段都分配了相应的虚拟地址。&lt;/p&gt;

&lt;p&gt;当链接完成，由于各个段地址已经确定，各个符号在段内的相对地址也是固定的，这样各个符号的绝对地址也已经确定了。可以通过 objdump 观察到 ab 文件中符号地址已经修正了：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-12.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实际上，链接器是通过 ELF 文件中的重定位表，来找到需要重定位的符号的。&lt;code&gt;objdump -r&lt;/code&gt; 可以查看重定位表的信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-13.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OFFSET 表示该重定位符号在段中的偏移。&lt;/p&gt;

&lt;h1&gt;装载&lt;/h1&gt;

&lt;p&gt;Linux 下创建一个进程，加载可执行文件并执行，大致步骤如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建一个独立的虚拟地址空间&lt;/li&gt;
&lt;li&gt;读取可执行文件头，建立虚拟空间与可执行文件的映射关系。&lt;strong&gt;Linux 内核的 VMA 结构就是用与建立虚拟空间和可执行文件的映射关系&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;将 CPU 指令寄存器设置为可执行文件的入口地址，启动运行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;操作系统并不关心可执行文件中各个段的实际内容，只关心段的权限。为了减少内存浪费（映射时以页长作为单位），对于相同权限的段，合并到一起当做一个段进行映射。因此 ELF 可执行文件引入了 segment 的概念。一个 segment 包含一个或多个属性相似的 section。描述 segment 的结构叫程序头（program header），可以使用 &lt;code&gt;readelf -l&lt;/code&gt; 查看 segment 信息。&lt;/p&gt;

&lt;p&gt;VMA 除了用以映射可执行文件的各个 segment 以外，还被用来管理进程的地址空间。进程执行需要的栈和堆，也是以 VMA 形式存在的。可以通过 &lt;code&gt;cat /proc/pid/maps&lt;/code&gt; 查看进程的虚拟空间分布。&lt;/p&gt;

&lt;h1&gt;动态链接&lt;/h1&gt;

&lt;p&gt;为了节省空间，简化程序的更新和发布，引入了动态链接的概念。动态链接的基本思想在于&lt;strong&gt;将链接的过程推迟到运行时&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;由于共享对象的加载地址在编译时是不确定的，因此需要在装载时重定位。但由于装载时重定位会修改指令，没有办法做到同一份指令被多个进程共享，因此引入了&lt;strong&gt;地址无关代码&lt;/strong&gt;（PIC）的概念。其基本思想是把指令中那些需要修改的部分抽离出来放到数据部分，这样的话指令部分可以保持不变，被多个进程共享，而数据部分每个进程都有一个副本。&lt;/p&gt;

&lt;p&gt;当使用 PIC 编译时，模块内的数据引用和函数访问是通过相对寻址的方式。模块间的数据引用和函数访问则通过数据段的全局偏移表（GOT）来实现。&lt;/p&gt;

&lt;p&gt;使用 gcc 编译时，指定 &lt;code&gt;-shared&lt;/code&gt; 使用装载时重定位的方法，如果指定了 &lt;code&gt;fPIC&lt;/code&gt; 则产生地址无关的共享对象。&lt;/p&gt;

&lt;p&gt;为了提高动态链接的效率，引入了 PLT 来实现延迟绑定。&lt;/p&gt;

&lt;p&gt;动态链接需要注意&lt;strong&gt;全局符号介入&lt;/strong&gt;的问题，当一个符号需要被加入全局符号表时，如果相同的符号名已经存在，则后加入的符号将被忽略。来看看下面的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;/* a1.c */

#include &amp;lt;stdio.h&amp;gt;

void a()
{
    printf(&amp;quot;a1.c\n&amp;quot;);
}

/* a2.c */
#include &amp;lt;stdio.h&amp;gt;

void a()
{
    printf(&amp;quot;a2.c\n&amp;quot;);
}

/* b1.c */
void a();

void b1()
{
    a();
}

/* b2.c */
void a();

void b2() 
{
    a();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;假设 b1.so 依赖于 a1.so，b2.so 依赖于 a2.so。将 b1.so 与 a1.so 进行链接，b2.so 与 a2.so 进行链接：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;gcc -fPIC -shared a1.c -o a1.so
gcc -fPIC -shared a2.c -o a2.so
gcc -fPIC -shared b1.c a1.so -o b1.so
gcc -fPIC -shared b2.c a2.so -o b2.so
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;当有程序同时使用 b1.c 的函数 b1 和 b2.c 中的函数 b2 时：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;/* main.c */

#include &amp;lt;stdio.h&amp;gt;

void b1();
void b2();

int main()
{
    b1();
    b2();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;编译并运行：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-16.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;观察到输出是两个 a1.c，意味着 a2.c 的 a() 函数被忽略了。&lt;/p&gt;

&lt;p&gt;如果是采用静态编译，则链接时会报重定义的错误：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-17.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再来看看另一个有趣的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;/* foo.c */
#include &amp;lt;stdio.h&amp;gt;

struct {
    int a;
    int b;
} b = { 3, 3 };

int main();

void foo()
{
    b.a = 4;
    b.b = 4;
    printf(&amp;quot;foo: b.a=%d, b.b=%d\n&amp;quot;, b.a, b.b);
}

/* t1.c */
#include &amp;lt;stdio.h&amp;gt;

int b = 1;
int c = 1;

int main()
{
    int count = 5;
    while (count-- &amp;gt; 0) {
        t2();
        foo();
        printf(&amp;quot;t1: b=%d, c=%d\n&amp;quot;, b, c);
        sleep(1);
    }
    return 0;
}

/* t2.c */
#include &amp;lt;stdio.h&amp;gt;

int b;
int c;

int t2()
{
    printf(&amp;quot;t2: b=%d, c=%d\n&amp;quot;, b, c);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;编译：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;gcc -shared -fPIC foo.c -o foo.so 
gcc  t1.c t2.c foo.so -o test -Xlinker -rpath ./
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;输出结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-18.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其实在动态链接时，foo.c 里定义的 struct b 符号被忽略了（因为 t1.c 里已经定义了符号 b）。因此在后续调用 foo() 时，&lt;code&gt;b.a = 4&lt;/code&gt; 中 b.a 的地址其实是 t1.c 文件中变量 b 的地址，&lt;code&gt;b.b = 4&lt;/code&gt; 中 b.b 的地址其实是 t1.c 文件中变量 c 的地址（因为 t1.c 中变量 b 和 c 的地址刚好相邻）。&lt;/p&gt;

&lt;p&gt;由于可能存在全局符号介入的问题，模块内函数的调用不能用相对地址调用，编译器会将其当做模块外部符号来处理，使用 .got.plt 进行重定位。因此为了提高模块内函数调用的效率，&lt;strong&gt;建议使用 static 关键字将被调用的函数设置为编译单元私有函数&lt;/strong&gt;。此时编译器会采用相对地址的编译方式，因为能确保该函数不会被其他模块所覆盖。&lt;/p&gt;

&lt;h1&gt;内存&lt;/h1&gt;

&lt;p&gt;下图为 Linux 进程内存空间的典型布局：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-14.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中 dynamic libraries 用于映射装载的动态链接库。&lt;/p&gt;

&lt;h2&gt;栈&lt;/h2&gt;

&lt;p&gt;栈在程序运行中起了举足轻重的地位。栈保存了一个函数调用所需要的维护信息，通常称为堆栈帧（Stack Frame）。通常有以下内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;函数的返回地址和参数&lt;/li&gt;
&lt;li&gt;临时变量，包括函数的非静态局部变量以及编译器自动生成的其他临时变量&lt;/li&gt;
&lt;li&gt;保存的上下文，包括函数调用前后需要保存的寄存器&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;i386 中，esp 寄存器始终指向栈顶，ebp 寄存器指向了堆栈帧的一个固定位置，因此 ebp 又称为帧指针（Frame Pointer），如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/linker-loader-library-note/llustration-15.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;i386 的函数调用过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;把参数压入栈中&lt;/li&gt;
&lt;li&gt;把当前指令的下一条指令的地址压入栈中&lt;/li&gt;
&lt;li&gt;跳转到函数体执行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;i386 的函数体开头大致如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;push ebp: 把旧的 ebp 压入栈中 &lt;/li&gt;
&lt;li&gt;mov ebp, esp: 将 esp 的值赋给 ebp，此时 ebp 和 esp 都指向栈顶，栈顶保存的就是 old ebp&lt;/li&gt;
&lt;li&gt;sub esp, XXX: 可选，在栈上分配 XXX 字节的临时空间&lt;/li&gt;
&lt;li&gt;push XXX: 可选，如有必要，保存名为 XXX 的寄存器 &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;函数返回的流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pop XXX: 可选，如有必要，恢复保存过的寄存器的值&lt;/li&gt;
&lt;li&gt;mov esp, ebp: 恢复 ESP，回收局部变量的空间&lt;/li&gt;
&lt;li&gt;pop ebp: 从栈顶恢复旧的 ebp 的值&lt;/li&gt;
&lt;li&gt;ret: 从栈顶取得返回地址，并跳转到该位置&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;eax 寄存器通常是函数返回值的通道。函数将返回值存储在 eax 中，返回后调用方读取 eax 得到函数的返回值。由于 eax 只有 4 个字节，如果返回的是 5~8 字节对象的情况，惯例是采用 eax 和 edx 联合返回的方式。超过 8 字节的，则通过临时变量的方式来实现：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用者在栈上开辟了一片空间，并且将这块空间的一部分作为传递返回值的临时对象 temp&lt;/li&gt;
&lt;li&gt;将 temp 对象的地址作为隐藏参数传递给被调用的函数&lt;/li&gt;
&lt;li&gt;被调用的函数将返回值拷贝给 temp 对象，并将 temp 对象的地址用 eax 传出&lt;/li&gt;
&lt;li&gt;返回后，调用者只需要拷贝 eax 指向的 temp 对象的内容就能得到返回值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这也是为什么不能直接对函数返回值进行取址的原因，因为函数返回后临时变量 temp 就被释放掉了。&lt;/p&gt;

&lt;h2&gt;堆&lt;/h2&gt;

&lt;p&gt;Linux 提供了两种堆空间分配的方式：brk() 和 mmap()。&lt;/p&gt;

&lt;p&gt;brk() 的作用是设置进程数据段的结束地址，它可以扩大或缩小数据段。mmap() 的作用是向操作系统申请一段虚拟地址空间，可以是映射到某个文件也可以是匿名空间。&lt;/p&gt;

&lt;p&gt;glibc 的 malloc 函数，对小于 128 KB 的请求来说，会在现有的堆空间里按照堆分配算法分配一块空间并返回。对于大于 128 KB 的请求来说，它会使用 mmap() 函数分配一块匿名空间。&lt;/p&gt;
</description>
        <pubDate>Sat, 25 Feb 2017 10:55:19 +0800</pubDate>
        <link>http://masutangu.com/2017/02/linker-loader-library-note/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/02/linker-loader-library-note/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Linux 性能监控</title>
        <description>&lt;p&gt;本文是对 &lt;a href=&quot;http://www.vpsee.com/2009/11/linux-system-performance-monitoring-introduction/&quot;&gt;《Linux 性能监测》&lt;/a&gt; 系列文章的读书笔记，并在此基础上丰富。&lt;/p&gt;

&lt;h2&gt;CPU 相关&lt;/h2&gt;

&lt;h3&gt;vmstat&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0 164412 143200  35916 243216    0    0    12    11   46   22  1  1 98  0  0
 0  0 164412 142960  35916 243260    0    0     0     0  583  854  2  1 97  0  0
 0  0 164412 142960  35916 243260    0    0     0     0  940 1184  2  1 97  0  0
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;r:    可运行队列的线程数，这些线程都是可运行状态，只不过 CPU 暂时不可用&lt;/li&gt;
&lt;li&gt;b:    被 blocked 的进程数，正在等待 IO 请求&lt;/li&gt;
&lt;li&gt;in:   被处理过的中断数&lt;/li&gt;
&lt;li&gt;cs:   系统上正在做上下文切换的数目&lt;/li&gt;
&lt;li&gt;us:   用户占用 CPU 的百分比&lt;/li&gt;
&lt;li&gt;sy:   内核和中断占用 CPU 的百分比&lt;/li&gt;
&lt;li&gt;wa:   所有可运行的线程被 blocked 以后都在等待 IO，这时候 CPU 空闲的百分比&lt;/li&gt;
&lt;li&gt;id:   CPU 完全空闲的百分比&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;合理的参数范围：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CPU 利用率：如果 CPU 达到 100％ 利用率，那么 us 和 sy 占比应该达到这样一个平衡：65％－70％ User Time，30％－35％ System Time，0％－5％ Idle Time&lt;/li&gt;
&lt;li&gt;上下文切换：上下文切换应该和 CPU 利用率联系起来看，如果能保持上面的 CPU 利用率平衡，大量的上下文切换是可以接受的&lt;/li&gt;
&lt;li&gt;可运行队列：每个处理器的可运行队列不应该超过3个线程，比如：双处理器系统的可运行队列里不应该超过6个线程&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;案例 1：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 4  0    140 2915476 341288 3951700  0    0     0     0 1057  523 19 81  0  0  0
 4  0    140 2915724 341296 3951700  0    0     0     0 1048  546 19 81  0  0  0
 4  0    140 2915848 341296 3951700  0    0     0     0 1044  514 18 82  0  0  0
 4  0    140 2915848 341296 3951700  0    0     0    24 1044  564 20 80  0  0  0
 4  0    140 2915848 341296 3951700  0    0     0     0 1060  546 18 82  0  0  0
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;上面的输出可以看出：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;system time（sy）一直保持在 80％ 以上，而且上下文切换较低（cs），说明某个进程可能一直霸占着 CPU&lt;/li&gt;
&lt;li&gt;run queue（r）刚好在4个&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;案例 2：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
14  0    140 2904316 341912 3952308  0    0     0   460 1106 9593 36 64  1  0  0
17  0    140 2903492 341912 3951780  0    0     0     0 1037 9614 35 65  1  0  0
20  0    140 2902016 341912 3952000  0    0     0     0 1046 9739 35 64  1  0  0
17  0    140 2903904 341912 3951888  0    0     0    76 1044 9879 37 63  0  0  0
16  0    140 2904580 341912 3952108  0    0     0     0 1055 9808 34 65  1  0  0
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;上面的输出可以看出：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;context switch（cs）比 interrupts（in）要高得多，说明内核不得不来回切换进程&lt;/li&gt;
&lt;li&gt;进一步观察发现 system time（sy）很高而 user time（us）很低，而且加上高频度的上下文切换（cs），说明正在运行的应用程序调用了大量的系统调用&lt;/li&gt;
&lt;li&gt;run queue（r）在14个线程以上，按照这个测试机器的硬件配置（四核），应该保持在12个以内。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;案例 3：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 5  0     92 185724 735036 2736384   0    0     2    11    0    1  2  0 98  0  0
 7  0     92 184980 735044 2736376   0    0     0   124 6682 7806 39  3 58  0  0
 6  0     92 184856 735044 2737064   0    0     0  1888 6721 7601 38  5 49  8  0
 2  0     92 184732 735044 2737064   0    0     0     0 6549 7525 46  4 50  0  0
 3  0     92 183988 735044 2737904   0    0     0     0 6375 7081 45  3 52  0  0
 1  0     92 184360 735048 2738156   0    0     0     0 6764 7601 44  4 51  0  0 
 8  1     92 183368 735048 2738508   0    0     0  1384 6774 8005 42  4 54  0  0
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;cpu 利用率上不去，vmstat 输出如上，分析可能的瓶颈：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;si、so 都是 0，free 也有，说明内存足够，排除内存。&lt;/li&gt;
&lt;li&gt;bi、bo 都不大，所以 io 似乎不严重，排除硬盘&lt;/li&gt;
&lt;li&gt;cs、in 都较大，说明 CPU 频于应付上下文切换和中断，而且从 r 的数字来看正在等 CPU 的进程较多，所以猜测服务器上运行的进程较多，CPU 疲于切换进程以及应付中断，猜测瓶颈是 CPU。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;mpstat&lt;/h3&gt;

&lt;p&gt;mpstat 和 vmstat 类似，不同的是 mpstat 可以输出多个处理器的数据。&lt;/p&gt;

&lt;h3&gt;ps&lt;/h3&gt;

&lt;p&gt;ps 用于查看某个程序、进程占用的 CPU 资源。&lt;/p&gt;

&lt;h3&gt;实践：定位 CPU 100% 的问题&lt;/h3&gt;

&lt;p&gt;某个进程 CPU 100% 了，如何定位？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果是多线程，使用 &lt;code&gt;ps -eL |grep 进程id&lt;/code&gt;，找出占用 CPU 最长时间的线程 id&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;strace -p 线程id -tt&lt;/code&gt; 观察调用的系统调用&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;perf top&lt;/code&gt; 看看哪个函数占用率最高&lt;/li&gt;
&lt;li&gt;或使用 &lt;code&gt;watch pstack 线程 id&lt;/code&gt; 看看调用堆栈&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之前工作中遇到过 CPU 使用率很高，通过 &lt;code&gt;perf top&lt;/code&gt; 和 &lt;code&gt;pstack&lt;/code&gt; 观察到写磁盘操作很多，推测是日志打印过多导致 CPU 使用率暴涨，调低了日志等级后顺利解决。&lt;/p&gt;

&lt;h2&gt;内存相关&lt;/h2&gt;

&lt;p&gt;kswapd daemon 用来检查 pages_high 和 pages_low，如果可用内存少于 pages_low，kswapd 就开始扫描并试图释放 32个页面，并且重复扫描释放的过程直到可用内存大于 pages_high 为止。&lt;/p&gt;

&lt;p&gt;扫描的时候检查3件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果页面没有修改，把页放到可用内存列表里&lt;/li&gt;
&lt;li&gt;如果页面被文件系统修改，把页面内容写到磁盘上&lt;/li&gt;
&lt;li&gt;如果页面被修改了，但不是被文件系统修改的，把页面写到交换空间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pdflush daemon 用来同步文件相关的内存页面，把内存页面及时同步到硬盘上。比如打开一个文件，文件被导入到内存里，对文件做了修改后并保存后，内核并不马上保存文件到硬盘，由 pdflush 决定什么时候把相应页面写入硬盘，这由一个内核参数 vm.dirty_background_ratio 来控制，比如下面的参数显示脏页面（dirty pages）达到所有内存页面10％的时候开始写入硬盘。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# /sbin/sysctl -n vm.dirty_background_ratio
10
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h3&gt;vmstat&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0 164412 143200  35916 243216    0    0    12    11   46   22  1  1 98  0  0
 0  0 164412 142960  35916 243260    0    0     0     0  583  854  2  1 97  0  0
 0  0 164412 142960  35916 243260    0    0     0     0  940 1184  2  1 97  0  0
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;swpd: 已使用的 SWAP 空间大小，KB 为单位&lt;/li&gt;
&lt;li&gt;free: 可用的物理内存大小，KB 为单位&lt;/li&gt;
&lt;li&gt;buff: 物理内存用来缓存读写操作的 buffer 大小，KB 为单位&lt;/li&gt;
&lt;li&gt;cache: 物理内存用来缓存进程地址空间的 cache 大小，KB 为单位&lt;/li&gt;
&lt;li&gt;si: 数据从 SWAP 读取到 RAM（swap in）的大小，KB 为单位&lt;/li&gt;
&lt;li&gt;so: 数据从 RAM 写到 SWAP（swap out）的大小，KB 为单位&lt;/li&gt;
&lt;li&gt;bi: 从块设备读取（block in）的大小，block 为单位&lt;/li&gt;
&lt;li&gt;bo: 写入块设备（block out）的大小，block 为单位&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache    si    so    bi    bo   in   cs us sy id  wa st
 0  3 252696   2432    268   7148  3604  2368  3608  2372  288  288  0  0 21  78  1
 0  2 253484   2216    228   7104  5368  2976  5372  3036  930  519  0  0  0 100  0
 0  1 259252   2616    128   6148 19784 18712 19784 18712 3821 1853  0  1  3  95  1
 1  2 260008   2188    144   6824 11824  2584 12664  2584 1347 1174 14  0  0  86  0
 2  1 262140   2964    128   5852 24912 17304 24952 17304 4737 2341 86 10  0   0  4
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;上面是一个频繁读写交换区的例子，可以观察到：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;buff 稳步减少说明系统知道内存不够了，kwapd 正在从 buff 那里借用部分内存&lt;/li&gt;
&lt;li&gt;kswapd 持续把脏页面写到 swap 交换区（so），并且从 swapd 逐渐增加看出确实如此。根据上面讲的 kswapd 扫描时检查的三件事，如果页面被修改了，但不是被文件系统修改的，把页面写到 swap，所以这里 swapd 持续增加&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;free&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ free
             total       used       free     shared    buffers     cached
Mem:       1014432     884460     129972      10168      43468     248580
-/+ buffers/cache:     592412     422020
Swap:      1046524     164376     882148
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;total: 总内存大小&lt;/li&gt;
&lt;li&gt;used: 已经使用的内存大小，包含 cached、buffers 和 shared 部分&lt;/li&gt;
&lt;li&gt;free: 空闲的内存大小&lt;/li&gt;
&lt;li&gt;shared: 进程间共享内存&lt;/li&gt;
&lt;li&gt;buffers: buffer cache&lt;/li&gt;
&lt;li&gt;cached: page cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-/+ buffers/cache 看作两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-buffers/cache：正在使用的内存大小，其值等于used1 减去 buffers1 再减去 cached1&lt;/li&gt;
&lt;li&gt;+buffers/cache：可用的内存大小，其值等于 free1 加上 buffers1 再加上 cached&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当空闲物理内存不多时，不一定表示系统运行状态很差，因为内存的 cached 及 buffers 部分可以随时被重用。swap 如果被频繁调用，bi 和 bo 长时间不为 0，才是内存资源是否紧张的依据。通过 free 看资源时，实际主要关注 -/+ buffers/cache 的值就可以知道内存到底够不够了。&lt;/p&gt;

&lt;h3&gt;valgrind&lt;/h3&gt;

&lt;p&gt;valgrind 是定位内存泄漏的好工具，使用 &lt;code&gt;valgrind --lead-check=full --log-file=valgrind.log ./a.out&lt;/code&gt; 即可在进程结束运行后输出内存泄露报告。&lt;/p&gt;

&lt;h3&gt;maps, smaps and status&lt;/h3&gt;

&lt;p&gt;参考这篇文章：&lt;a href=&quot;https://jameshunt.us/writings/smaps.html&quot;&gt;maps, smaps and Memory Stats!&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;pmap&lt;/h3&gt;

&lt;p&gt;待补充&lt;/p&gt;

&lt;h2&gt;IO 相关&lt;/h2&gt;

&lt;h3&gt;Buffers &amp;amp; Cached&lt;/h3&gt;

&lt;p&gt;Linux 利用虚拟内存极大的扩展了程序地址空间，使得原来物理内存不能容下的程序也可以通过内存和硬盘之间的不断交换（把暂时不用的内存页交换到硬盘，把需要的内存页从硬盘读到内存）来赢得更多的内存，看起来就像物理内存被扩大了一样。如果数据不在内存里就引起一个缺页中断（Page Fault），然后从硬盘读取缺页，并把缺页缓存到物理内存里。缺页中断可分为主缺页中断（Major Page Fault）和次缺页中断（Minor Page Fault），要&lt;strong&gt;从磁盘读取数据而产生的中断是主缺页中断&lt;/strong&gt;；数据已经被读入内存并被缓存起来，&lt;strong&gt;从内存缓存区中而不是直接从硬盘中读取数据而产生的中断是次缺页中断&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;从内存缓存区（页高速缓存）读取页比从硬盘读取页要快得多，所以 Linux 内核希望能尽可能产生次缺页中断（从页高速缓存读），并且尽可能避免主缺页中断（从硬盘读），这样随着次缺页中断的增多，缓存区也逐步增大，直到系统只有少量可用物理内存的时候 Linux 才开始释放一些不用的页。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ cat /proc/meminfo
MemTotal:        1014432 kB
MemFree:          137884 kB
Buffers:           43732 kB
Cached:           248744 kB
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;这台服务器总共有 10GB 物理内存（MemTotal），1GB 左右可用内存（MemFree），43 MB 左右用来做磁盘缓存（Buffers），248 MB 左右用来做文件缓存区（Cached）。&lt;/p&gt;

&lt;p&gt;Linux 中内存页面有三种类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read pages: 只读页（或代码页），通过主缺页中断从硬盘读取的页面，包括不能修改的静态文件、可执行文件、库文件等。当内核需要它们的时候把它们读到内存中，当内存不足的时候，内核就释放它们到空闲列表，当程序再次需要它们的时候需要通过缺页中断再次读到内存。&lt;/li&gt;
&lt;li&gt;Dirty pages: 脏页，指在内存中被修改过的数据页，比如文本文件等。这些文件由 pdflush 负责同步到硬盘，内存不足的时候由 kswapd 和 pdflush 把数据写回硬盘并释放内存。&lt;/li&gt;
&lt;li&gt;Anonymous pages: 匿名页，属于某个进程但是又和任何文件无关联，不能被同步到硬盘上，内存不足的时候由 kswapd 负责将它们写到交换分区并释放内存。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;顺序 IO 和 随机 IO&lt;/h3&gt;

&lt;p&gt;IO 可分为顺序 IO 和 随机 IO 两种。顺序 IO 重视每次 IO 的吞吐能力（KB per IO），而随机 IO 重视的是每秒能 IOPS 的次数，而不是每次 IO 的吞吐能力（KB per IO）。&lt;/p&gt;

&lt;h3&gt;Swap&lt;/h3&gt;

&lt;p&gt;当系统没有足够物理内存来应付所有请求的时候就会用到 swap 设备，swap 设备可以是一个文件，也可以是一个磁盘分区。使用 swap 的代价非常大，如果系统没有物理内存可用，就会频繁 swapping。如果 swap 设备和程序正要访问的数据在同一个文件系统上，那会碰到严重的 IO 问题，最终导致整个系统迟缓，甚至崩溃。swap 设备和内存之间的 swapping 状况是判断 Linux 系统性能的重要参考，我们已经有很多工具可以用来监测 swap 和 swapping 情况，比如：top、cat /proc/meminfo、vmstat。&lt;/p&gt;

&lt;h2&gt;Network 相关&lt;/h2&gt;

&lt;h3&gt;iptraf&lt;/h3&gt;

&lt;p&gt;两台主机之间有网线（或无线）、路由器、交换机等设备，测试两台主机之间的网络性能的一个办法就是在这两个系统之间互发数据并统计结果，看看吞吐量、延迟、速率如何。iptraf 就是一个很好的查看本机网络吞吐量的好工具。&lt;/p&gt;

&lt;h3&gt;tcpdump&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;sudo tcpdump -i eth0 port 80  -w output.dump    // 指定端口
sudo tcpdump -i eth0 src port 80                // 指定源端口
sudo tcpdump -i eth0 dst port 80                // 指定目的端口
sudo tcpdump -i eth0 src host 192.168.1.1       // 指定源地址
sudo tcpdump -i eth0 dst host 192.168.1.1       // 指定目的地址
sudo tcpdump -i eth0 tcp                        // 协议过滤
sudo tcpdump -i eth0 udp                        // 协议过滤
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h3&gt;netstat&lt;/h3&gt;

&lt;p&gt;使用 netstat 实时监控 udp 网络收发包情况：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ watch netstat -su

IcmpMsg:
    InType0: 19
    InType3: 55
    InType8: 918828
    InType11: 5
    OutType0: 918828
    OutType3: 55
    OutType8: 20
Udp:
    101881 packets received
    51 packets to unknown port received.
    0 packet receive errors  // 关注该项 如果持续增长，说明在丢包。网卡收到了，但是应用层来不及处理
    164740 packets sent
UdpLite:
IpExt:
    InNoRoutes: 8
    OutMcastPkts: 2
    InBcastPkts: 1938486
    InOctets: 26851362935
    OutOctets: 18903227033
    OutMcastOctets: 80
    InBcastOctets: 300768203
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h2&gt;常用工具汇总&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;工具&lt;/th&gt;
&lt;th&gt;简单介绍&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;top&lt;/td&gt;
&lt;td&gt;查看进程活动状态以及一些系统状况&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmstat&lt;/td&gt;
&lt;td&gt;查看系统状态、硬件和系统信息等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iostat&lt;/td&gt;
&lt;td&gt;查看CPU 负载，硬盘状况&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sar&lt;/td&gt;
&lt;td&gt;综合工具，查看系统状况&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mpstat&lt;/td&gt;
&lt;td&gt;查看多处理器状况&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;netstat&lt;/td&gt;
&lt;td&gt;查看网络状况&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iptraf&lt;/td&gt;
&lt;td&gt;实时网络状况监测&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tcpdump&lt;/td&gt;
&lt;td&gt;抓取网络数据包，详细分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tcptrace&lt;/td&gt;
&lt;td&gt;数据包分析工具&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;netperf&lt;/td&gt;
&lt;td&gt;网络带宽工具&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dstat&lt;/td&gt;
&lt;td&gt;综合工具，综合了 vmstat, iostat, ifstat, netstat 等多个信息&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;相关资料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.thomas-krenn.com/en/wiki/Linux_Performance_Measurements_using_vmstat&quot;&gt;《Linux Performance Measurements using vmstat》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://linuxwiki.github.io/NetTools/tcpdump.html&quot;&gt;《tcpdump使用技巧》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/&quot;&gt;《Perf -- Linux下的系统性能调优工具》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.51testing.com/html/56/490256-3711169.html&quot;&gt;《通过/proc查看Linux内核态调用栈来定位问题》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://linuxtools-rst.readthedocs.io/zh_CN/latest/advance/03_optimization.html&quot;&gt;《Linux性能优化》&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Feb 2017 22:57:39 +0800</pubDate>
        <link>http://masutangu.com/2017/02/linux-performance-monitor/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/02/linux-performance-monitor/</guid>
        
        
        <category>工作</category>
        
      </item>
    
      <item>
        <title>我的 2016</title>
        <description>&lt;p&gt;时间飞逝，又到了写年终总结的时候了。翻回看去年的总结和展望，回想起去年的雄心壮志，今年则是坎坷、困惑与焦虑的一年。&lt;/p&gt;

&lt;h1&gt;回顾过去&lt;/h1&gt;

&lt;p&gt;在这整整一年中，我所思考的和困扰的，可以由三个词来概括：&lt;strong&gt;成绩&lt;/strong&gt;、&lt;strong&gt;成长&lt;/strong&gt;和&lt;strong&gt;价值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;工作已经两年半，对自己目前取得的成绩，对自己技术成长的预期，对自己为团队创造的价值，实话实说，都不满意。&lt;/p&gt;

&lt;p&gt;从今年年初开始，我就有点焦虑，觉得自己没有太多的成长，一直在原地踏步，已经没有刚入职时那种回顾过去会觉得自己突飞猛进的感觉（工作后做的每个比较大的需求，我都会整理保存下来 ppt，回看时就会清楚自己相比起过去进步了多少）。困于瓶颈期时，我尝试更加主动的去做事情。在那段时间，我开始关注些不仅仅是技术上的知识，例如关注了&lt;a href=&quot;https://wanqu.co/&quot;&gt;《湾区日报》&lt;/a&gt;，做了些读书笔记&lt;a href=&quot;http://masutangu.com/2015/12/dewdrop-note-1/&quot;&gt;《水滴石穿》&lt;/a&gt;。那段时间有些收获，但当我尝试把我阅读中收获的这些想法，运用到工作中时，却发现有层层阻碍。其中细节就不展开了，这让我非常沮丧。一方面，我理解在大公司工作，最重要是“稳”。有新的方案，新的想法，首要考虑的不是这个新的方案、新的想法的收益，而是运维成本。另一方面，这也让我觉得在大公司工作非常乏味，每个人的贡献都是固定的，尤其是工程师，你的定位就是执行者。公司也已经有很多很成熟的组件，即使不太适应于业务场景，也能勉强用用，你的工作就是做做接入，其它不需要操心。&lt;/p&gt;

&lt;p&gt;这种状况让我觉得非常的不适应。在学生时代，每一阶段、每个学期你都会学习到新的知识。而在工作中，更多的时候你是在重复做些体力活，例如，复制粘贴的完成一些需求。又例如，一遍遍的用已经成熟的方案去解决一些问题。而我其实不太喜欢这样的工作方式。我蛮希望自己是一个比较有创造力的工程师，我倾向于用不同的方案去完成类似的需求，通过真正的实践来对比不同的方案的优劣性。但在大公司来说，&lt;strong&gt;进度&lt;/strong&gt;和&lt;strong&gt;稳定性&lt;/strong&gt;是首要目标。只有在发展迅猛的前沿/明星项目中，你才有可能遇到各种新的问题，才有得到锻炼的机会。&lt;/p&gt;

&lt;p&gt;互联网不是按照工龄来排资论辈，同样工作三五年，差距可能非常大。有一句话说得好，&lt;strong&gt;“Good judgment comes from experience, experience comes from bad judgment.”&lt;/strong&gt;（正确的判断来自于丰富的经验,而丰富的经验来自错误的判断）。如果工作中没有太多挑战，不能促使你学习进步以应对工作的要求，一味追求保守无法得到犯错踩坑进步的机会，那你就应该停下来思考自己的未来了。我非常不希望把工作当做是一个任务，我希望工作是融于生活的，能带给我挑战和战胜难题的动力，能带给我解决问题后的成就感。如果说做为工程师，工作于己如同体力劳动般每日重复，那和流水线工人又有什么区别呢？&lt;/p&gt;

&lt;p&gt;再一个让我不适应的，是大公司的考核机制。并不像在学生时代，你的排名和你考出来的成绩相关，和你上课积不积极听讲，有没经常举手回答问题，一点关系都没有。所以你的成绩很差，抱歉，是你不够努力，你知道是自己的原因，所以你会努力去学习让自己进步。工作则不是如此简单纯粹。由于工作中有挑战的难点并不多，每个人做的事情都差不多，那只好看谁做事比较积极主动（并非吐槽，我也非常理解）。而我不太喜欢“伪加班”，也希望工作完成后能有更多自己的时间去学习充电。因此，工作后自己取得的绩效非常平庸。我意识到再这样下去，职业生涯也许就这样了。做为一个已经不算新人的员工来说，我渴望做出成绩，希望能为团队贡献更多的价值。我觉得自己还有很多的能量，我能做的不仅仅是完成我手上的工作。我也意识需要到达一定的职位，才能去推动些什么，去改变些什么。但以我这平庸的考核，我对自己未来的发展不抱有太多的信心，所以我开始寻求改变。&lt;/p&gt;

&lt;p&gt;既然不想再在大公司做了，我开始寻找一些有意思的创业公司。在 v2ex 上逛的时候，&lt;a href=&quot;https://www.xiaohongshu.com/&quot;&gt;小红书&lt;/a&gt; 吸引了我的注意。CTO 来自前谷歌中国工程院副院长，团队很多海归名校的背景，非常有吸引力。在经过电面，飞到上海去面试后，我拿到了offer。然而犹豫再三，我还是放弃了。放弃的原因主要是因为我资历尚浅，5月份拿到 offer，那时我才工作了一年多，当时小红书的开发团队已经有几十人，已经算是个大团队了。但不得不说，面试官给我的感觉非常的好，如果是在更早期，团队还是小规模的时候，我一定愿意加入这样的团队。&lt;/p&gt;

&lt;p&gt;之后就在公司内部寻找些机会，7月份到了现在的游戏工作室。小团队，后台只有几个人，听起来很有成就感。转岗之前我也有点小担心，担心难以融入，担心过来后也是打打杂，幸好过来后发现多虑了。工作室给我的感觉，很明显，大家共同的目标就是把游戏做好。当然从做社交，到做游戏，一开始还是有些不适应。做社交，本质上是各类存储的设计。游戏则不太一样，复杂的都是在业务逻辑。&lt;/p&gt;

&lt;p&gt;下半年的工作，新的环境，新的方向。学习了新的后台框架，对比之前用的框架，自己造了个轮子，写了篇文章&lt;a href=&quot;http://masutangu.com/2016/08/simple-async-framework/&quot;&gt;《简单异步应用框架的实现》&lt;/a&gt;，对后台异步框架的设计有一些了解，不再局限于只会用框架了。浅读了 libuv 和 libco 的源码，同样写了两篇文章&lt;a href=&quot;http://masutangu.com/2016/10/libuv-source-code/&quot;&gt;《Libuv 源码阅读》&lt;/a&gt; 和 &lt;a href=&quot;http://masutangu.com/2016/10/learn-libco/&quot;&gt;《浅读 Libco》&lt;/a&gt;，对异步和协程两种模式都有了进一步的认识。之后，在日常查问题的时候，我意识到自己定位问题的能力有所缺失，主要是对 linux 的各种监控系统状态的命令还不熟悉，操作系统和网络的知识也忘掉差不多了，所以决定重新翻读下 《UNIX 环境高级编程》、《现代操作系统》，并结合 《Linux 内核设计与实现》，了解些操作系统的理论和实现，这个系列的文章&lt;a href=&quot;http://masutangu.com/2016/11/linux-kernel-serial-1/&quot;&gt;《Linux 内核系列－进程》&lt;/a&gt; 还没写完。&lt;/p&gt;

&lt;p&gt;经过这阵子的摸索，我明确了之后的突破点，找到了进步的方向。虽然说，这一年，我有很多困惑、迷茫和焦虑，心情经常非常差，情绪起伏也很大。我明白这些都是成长的必经之路，这么看来，这些起起伏伏，也是蛮值得的。&lt;/p&gt;

&lt;h1&gt;展望未来&lt;/h1&gt;

&lt;p&gt;新的一年，工作上认真努力，不必多说。这里来聊聊新的一年，个人方面，我想做些什么。&lt;/p&gt;

&lt;p&gt;今年我意识到，对于大多数项目来说，技术并不是最重要的，只要不出问题就好（虽然我热爱技术，但我也不得不承认这点）。那技术人员应该如何发挥更大的价值呢？&lt;/p&gt;

&lt;p&gt;我给自己定的目标是：&lt;strong&gt;提高核心竞争力&lt;/strong&gt;和&lt;strong&gt;变得更全面&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;提升技术能力，这是首要目标。我希望新的一年可以把基础打扎实了，作为后台开发，操作系统和网络知识要打牢。另外了解业界流行的中间件（消息队列、数据库等）的实现，比较深入的选择一个方向（游戏引擎、数据分析、图形学、编程语言等）去研究。&lt;/p&gt;

&lt;p&gt;另外在公司工作，技能点很容易变得非常窄。我希望自己可以变得更加全面，不希望把自己局限于后台开发，也不仅局限于技术。新的一年，我希望自己能够做一个上线的 side project 并用心运营。目前看来我应该会选择做一款独立游戏，技术上独立完成前台－接入层－后台的实现，也借此学习下游戏策划的一些思想，让自己更加了解游戏这个行业。&lt;/p&gt;

&lt;h1&gt;总结心得&lt;/h1&gt;

&lt;p&gt;今天读了陈皓的&lt;a href=&quot;http://coolshell.cn/articles/17583.html&quot;&gt;《技术人员的发展之路》&lt;/a&gt;，写的很好，推荐大家都读一读。这一年，有两句话，我经常对自己说的，一是：&lt;strong&gt;不忘初心&lt;/strong&gt;，二是：&lt;strong&gt;Pursue excellence, and success will follow&lt;/strong&gt; (追求卓越，成功就会如期而至)。&lt;/p&gt;

&lt;p&gt;不忘初心，是提醒自己，选择了计算机的道路，是因为我喜欢编程，喜欢创造。不要因为眼前的失意去妥协自己。希望自己工作三年，五年，十年，都能保持学生的心态。&lt;strong&gt;stay hungry, stay foolish&lt;/strong&gt;。 &lt;/p&gt;

&lt;p&gt;追求卓越，是因为我相信，让自己成为一个优秀的工程师，比拿到好的绩效更加重要。自己要为自己的职业生涯负责，更勇敢的去追求梦想。&lt;/p&gt;

&lt;p&gt;大公司工作，可以带给你光环，但自己要想清楚，去掉光环后，自己还剩下些什么。也许我们都是平凡人，但平凡人也希望做出稍微那么不平凡的成绩。2017 年，希望自己不留遗憾！&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Dec 2016 22:19:43 +0800</pubDate>
        <link>http://masutangu.com/2016/12/review/</link>
        <guid isPermaLink="true">http://masutangu.com/2016/12/review/</guid>
        
        
        <category>随笔</category>
        
      </item>
    
  </channel>
</rss>
