<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Masutangu</title>
    <description>也許我這一生　始終在追逐那顆九號球</description>
    <link>http://masutangu.com/</link>
    <atom:link href="http://masutangu.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 02 Apr 2018 23:32:34 +0800</pubDate>
    <lastBuildDate>Mon, 02 Apr 2018 23:32:34 +0800</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>Programming Language 课程笔记</title>
        <description>&lt;p&gt;本文是学习&lt;a href=&quot;https://www.coursera.org/learn/programming-languages-part-b/home/info&quot;&gt;Coursera Programming Language&lt;/a&gt;课程的学习笔记，文章内容及代码均取自课程材料。&lt;/p&gt;

&lt;h1&gt;Interpreter or Compiler&lt;/h1&gt;

&lt;p&gt;实现编程语言的 workflow 如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/programming-language-b/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Parser 读取程序文本，检查 syntax，如果语法正确则输出 AST（abstract syntax tree）。如果该编程语言有 type checker，则将 AST 丢给 type checker 检查，通过 type check 后，就由 interpreter 或 compile 来运行程序并输出结果。&lt;/p&gt;

&lt;p&gt;在实现编程语言 B 通常有下面两种办法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用另一种编程语言 A 来实现 interpreter（命名为 evaluator 或 executor 更恰当），输入 B 语言写的代码，输出结果&lt;/li&gt;
&lt;li&gt;使用另一种编程语言 A 实现 compiler（命名为 translator 更恰当），将 B 翻译成第三种编程语言 C&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Skipping Parsing&lt;/h1&gt;

&lt;p&gt;如果基于编程语言 A 来实现编程语言 B，就可以跳过 parsing 阶段：&lt;strong&gt;Have B programmers write ASTs directly in PL A&lt;/strong&gt;&lt;/p&gt;

&lt;h1&gt;ML from a Racket perspective&lt;/h1&gt;

&lt;p&gt;ML is like a well-defined subset of Racket&lt;/p&gt;

&lt;h1&gt;Racket from an ML Perspective&lt;/h1&gt;

&lt;p&gt;One way to describe Racket is that &lt;strong&gt;it has “one big datatype”&lt;/strong&gt;：all values have this type.&lt;/p&gt;

&lt;p&gt;我的理解由于 Racket 是 dynamic typing，所以 ML 程序员看来 Racket 只有一个类型（这样 static type check 都能成功）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Constructors are applied implicitly (&lt;strong&gt;values are tagged&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;42 is really like Int 42&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Primitives implicitly check tags and extract data, raising errors for wrong constructors&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;fun car v = case v of Pair(a,b) =&amp;gt; a | _ =&amp;gt; raise …

fun pair? v = case v of Pair _ =&amp;gt; true | _ =&amp;gt; false
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Weak Typing&lt;/h1&gt;

&lt;p&gt;There exist programs that, by definition, must pass static checking but then when run can &amp;quot;set the computer on fire&amp;quot;?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ease of language implementation: Checks left to the programmer&lt;/li&gt;
&lt;li&gt;Performance: Dynamic checks take time&lt;/li&gt;
&lt;li&gt;Lower level: Compiler does not insert information like array sizes, so it cannot do the checks&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Racket is not weakly typed&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It just checks most things dynamically*&lt;/li&gt;
&lt;li&gt;Dynamic checking is the definition – if the implementation can analyze the code to ensure some checks are not needed, then it can optimize them away&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 02 Apr 2018 19:10:35 +0800</pubDate>
        <link>http://masutangu.com/2018/04/programing-language-b/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/04/programing-language-b/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>The Part-Time Parliament 论文笔记</title>
        <description>&lt;h1&gt;背景&lt;/h1&gt;

&lt;p&gt;Paxos 岛兼职议会类似容错式分布式系统面对的问题：&lt;strong&gt;议员对应分布式系统中的进程，议员缺席对应进程挂掉。Paxos 设计的议会协议在议员经常缺席的情况下可以保证法令的一致性。&lt;/strong&gt;&lt;/p&gt;

&lt;h1&gt;The Single-Decree Synod&lt;/h1&gt;

&lt;p&gt;单一法令的神会协议的演化如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先由几个&lt;strong&gt;能保证一致性和允许进展性的约束&lt;/strong&gt;推导出&lt;strong&gt;初级协议（preliminary protocol)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;preliminary protocol&lt;/strong&gt;的约束版本得到&lt;strong&gt;基本协议（basic protocol）&lt;/strong&gt;，其满足一致性但不保证进展性&lt;/li&gt;
&lt;li&gt;进一步约束&lt;strong&gt;basic protocol&lt;/strong&gt;得到完整的神会协议，既满足一致性又保证进展性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下来四小节先讲解保证一致性的约束，再依次得出 preliminary protocol、basic protocol 和完整的神会协议。&lt;/p&gt;

&lt;h2&gt;一致性的约束条件&lt;/h2&gt;

&lt;p&gt;神会法令通过多轮带编号的&lt;strong&gt;表决（ballot）&lt;/strong&gt;选出。每一轮 ballot 是对单一法令的投票。每轮 ballot 牧师只能选择投票（表示赞成）或不投票（表示不赞成）。每轮 ballot 都关联一个&lt;strong&gt;法定人数集（quorum）&lt;/strong&gt;的牧师集合。只有当 quorum 中的每一位牧师都投票，这轮 ballot 才算成功。一轮 ballot B 包括以下四个元素：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;B(dec): 被投票的法令&lt;/li&gt;
&lt;li&gt;B(qrm): 该轮 ballot 的 quorum&lt;/li&gt;
&lt;li&gt;B(vot): 投票的牧师集合&lt;/li&gt;
&lt;li&gt;B(bal): ballot 的编号&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当且仅当 B(qrm) ⊆ B(vot) 时，这轮 ballot 才算成功。&lt;strong&gt;B(bal) 值的大小和进行 ballot 的顺序无关，B(bal) 比较大的 ballot 有可能在 B(bal) 比较小的 ballot 之前发生。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Paxos 数学家在由多轮 ballot 组成的集合 𝜷 上定义了三个约束条件，如果集合中的 ballot 都满足这些条件，则可以保证一致性会：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;B1(𝜷): 𝜷 中的每一轮 ballot 都拥有唯一的编号&lt;/li&gt;
&lt;li&gt;B2(𝜷): 𝜷 中任意两轮 ballot 的 quorum 至少有一个公共成员&lt;/li&gt;
&lt;li&gt;B3(𝜷): 𝜷 中任意一轮 ballot B，如果 B(qrm) 中任何一个牧师在 𝜷 中更早的 ballot 中投过票，则 B(dec) 与所有更早的 ballots 中最后那轮 ballot 的法令相同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图是对 B3(𝜷) 的图解。五轮 ballot 组成集合 𝜷，A、B、Γ、∆ 和 E 表示五位牧师，方框圈起的表示投票的牧师。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/the-part-time-parliament-note/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;编号 2 是最早的一轮 ballot，因此显而易见三个条件都满足（没有比他更早的 ballot了）&lt;/li&gt;
&lt;li&gt;编号 5 的 ballot，四位 quorum 都没有在更早的 ballot 中投过票，因此三个条件也满足&lt;/li&gt;
&lt;li&gt;编号 14 的 ballot 中，∆ 是 quorum 中唯一一位在更早的 ballot 中投过票的，因此这一轮的法令必须和编号为 2 的 ballot 一致，都为 𝜶&lt;/li&gt;
&lt;li&gt;编号 27 是一轮成功的 ballot，A、Γ 和 ∆ 为该轮的 quorum 成员。Γ 在编号 5 的 ballot 中投过票，∆ 在编号为 2 的 ballot 中投过票，根据 B3(𝜷)，这一轮的法令必须和编号 5 的法令一致（编号 5 比编号 2 更新）&lt;/li&gt;
&lt;li&gt;编号 29 的 quorum 成员为 B、Γ 和 ∆。B 在编号 14 的 ballot 投过票，Γ 在编号 5 和 27 的 ballots 都投过票，∆ 在编号为 2 和 27 的ballots 也投过票，这些 ballots 中最新的编号为 27，因此这轮 ballot 的法令必须和编号 27 的法令一致&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面给出 B1(𝜷) B2(𝜷) B3(𝜷) 的数学定义：&lt;/p&gt;

&lt;p&gt;符号 v 表示一次投票，其中 v(pst) 表示投票的牧师，v(bal) 表示投票的编号，v(dec) 表示投票的法令。集合 Votes(𝜷) 表示满足以下条件的投票 v 的集合：v(pst) ∈ B(vot)，v(bal) = B(bal)，v(dec) = B(dec)，B ∈ 𝜷。定义 p 为牧师，b 为 ballot 编号，则 MaxVote(b, p, 𝜷) 表示集合 &lt;strong&gt;{ v ∈ Votes(B): (v(pst) = p) ∧ (v(bal) &amp;lt; b) } ∪ { null(p) }&lt;/strong&gt; 中 ballot 最大的投票。&lt;em&gt;注：null(p) 表示 null 投票，即 v(bal) 为-∞, v(dec) 为 BLANK 且 v(pst) = p&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;对任意非空的牧师集合 Q，MaxVote(b, Q, 𝜷) 定义为 Q 中的所有牧师 p 的 MaxVote(b, p, 𝜷) 的最大值。&lt;em&gt;注：翻译得不太妥当，原文：For any nonempty set Q of priests, MaxVote(b, Q, B) was defined to equal the maximum of all votes MaxVote(b, p, B) with p in Q.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;B1(𝜷) B2(𝜷) B3(𝜷) 的数学定义如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;B1(𝜷) ≜ ∀B, B&amp;#39; ∈ 𝜷 : (B ≠ B&amp;#39;) ⇒ (B(bal) ≠ B&amp;#39;(bal))&lt;/li&gt;
&lt;li&gt;B2(𝜷) ≜ ∀B, B&amp;#39; ∈ 𝜷 : B(qrm) ∩ B&amp;#39;(qrm) ≠ ∅&lt;/li&gt;
&lt;li&gt;B3(𝜷) ≜ ∀B ∈ 𝜷 : (MaxVote(B(bal) , B(qrm), 𝜷)bal ≠ −∞) ⇒ (B(dec) = MaxVote(B(bal) , B(qrm) , 𝜷)dec)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;引理：如果满足 B1(𝜷) B2(𝜷) B3(𝜷)，则 ((B(qrm) ⊆ B(vot)) ∧ (B&amp;#39;(bal) &amp;gt; B(bal))) ⇒ (B(dec) = B&amp;#39;(dec))&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;引理证明如下：&lt;/p&gt;

&lt;p&gt;对 𝜷 中任意 ballot B，定义 Ψ(B, 𝜷) 表示 𝜷 中所有编号大于 B(bal) 且法令不等于 B(dec) 的 ballot 的集合：&lt;strong&gt;Ψ(B, 𝜷) ≜ {B&amp;#39; ∈ 𝜷 : (B&amp;#39;(bal) &amp;gt; B(bal) ∧ (B&amp;#39;(dec) ≠ B(dec))}&lt;/strong&gt;。如果证明 B(qrm) ⊆ B(vot) 则 Ψ(B, 𝜷) 为空，引理即成立。下面是反证法，假设存在 B 满足 B(qrm) ⊆ B(vot) 且 Ψ(B, 𝜷) 不为空，下面推导得出矛盾的结论：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择 C ∈ Ψ(B, B) 满足 C(bal) = min{ B&amp;#39;(bal) : B&amp;#39; ∈ Ψ(B, 𝜷)}&lt;/li&gt;
&lt;li&gt;C(bal) &amp;gt; B(bal)：由 1 和 Ψ(B, 𝜷) 的定义得出&lt;/li&gt;
&lt;li&gt;B(vot) ∩ C(qrm) ≠ ∅：由 B2(𝜷) 和 B(qrm) ⊆ B(vot) 得出&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷)bal ≥ B(bal)：由 2、3 和 MaxVote 的定义得出&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷) ∈ Votes(B)：由 4 知道 MaxVote(C(bal), C(qrm) , 𝜷) 不是 null 投票&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷)dec = C(dec)：由 5 和 B3(𝜷) 得出&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷)dec ≠ B(dec)：由 6、1 和 Ψ(B, 𝜷) 的定义得出&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷)bal &amp;gt; B(bal)：由 4 7 和 B1(𝜷) 的出&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷) ∈ Votes(Ψ(B, 𝜷))：由 7 8 和 Ψ(B, 𝜷) 的定义得出&lt;/li&gt;
&lt;li&gt;MaxVote(C(bal), C(qrm), 𝜷)bal &amp;lt; C(bal)：由 MaxVote(C(bal), C(qrm), 𝜷) 的定义得出&lt;/li&gt;
&lt;li&gt;由 9 10 1 得出矛盾，因为 1 中定义了 C(bal) 是 Ψ(B, 𝜷) 中编号最小的，由 9 知道 MaxVote(C(bal), C(qrm), 𝜷) 属于 Votes(Ψ(B, 𝜷))，那么 MaxVote(C(bal), C(qrm), 𝜷) 必须大于等于 C(bal)，这与 10 得到的 MaxVote(C(bal), C(qrm), 𝜷)bal &amp;lt; C(bal) 矛盾&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由引理得出，如果满足 B1(𝜷) B2(𝜷) B3(𝜷)，则任意两轮成功的 ballots 都是相同的法令，数学表示：&lt;strong&gt;((B(qrm) ⊆ B(vot)) ∧ (B&amp;#39;(qrm) ⊆ B&amp;#39;(vot))) ⇒ (B&amp;#39;dec = Bdec)&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;The Preliminary Protocol&lt;/h2&gt;

&lt;p&gt;为了遵守 B3(𝜷)，牧师在发起表决前需要先确定 MaxVote(b, Q, 𝜷)dec，因此需要确定 Q 中的每一个 p 的 MaxVote(b, q, 𝜷)dec。Preliminary protocol 的前两步如下：&lt;/p&gt;

&lt;p&gt;(1) 牧师 p 选择新的 ballot number b，发送一条 NextBallot(b) 消息给某些牧师&lt;/p&gt;

&lt;p&gt;(2) 牧师 q 收到 NextBallot(b) 消息后，发送 LastVote(b, v) 消息给 p，v 为 MaxVote(b, q, 𝜷)&lt;/p&gt;

&lt;p&gt;发送完 LastVote(b, v)，&lt;strong&gt;q 承诺不再给编号在 [v(bal), b] 区间的表决投票。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：这个承诺是为了解决乱序的问题， ballot 编号大的投票发生在编号小的之前时，如下图，B 和 C 投票的法令出现了不一致［参考自&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21438357&quot;&gt;Paxos理论介绍(1): 朴素Paxos算法理论推导与证明&lt;/a&gt;］：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/the-part-time-parliament-note/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;接下来两个步骤是：&lt;/p&gt;

&lt;p&gt;(3) 在收到 majority 的 LastVote(b, v) 之后，p 发起一轮编号为 b，quorum 为 Q 且法令为 d（d 需要满足B3(𝜷)）的表决，给 Q 中每一个牧师发送 BeginBallot(b, d) 消息&lt;/p&gt;

&lt;p&gt;(4) 在收到 BeginBallot(b, d) 消息后，q 决定是否投票，如果可以投票（不违背上面的承诺），则发送 Voted(b, q) 消息给 p&lt;/p&gt;

&lt;p&gt;步骤 3 将表决 B 加入到集合 𝜷 中，其中 B(bal) = b，B(qrm) = Q，B(vot) = ∅。步骤 4 中，如果牧师 q 投票了，则将 q 加入 B(vot) 中。&lt;/p&gt;

&lt;p&gt;协议的剩余步骤如下：&lt;/p&gt;

&lt;p&gt;(5) 如果 p 收到了 Q 中每一个 q 的 Voted(b, q) 消息，那么 p 在律簿上记录下法令 d，并且发送 Success(d) 给每一个牧师&lt;/p&gt;

&lt;p&gt;(6) 牧师 q 收到 Success(d) 消息后，将法令 d 记录在自己的律簿上&lt;/p&gt;

&lt;h2&gt;The Basic Protocol&lt;/h2&gt;

&lt;p&gt;在 preliminary protocol 中，每一个牧师都必须记录 &lt;strong&gt;(i) 他发起的每一个ballot number (ii) 他投过的每一次票 (iii) 他发送过的每一个 LastVote 消息&lt;/strong&gt;。要记录这么多信息是非常困难的，因此 Paxos 人对 preliminary protocol 做了进一步约束，得到更实用的 basic protocol，每个牧师只需要在律簿上记录下面三个信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;lastTried[p]&lt;/strong&gt;：p 发起的最后一个表决的编号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;prevVote[p]&lt;/strong&gt;：p 投过的所有表决中编号最大的那次投票&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nextBal[p]&lt;/strong&gt;：p 发过的所有 LastVote(b, v) 消息中 b 的最大值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Preliminary protocol 允许牧师并行管理任意数量的表决，basic protocol 中牧师在一个时间内只管理一个表决，编号为 lastTried[p]，忽略和之前发起的表决相关的消息。Basic protocol 中对 LastVote 做了更强的承诺：&lt;strong&gt;不再对编号小于 b 的表决进行投票。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Basic protocol 步骤如下：&lt;/p&gt;

&lt;p&gt;(1) 牧师 p 选择新的 ballot number b，必须大于 lastTried[p]，并更新 lastTried[p] 置为 b，然后发送 NextBallot(b) 消息给某些牧师&lt;/p&gt;

&lt;p&gt;(2) 牧师 q 收到 NextBallot(b) 消息，如果 b 大于 nextBal(q)，则更新 nextBal(q) 置为 b 并发送 LastVote(b, v) 消息给 p，v 即 prevVote[q]。如果 b 小于等于 nextBal(q) 则忽略该 NextBallot 消息。&lt;/p&gt;

&lt;p&gt;(3) 在收到 majority 的 LastVote(b, v) 消息后，如果 b 等于 lastTried[p]，则 p 发起一轮新的表决，指定编号为 b，quorum 为 Q，法令为 d，d 需要满足 B3(𝜷)。p 发送 BeginBallot(b, d) 消息给 Q 中所有牧师。&lt;/p&gt;

&lt;p&gt;(4) 在收到 BeginBallot(b, d) 消息后，如果 b 等于 nextBal[q]，牧师 q 投票，并将 prevVote[q] 设置为本次投票，然后发送 Voted(b, q) 消息给 p。如果 b 不等于 nextBal[q]，则忽略该 BeginBallot 消息&lt;/p&gt;

&lt;p&gt;(5) 如果 p 收到 Q 中所有牧师的 Voted(b, q) 消息，并且 b 等于 lastTried[p]，他在律簿上记录下法令 d，并发送 Success(d) 消息给所有的牧师。&lt;/p&gt;

&lt;p&gt;(6) 在收到 Success(d) 消息后，牧师在律簿上记录下法令 d&lt;/p&gt;

&lt;p&gt;Basic protocol 是 preliminary protocol 的约束版本，preliminary protocol 满足一致性的条件，那么 basic protocol 也一定满足。不过和 preliminary protocol 一样，basic protocol 也没要求必须执行某个操作，因此同样没有解决进展性的问题。&lt;/p&gt;

&lt;h2&gt;The Complete Synod Protocol&lt;/h2&gt;

&lt;p&gt;为了保证进展性，&lt;strong&gt;关键在于决定牧师什么时候应该发起表决&lt;/strong&gt;。永远不发起表决和太频繁的发起表决都会影响进展性。完整的神会协议在 basic protocol 的基础上新增一个流程来选择唯一的牧师--总统，来发起表决。这部分细节参照论文的 2.4 节。&lt;/p&gt;

&lt;h1&gt;The Multi-Decree Parliament&lt;/h1&gt;

&lt;h2&gt;The Protocol&lt;/h2&gt;

&lt;p&gt;Paxos 议会需要通过一系列法令而不仅仅单一一个法令。法令提议给总统，由总统赋予 ballot number 并尝试通过它。协议对每个法令使用不同实例的 Complete Synod Protocol，但这系列实例只需要一位总统来负责，并且神会协议的前两步只需要执行一次。&lt;/p&gt;

&lt;p&gt;在神会协议中，总统在第三步之前不会选择法令和 quorum，因此总统可以为所有实例发送一条 NextBallot(b) 消息，议员回复一条 LastVote 消息，规则和单一法令的协议相同，只是把所有待表决的实例信息包含在一条 LastVote 信息里。&lt;/p&gt;

&lt;p&gt;当总统收到 majority 的回复后，就为每个待表决的实例执行 Complete Synod Protocol 的第三步。&lt;/p&gt;

&lt;p&gt;因此 Paxon 的议会协议如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;已经知道结果的表决不再需要再走一遍 Complete Synod Protocol 的流程，因此当新选举的总统 p 在律簿上已经记录有编号小于等于 n 的法令，那他将发送 NextBallot(b, n) 代替之前的 NextBallot(b) 消息。议员收到 NextBallot 消息后，将他律簿上所有编号大于 n 的法令返回给 p，并且告诉 p 他缺失的编号小于等于 n 的法令。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Properties of the Protocol&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;在总统提议任何法令之前，必须先向 majority 学习他们已经投过票的法令。&lt;/strong&gt;任何一个已经通过的法令一定被至少一个多数集合中的议员投过票。这样就保证了总统在提议新法令前律簿上不会有空缺。&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Mar 2018 20:36:16 +0800</pubDate>
        <link>http://masutangu.com/2018/03/the-part-time-parliament-note/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/03/the-part-time-parliament-note/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>多排行榜数据刷新方案</title>
        <description>&lt;h2&gt;一. 背景&lt;/h2&gt;

&lt;p&gt;最近工作遇到一个棘手的问题：&lt;strong&gt;多个不同的排行榜的玩家信息如何保持一致&lt;/strong&gt;。简单描述下场景，以小游戏跳一跳为例子，一开始游戏只有一个好友排行榜，好友排行榜以玩家的最高分数进行排序，这样好处理，搭一个关系链svr，该 svr 上缓存玩家好友的信息（避免每次去 DB 查询），并使用玩家信息中的最高分数进行排序。客户端请求时下方相应的排名和玩家信息，包括最高分数信息（客户端需要展示）即可。但如果我们要新增一个全国排行榜，全国排行榜以玩家的最高步数＋耗时进行排行。这时需要搭一个全国排行榜svr，该svr上同样缓存进入全国排行榜的玩家的信息，使用玩家信息中的最高分数＋耗时进行排序。同样的在客户端请求时下发相应的排名和玩家信息。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;方案一：各个排行榜有自己的玩家缓存：&lt;/em&gt;
&lt;img src=&quot;/assets/images/multi-rank-refresh-design/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;二. 问题&lt;/h2&gt;

&lt;p&gt;但问题来了，&lt;strong&gt;如何保证出现在两个榜单的同一玩家数据是一致的？&lt;/strong&gt;（不一致是因为两个排行 svr 分别缓存了玩家的信息，每个svr的排行数据缓存刷新周期也不一致）。&lt;/p&gt;

&lt;p&gt;这时候就需要使用一个全局的 data svr 来缓存玩家的信息，&lt;strong&gt;保证不同排行 svr 取到的玩家数据是一致的，同时两个排行svr刷新缓存的周期需要保持一致&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;方案二：每个排行榜都从data svr 拉取玩家数据：&lt;/em&gt;
&lt;img src=&quot;/assets/images/multi-rank-refresh-design/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;数据源都从 data svr 拉取这个很简单，关键在于&lt;strong&gt;如何让不同排行榜的刷新周期保持一致&lt;/strong&gt;。本文提出一个不成熟有待考验的方案解决这个问题，并给出简单的协议例子说明。&lt;/p&gt;

&lt;h2&gt;三. 方案&lt;/h2&gt;

&lt;h3&gt;1. 协议&lt;/h3&gt;

&lt;p&gt;CSProto 表示从客户端到服务器的协议，SSProto 表示服务器之间的协议。Common 为公用协议。定义排行榜相关协议如下：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;公用协议：&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;package Common;

message PlayerInfo {
    int32 uid              = 1;  // 玩家 uid
    int64 udpate_timestamp = 2;  // 玩家信息的更新时间
    // 玩家信息 包括例如最高分 耗时 具体字段略过略
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;客户端和服务器之间的协议：&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;import &amp;quot;common.proto&amp;quot;;

package CSProto;

message RankReq {
    int32 rank_type = 1; // 排行榜类型 例如好友排行榜 全国排行榜
}

message RankInfo {
    Common.PlayerInfo info = 1; // 玩家信息 客户端展示用
    int32 rank_idx         = 2; // 玩家排名
}

message RankRes {
    int32 rank_type             = 1; // 排行榜类型 例如好友排行榜 全国排行榜
    repeated RankInfo rank_list = 2; // 排行数据
    int64 update_timestamp      = 3; // 排行榜更新时间
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;服务器和服务器之间的协议：&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;package SSProto;

message GetPlayerInfoReq {
    repeated int32 uid_list = 1;
} 

message GetPlayerInfoRes {
    repeated Common.PlayerInfo info_list = 1;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;消息流如下：&lt;/em&gt;
&lt;img src=&quot;/assets/images/multi-rank-refresh-design/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h3&gt;2. 保证排行榜 svr 刷新周期一致&lt;/h3&gt;

&lt;p&gt;排行榜 svr 从 Data svr 中拉取玩家的信息进行排序，而 Data svr 会定期去更新玩家的信息，可以推导出：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;排行榜的刷新时间等于max(排行榜上榜的玩家数据的更新时间戳)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;因此回包给客户端的 RankRes 中的 update_timestamp 取值的伪代码如下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;rank_res.update_timestamp =  max([rankinfo.info.udpate_timestamp for rankinfo in rank_res.rank_list])
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;客户端使用 map 来管理每个排行榜的更新时间：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# rank_map
&amp;lt;ranktype1, udpate_timestamp1&amp;gt;
&amp;lt;ranktype2, update_timestamp2&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;并定义所有排行榜的最新更新时间 rank_max_update_timestamp，取值伪代码如下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;rank_max_update_timestamp = max([update_timestamp for _, update_timestamp in rank_map]) 
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;当玩家点击某个排行榜，客户端发现该排行榜的 update_timestamp 小于 rank_max_update_timestamp，就能判定该排行榜上存在过时的玩家数据，这时就应该向后台发起 RankReq 获取排行榜请求。&lt;strong&gt;通过及时请求过时排行榜数据，保证每个排行榜的 update_timestamp 一致，就能保证排行榜上玩家信息的一致，也就保证了在多个排行榜上玩家信息的一致。&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;3. 优化&lt;/h3&gt;

&lt;p&gt;上面提到，每次 update_timestamp 小于 rank_max_update_timestamp，客户端都会重新请求一次排行榜，后台会返回最新的排行榜数据，这里其实可以做下优化。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multi-rank-refresh-design/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;客户端先请求了 ranktype1 的排行榜，可以看出 ranktype1 的 update_timestamp 为 t1。之后又请求了 rank_type2 的排行榜，返回 ranktype2 的 update_timestamp 为 t2。由于 t1 &amp;lt; t2，客户端会发现需要更新 ranktype1。但从图上可以看出其实后台不需要再返回一次 ranktype1 的排行数据了（ranktype1 榜上的玩家 uid1 uid2 uid3 的数据并没有变化）。因此我们在 RankReq 里加上客户端本地该排行榜的 update_timestamp，如果后台发现客户端的 update_timestamp 和后台的是一致的，就返回特定的错误码告诉客户端排行榜依然有效。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;新增客户端本地该排行榜的 update_timestamp：&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;message RankReq {
    int32 rank_type               = 1; // 排行榜类型 例如好友排行榜 全国排行榜
    int64 client_update_timestamp = 2; // 客户端本地该排行榜的 update_timestamp
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;采用这种方式的话，每次点击该排行榜客户端还是需要发起一次（无效）的后台请求，如何做优化呢？这里用了一个简单的方案，如果客户端收到排行榜依然有效的错误码，就把本地该排行榜 update_timestamp 更新为 rank_max_update_timestamp：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multi-rank-refresh-design/illustration-5.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;每次客户端请求，rank svr 都需要到 data svr 查询玩家信息。可以在 rank svr 上缓存玩家的信息，到 data svr 查询时如果玩家数据无变化，则返回特定错误码，rank svr 继续使用本地的玩家信息缓存。&lt;/p&gt;

&lt;h2&gt;三. 总结&lt;/h2&gt;

&lt;p&gt;本文提出了一个有待考验的多排行榜数据刷新方案，为解决多个排行榜数据不一致的问题。该方案还有一些细节有待考量，欢迎大家有任何想法或有更好的方案邮件我一起讨论。&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Mar 2018 09:00:42 +0800</pubDate>
        <link>http://masutangu.com/2018/03/multi-rank-refresh-design/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/03/multi-rank-refresh-design/</guid>
        
        
        <category>工作</category>
        
      </item>
    
      <item>
        <title>Programming Language 课程笔记</title>
        <description>&lt;p&gt;本文是学习&lt;a href=&quot;https://www.coursera.org/learn/programming-languages/home/info&quot;&gt;Coursera Programming Language&lt;/a&gt;课程的学习笔记，文章内容及代码均取自课程材料。&lt;/p&gt;

&lt;h2&gt;一. 声明式编程和命令式编程&lt;/h2&gt;

&lt;h4&gt;声明式编程（Declarative Programming）&lt;/h4&gt;

&lt;p&gt;Building the structure and elements of computer programs, that &lt;strong&gt;expresses the logic of a computation without describing its control flow&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;命令式编程（Imperative Programming）&lt;/h4&gt;

&lt;p&gt;Describes computation &lt;strong&gt;in terms of statements that change a program state&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;左图为 Declarative Programming 范式，右图为 Imperative Programming 范式：&lt;/em&gt;
&lt;img src=&quot;/assets/images/programming-language/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;二. 函数式编程&lt;/h2&gt;

&lt;p&gt;三大特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No Mutation&lt;/li&gt;
&lt;li&gt;First Class Function&lt;/li&gt;
&lt;li&gt;Tail Recursion Optimization&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;1. No Mutation&lt;/h3&gt;

&lt;p&gt;知乎上&lt;a href=&quot;https://www.zhihu.com/question/28292740&quot;&gt;《什么是函数式编程思维》&lt;/a&gt;其中一个回到写得很好：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;函数式编程的本质：
函数式编程中的&lt;strong&gt;函数&lt;/strong&gt;这个术语不是指计算机中的函数，而是指数学中的函数，即自变量的映射。也就是说一个函数的值仅决定于函数参数的值，不依赖其他状态。在函数式语言中，&lt;strong&gt;函数作为一等公民&lt;/strong&gt;，可以在任何地方定义，在函数内或函数外，可以作为函数的参数和返回值，可以对函数进行组合。纯函数式编程语言中的&lt;strong&gt;变量&lt;/strong&gt;也不是命令式编程语言中的变量表示存储状态的单元，而是&lt;strong&gt;代数中的变量&lt;/strong&gt;，即一个值的名称。变量的值是&lt;strong&gt;不可变的（immutable）&lt;/strong&gt;，也就是说不允许像命令式编程语言中那样多次给一个变量赋值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;没有 Mutation 带来的好处:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No code can ever distinguish aliasing vs. identical copies&lt;/li&gt;
&lt;li&gt;No need to think about aliasing: focus on other things&lt;/li&gt;
&lt;li&gt;Can use aliasing, which saves space, without danger &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;2. How to build bigger types&lt;/h3&gt;

&lt;p&gt;3 most important type building-blocks in any language&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Each of&lt;/strong&gt;: A t value contains values of each of t1 t2 … tn&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One of&lt;/strong&gt;: A t value contains values of one of t1 t2 … tn&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self reference&lt;/strong&gt;: A t value can refer to other t values&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;3. Tail Recursion and Accumulators&lt;/h3&gt;

&lt;p&gt;ML recognizes these tail calls in the compiler and treats them differently: &lt;strong&gt;pop the caller before the call, allowing callee to reuse the same stack space&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;非尾递归的调用堆栈&lt;/em&gt;
&lt;img src=&quot;/assets/images/programming-language/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;尾递归优化后的调用堆栈&lt;/em&gt;
&lt;img src=&quot;/assets/images/programming-language/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h3&gt;4. Higher Order Function&lt;/h3&gt;

&lt;h4&gt;Map&lt;/h4&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;fun map (f,xs) =
 case xs of
 [] =&amp;gt; []
 | x::xs’ =&amp;gt; (f x)::(map(f,xs’))
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h4&gt;Filter&lt;/h4&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;fun filter (f,xs) =
 case xs of
 [] =&amp;gt; []
 | x::xs’ =&amp;gt; if f x
 then x::(filter(f,xs’))
 else filter(f,xs’)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h4&gt;Fold&lt;/h4&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;fun fold f acc xs =
 case xs of
 [] =&amp;gt; acc
 | x::xs’ =&amp;gt; fold f (f(acc,x)) xs’
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h4&gt;Higher Order Function vs For Loop&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Fold&lt;/strong&gt;  is another very famous iterator over recursive structures. This pattern separates recursive traversal from data processing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Can reuse same traversal for different data processing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Can reuse same data processing for different data structures&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In both cases, using common vocabulary concisely communicates intent&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;遍历函数和处理函数分离，提高复用性。&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;5. Function Closure&lt;/h3&gt;

&lt;p&gt;A function value has two parts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The code&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The environment&lt;/strong&gt; that was current when the function was defined&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This pair is called a &lt;strong&gt;function closure&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;6. Abstract Data Types With Closure&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;datatype set = S of { insert : int -&amp;gt; set, 
              member : int -&amp;gt; bool, 
              size   : unit -&amp;gt; int }

val empty_set =
    let
        fun make_set xs = (* xs is a &amp;quot;private field&amp;quot; in result *)
            let (* contains a &amp;quot;private method&amp;quot; in result *)
                fun contains i = List.exists (fn j =&amp;gt; i=j) xs
            in
                S { insert = fn i =&amp;gt; if contains i 
                                     then make_set xs 
                                     else make_set (i::xs),
                    member = contains,
                    size   = fn () =&amp;gt; length xs
                  }
            end
    in
        make_set []
    end 

fun use_sets () =
    let val S s1 = empty_set
        val S s2 = (#insert s1) 34
        val S s3 = (#insert s2) 34
        val S s4 = #insert s3 19
    in
        if (#member s4) 42
        then 99
        else if (#member s4) 19
        then 17 + (#size s3) ()
        else 0
    end 
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;使用 Java 实现：&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;interface Func&amp;lt;B,A&amp;gt; {
    B m(A x);
}
interface Pred&amp;lt;A&amp;gt; {
    boolean m(A x);
}

class List&amp;lt;T&amp;gt; {
    T       head;
    List&amp;lt;T&amp;gt; tail;
    List(T x, List&amp;lt;T&amp;gt; xs) {
    head = x;
    tail = xs;
    }

    // * the advantage of a static method is it allows xs to be null
    //    -- a more OO way would be a subclass for empty lists
    // * a more efficient way in Java would be a messy while loop
    //   where you keep a pointer to the previous element and mutate it
    //   -- (try it if you do not believe it is messy)
    static &amp;lt;A,B&amp;gt; List&amp;lt;B&amp;gt; map(Func&amp;lt;B,A&amp;gt; f, List&amp;lt;A&amp;gt; xs) {
        if(xs==null)
            return null;
        return new List&amp;lt;B&amp;gt;(f.m(xs.head), map(f,xs.tail));
    }

    static &amp;lt;A&amp;gt; List&amp;lt;A&amp;gt; filter(Pred&amp;lt;A&amp;gt; f, List&amp;lt;A&amp;gt; xs) {
        if(xs==null)
            return null;
        if(f.m(xs.head))
            return new List&amp;lt;A&amp;gt;(xs.head, filter(f,xs.tail));
        return filter(f,xs.tail);
    }

    // * again recursion would be more elegant but less efficient
    // * again an instance method would be more common, but then
    //   all clients have to special-case null 
    static &amp;lt;A&amp;gt; int length(List&amp;lt;A&amp;gt; xs) {
        int ans = 0;
        while(xs != null) {
            ++ans;
            xs = xs.tail;
        }
        return ans;
    }
}

class ExampleClients {
    static List&amp;lt;Integer&amp;gt; doubleAll(List&amp;lt;Integer&amp;gt; xs) {
        return List.map((new Func&amp;lt;Integer,Integer&amp;gt;() { 
                     public Integer m(Integer x) { return x * 2; } 
                         }), xs);
    }
    static int countNs(List&amp;lt;Integer&amp;gt; xs, final int n) {
        return List.length(List.filter(
           (new Pred&amp;lt;Integer&amp;gt;() { 
               public boolean m(Integer x) { return x==n; } 
           }), xs));
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;使用 C 实现:&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;typedef struct List list_t;
struct List {
  void * head;
  list_t * tail;
};

list_t * makelist (void * x, list_t * xs) {
  list_t * ans = (list_t *)malloc(sizeof(list_t));
  ans-&amp;gt;head = x;
  ans-&amp;gt;tail = xs;
  return ans;
}

list_t * map(void* (*f)(void*,void*), void* env, list_t * xs) {
  if(xs==NULL)
    return NULL;
  return makelist(f(env,xs-&amp;gt;head), map(f,env,xs-&amp;gt;tail));
}

list_t * filter(bool (*f)(void*,void*), void* env, list_t * xs) {
  if(xs==NULL)
    return NULL;
  if(f(env,xs-&amp;gt;head))
    return makelist(xs-&amp;gt;head, filter(f,env,xs-&amp;gt;tail));
  return filter(f,env,xs-&amp;gt;tail);
}

int length(list_t* xs) {
  int ans = 0;
  while(xs != NULL) {
    ++ans;
    xs = xs-&amp;gt;tail;
  }
  return ans;
}

// clients of our list implementation:
// [the clients that cast from void* to intptr_t are technically not legal C, 
//  as explained in detail below if curious]

// awful type casts to match what map expects
void* doubleInt(void* ignore, void* i) {
  return (void*)(((intptr_t)i)*2);
}

// assumes list holds intptr_t fields
list_t * doubleAll(list_t * xs) {
  return map(doubleInt, NULL, xs);
}

// awful type casts to match what filter expects
bool isN(void* n, void* i) {
  return ((intptr_t)n)==((intptr_t)i);
}

// assumes list hold intptr_t fields
int countNs(list_t * xs, intptr_t n) {
  return length(filter(isN, (void*)n, xs));
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Fri, 02 Mar 2018 19:45:24 +0800</pubDate>
        <link>http://masutangu.com/2018/03/programing-language/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/03/programing-language/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>基于 Replicated State Machine 实现游戏进程恢复</title>
        <description>&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;游戏服务器实现的业务逻辑普遍比较复杂，且大部分是带有状态的。如果进程重启或意外崩溃，会导致该服务器上的玩家断线，丢失进行中的游戏数据，带来极差的游戏体验。为了避免这种情况出现，一般游戏服务器都会持久化玩家数据以实现进程恢复，当重启或进程意外崩溃时，重新拉起进程后可以恢复到之前的状态。&lt;strong&gt;常用的做法是将玩家的状态信息保存在共享内存中，重启时加载共享内存进行恢复。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;共享内存虽然方便，但会有许多限制。比如 C++ 涉及到&lt;strong&gt;多态（虚函数表）、STL容器（heap分配）&lt;/strong&gt;，都不能直接映射到共享内存中：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/game-recover-based-on-replicated-state-machine/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;这篇文章提供了新的思路，提出一个实现游戏进程恢复更简洁的做法。&lt;/p&gt;

&lt;h2&gt;Replicated State Machine&lt;/h2&gt;

&lt;p&gt;在分布式系统中，&lt;strong&gt;replicated State Machine 是实现 fault tolerance 的一个重要方式&lt;/strong&gt;，通常由复制日志来实现。每一台服务器保存一份日志，日志中包含一系列的命令，状态机会按顺序执行这些命令。因为每一台计算机的状态机都是确定的（deterministic state machine），执行的命令相同，最后输出的结果也相同。&lt;/p&gt;

&lt;h2&gt;Combine&lt;/h2&gt;

&lt;p&gt;如果我们能&lt;strong&gt;以确定状态机（deterministic state machine）来实现游戏逻辑&lt;/strong&gt;，就可以运用 replicated State Machine 的思想。只要我们把所有触发状态机状态变更的 event 都保存下来，重启时直接重放一遍，就可以回到重启前的状态了。&lt;/p&gt;

&lt;p&gt;假设实现一个回合制 pvp 的对战游戏逻辑。RoomSvr 上有若干个房间。我们&lt;strong&gt;以 state machine 来实现业务逻辑，每个房间通过 state machine 维护自己的状态信息，将玩家的请求和定时器超时都转化为 event，通过 event distributer 分发给相应房间处理，并且将 event 通过 logging module 序列化保存到本地&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/game-recover-based-on-replicated-state-machine/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;进程重启时直接加载日志，读取 event 丢给房间的 state machine 进行重放，就能将每个房间恢复到进程重启/挂掉之前的状态了。&lt;/p&gt;

&lt;h2&gt;How to Snapshot&lt;/h2&gt;

&lt;p&gt;日积月累，序列化的日志会越来越多，如何能清理到不再需要的日志，提高重启时加载的速度呢？由于游戏逻辑不是简单的 kv 存储，无法直接做 snapshot，也无法参考 leveldb LSM-Tree 的做法，需要换一种方式来减少日志的堆积。&lt;/p&gt;

&lt;p&gt;当这一局已经结束时，这局的相关 event 就可以全部删掉了。如果将 event 序列化到日志中，要删除会比较麻烦。所以考虑利用共享内存来实现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/game-recover-based-on-replicated-state-machine/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;实现 ShmStore 来管理共享内存的存储，替换掉上图的 Logging Module。&lt;code&gt;ShmArr&lt;/code&gt; 为映射到共享内存的 &lt;code&gt;RoomInfo&lt;/code&gt;数组，&lt;code&gt;RoomInfo&lt;/code&gt; 是 C struct 结构，记录了房间id、房间的状态（是否有效）和房间的所有 event。  &lt;code&gt;room_idx_map_&lt;/code&gt; 维护着房间 id 到 &lt;code&gt;ShmArr&lt;/code&gt; 下标的关系，&lt;code&gt;free_idx_&lt;/code&gt; 保存着 &lt;code&gt;ShmArr&lt;/code&gt; 中空闲的下标。当创建新房间时，从 &lt;code&gt;free_idx_&lt;/code&gt; 中取一个空闲下标，并把房间 id 到该下标的映射关系保存于 &lt;code&gt;room_idx_map_&lt;/code&gt; 中，将 &lt;code&gt;RoomInfo&lt;/code&gt; 的 status 置为有效。之后该房间的所有 event 就保存在对应的 &lt;code&gt;RoomInfo&lt;/code&gt; 结构里。当房间销毁时，将对应的 &lt;code&gt;RoomInfo&lt;/code&gt; 结构清空，同时从&lt;code&gt;room_idx_map_&lt;/code&gt;删除对应的映射关系，并把该 &lt;code&gt;RoomInfo&lt;/code&gt; 的下标添加回 &lt;code&gt;free_idx_&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;当读取共享内存重建房间状态时，只加载 status 为有效的 &lt;code&gt;RoomInfo&lt;/code&gt; 结构。&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;最终模块图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/game-recover-based-on-replicated-state-machine/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 13 Jan 2018 11:30:24 +0800</pubDate>
        <link>http://masutangu.com/2018/01/game-recover-based-on-replicated-state-machine/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/01/game-recover-based-on-replicated-state-machine/</guid>
        
        
        <category>工作</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（五）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;FaRM Note&lt;/h1&gt;

&lt;p&gt;FaRM writes go to RAM, not disk -- eliminates a huge bottleneck. Can write RAM in 200 ns, but takes 10 ms to write hard drive, 100 us for SSD, but RAM loses content in power failure! Not persistent by itself.  &lt;/p&gt;

&lt;p&gt;Just write to RAM of f+1 machines, to tolerate f failures? Might be enough if failures were always independent, but power failure is not independent -- may strike 100% of machines!&lt;/p&gt;

&lt;p&gt;So batteries in every rack, can run machines for a few minutes: &lt;strong&gt;&amp;quot;non-volatile RAM&amp;quot;&lt;/strong&gt;. What if crash prevents s/w from writing SSD, e.g bug in FaRM or kernel, or cpu/memory/hardware error. FaRM copes with single-machine crashes by copying data from RAM of machines&amp;#39; replicas to other machines to ensure always f+1 copies. Crashes (other than power failure) must be independent!&lt;/p&gt;

&lt;p&gt;why is the network often a performance bottleneck?&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;    the usual setup:
    app                       app
    ---                       ---
    socket buffers            buffers
    TCP                       TCP
    NIC driver                driver
    NIC  -------------------- NIC
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;lots of expensive CPU operations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;system calls&lt;/li&gt;
&lt;li&gt;copy messages&lt;/li&gt;
&lt;li&gt;interrupts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and all twice if RPC! It&amp;#39;s hard to build RPC than can deliver more than a few 100,000 / second wire b/w (e.g. 10 gigabits/second) is rarely the limit for short RPC. These per-packet CPU costs are the limiting factor for small messages.&lt;/p&gt;

&lt;p&gt;Two classes of concurrency control for transactions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pessimistic:
wait for lock on first use of object; hold until commit/abort, called two-phase locking. Conflicts cause delays&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;optimistic:
access object without locking; commit &amp;quot;validates&amp;quot; to see if OK. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Valid: do the writes&lt;/li&gt;
&lt;li&gt;Invalid: abort
called &lt;strong&gt;Optimistic Concurrency Control (OCC)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;FaRM uses OCC. The reason: OCC lets FaRM read using one-sided RDMA reads, server storing the object does not need to set a lock, due to OCC.&lt;/p&gt;

&lt;p&gt;FaRM transaction API (simplified):&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  txCreate()
  o = txRead(oid)  -- RDMA
  o.f += 1
  txWrite(oid, o)  -- purely local
  ok = txCommit()  -- Figure 4
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;What&amp;#39;s in an oid: &lt;region #, address&gt;. &lt;code&gt;region #&lt;/code&gt; indexes a mapping to [ primary, backup1, ... ]. Target NIC can use address directly to read or write RAM so target CPU doesn&amp;#39;t have to be involved.&lt;/p&gt;

&lt;p&gt;Server memory layout: regions, each an array of objects
Object layout: header with version # and lock&lt;/p&gt;

&lt;p&gt;Every region replicated on one primary, f backups -- f+1 replicas. Only the primary serves reads; all f+1 see commits+writes replication yields availability if &amp;lt;= f failures, i.e. available as long as one replica stays alive; better than Raft!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;txRead
one-sided RDMA to fetch object direct from primary&amp;#39;s memory -- fast!
also fetches object&amp;#39;s version number, to detect concurrent writes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;txWrite
must be preceded by txRead
just writes local copy; no communication&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transaction execution / commit protocol without failure -- Figure 4&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;LOCK (first message in commit protocol)&lt;/p&gt;

&lt;p&gt;TC sends to primary of each written object
TC uses RDMA to append to its log at each primary
LOCK record contains oid, version # xaction read, new value
primary s/w polls log, sees LOCK, validates, sends &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; reply message
note LOCK is both logged in primary&amp;#39;s NVRAM &lt;em&gt;and&lt;/em&gt; an RPC exchange&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;what does primary CPU do on receipt of LOCK?&lt;/p&gt;

&lt;p&gt;(for each object)
if object locked, or version != what xaction read, reply &amp;quot;no&amp;quot;
otherwise set the lock flag and return &amp;quot;yes&amp;quot;
note: does &lt;em&gt;not&lt;/em&gt; block if object is already locked&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TC waits for all LOCK reply messages&lt;/p&gt;

&lt;p&gt;if any &amp;quot;no&amp;quot;, abort
send ABORT to primaries so they can release locks
returns &amp;quot;no&amp;quot; from txCommit()&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TC sends COMMIT-PRIMARY to primary of each written object&lt;/p&gt;

&lt;p&gt;uses RDMA to append to primary&amp;#39;s log
TC only waits for hardware ack -- does not wait for primary to process log entry
TC returns &amp;quot;yes&amp;quot; from txCommit()&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;what does primary do when it processes the COMMIT-PRIMARY in its log?&lt;/p&gt;

&lt;p&gt;copy new value over object&amp;#39;s memory
increment object&amp;#39;s version #
clear object&amp;#39;s lock flag&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Paxos Made Simple&lt;/h1&gt;

&lt;h2&gt;The Consensus Algorithm&lt;/h2&gt;

&lt;p&gt;Assume a collection of processes that can propose values. &lt;/p&gt;

&lt;p&gt;The safety
requirements for consensus are:
• Only a value that has been proposed may be chosen,
• Only a single value is chosen, and
• A process never learns that a value has been chosen unless it actually has been&lt;/p&gt;

&lt;p&gt;We let the three roles in the consensus algorithm be performed by three classes of agents: proposers, acceptors, and learners.&lt;/p&gt;

&lt;p&gt;A proposer sends a proposed value to a
set of acceptors. An acceptor may accept the proposed value. The value is
chosen when a large enough set of acceptors have accepted it. Because any two majorities
have at least one acceptor in common, this works if an acceptor can accept
at most one value. &lt;/p&gt;

&lt;p&gt;In the absence of failure or message loss, we want a value to be chosen
even if only one value is proposed by a single proposer. This suggests the
requirement:
P1. An acceptor must accept the first proposal that it receives.&lt;/p&gt;

&lt;p&gt;P1 and the requirement that a value is chosen only when it is accepted
by a majority of acceptors imply that an acceptor must be allowed to accept
more than one proposal. We keep track of the different proposals that an
acceptor may accept by assigning a (natural) number to each proposal, so a
proposal consists of a proposal number and a value. To prevent confusion,
we require that different proposals have different numbers.  A
value is chosen when a single proposal with that value has been accepted by
a majority of the acceptors. In that case, we say that the proposal (as well
as its value) has been chosen.&lt;/p&gt;

&lt;p&gt;We can allow multiple proposals to be chosen, but we must guarantee
that all chosen proposals have the same value. By induction on the proposal
number, it suffices to guarantee:
P2. If a proposal with value v is chosen, then every higher-numbered proposal
that is chosen has value v.&lt;/p&gt;

&lt;p&gt;To be chosen, a proposal must be accepted by at least one acceptor. So,
we can satisfy P2 by satisfying:
P2a
. If a proposal with value v is chosen, then every higher-numbered proposal
accepted by any acceptor has value v.&lt;/p&gt;

&lt;p&gt;Because communication
is asynchronous, a proposal could be chosen with some particular
acceptor c never having received any proposal. Suppose a new proposer
“wakes up” and issues a higher-numbered proposal with a different value.
P1 requires c to accept this proposal, violating P2a
. Maintaining both P1
and P2a
requires strengthening P2a
to:
P2b
. If a proposal with value v is chosen, then every higher-numbered proposal
issued by any proposer has value v.&lt;/p&gt;

&lt;p&gt;Since any set S consisting of a majority of acceptors contains at least one
member of C , we can conclude that a proposal numbered n has value v by
ensuring that the following invariant is maintained:
P2c
. For any v and n, if a proposal with value v and number n is issued,
then there is a set S consisting of a majority of acceptors such that
either (a) no acceptor in S has accepted any proposal numbered less
than n, or (b) v is the value of the highest-numbered proposal among
all proposals numbered less than n accepted by the acceptors in S.&lt;/p&gt;

&lt;p&gt;We can therefore satisfy P2b by maintaining the invariance of P2c
.&lt;/p&gt;

&lt;p&gt;To maintain the invariance of P2c
, a proposer that wants to issue a proposal
numbered n must learn the highest-numbered proposal with number
less than n, if any, that has been or will be accepted by each acceptor in
some majority of acceptors.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A proposer chooses a new proposal number n and sends a request to
each member of some set of acceptors, asking it to respond with:
(a) A promise never again to accept a proposal numbered less than
n, and
(b) The proposal with the highest number less than n that it has
accepted, if any.
I will call such a request a prepare request with number n.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the proposer receives the requested responses from a majority of
the acceptors, then it can issue a proposal with number n and value
v, where v is the value of the highest-numbered proposal among the
responses, or is any value selected by the proposer if the responders
reported no proposals.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A proposer issues a proposal by sending, to some set of acceptors, a request
that the proposal be accepted. (This need not be the same set of acceptors
that responded to the initial requests.) Let’s call this an accept request&lt;/p&gt;

&lt;p&gt;P1a
. An acceptor can accept a proposal numbered n iff it has not responded
to a prepare request having a number greater than n.&lt;/p&gt;

&lt;p&gt;The final
algorithm is obtained by making one small optimization.
Suppose an acceptor receives a prepare request numbered n, but it has
already responded to a prepare request numbered greater than n, thereby
promising not to accept any new proposal numbered n. There is then no
reason for the acceptor to respond to the new prepare request, since it will
not accept the proposal numbered n that the proposer wants to issue. We also have it ignore
a prepare request for a proposal it has already accepted.&lt;/p&gt;

&lt;p&gt;Phase 1. (a) A proposer selects a proposal number n and sends a prepare
request with number n to a majority of acceptors.
(b) If an acceptor receives a prepare request with number n greater
than that of any prepare request to which it has already responded,
then it responds to the request with a promise not to accept any more
proposals numbered less than n and with the highest-numbered proposal
(if any) that it has accepted.&lt;/p&gt;

&lt;p&gt;Phase 2. (a) If the proposer receives a response to its prepare requests
(numbered n) from a majority of acceptors, then it sends an accept
request to each of those acceptors for a proposal numbered n with a
value v, where v is the value of the highest-numbered proposal among
the responses, or is any value if the responses reported no proposals.
(b) If an acceptor receives an accept request for a proposal numbered
n, it accepts the proposal unless it has already responded to a prepare
request having a number greater than n.&lt;/p&gt;

&lt;p&gt;It’s easy to construct a scenario in which two proposers each keep issuing
a sequence of proposals with increasing numbers, none of which are ever
chosen. Proposer p completes phase 1 for a proposal number n1. Another
proposer q then completes phase 1 for a proposal number n2 &amp;gt; n1. Proposer
p’s phase 2 accept requests for a proposal numbered n1 are ignored because
the acceptors have all promised not to accept any new proposal numbered
less than n2.&lt;/p&gt;

&lt;p&gt;To guarantee progress, a distinguished proposer must be selected as the
only one to try issuing proposals.&lt;/p&gt;

&lt;p&gt;The Paxos algorithm [5] assumes a network of processes. In its consensus
algorithm, each process plays the role of proposer, acceptor, and learner.
The algorithm chooses a leader, which plays the roles of the distinguished proposer and the distinguished learner. &lt;/p&gt;

&lt;p&gt;All that remains is to describe the mechanism for guaranteeing that no
two proposals are ever issued with the same number. Different proposers
choose their numbers from disjoint sets of numbers, so two different proposers
never issue a proposal with the same number. Each proposer remembers
(in stable storage) the highest-numbered proposal it has tried to issue,
and begins phase 1 with a higher proposal number than any it has already
used.&lt;/p&gt;

&lt;h1&gt;MDCC: Multi-Data Center Consistency&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;MDCC (Multi-Data Center Consistency) is an
optimistic commit protocol for geo-replicated transactions,
that does not require a master or static partitioning, and is
strongly consistent at a cost similar to eventually consistent
protocols. &lt;/p&gt;

&lt;p&gt;Replication across geographically diverse data centers
(called geo-replication) is qualitatively different from replication
within a cluster, data center or region, because interdata
center network delays are in the hundreds of milliseconds
and vary significantly. These delays are close enough
to the limit on total latency that users will tolerate, so it becomes
crucial to reduce the number of message round-trips taken between data centers, and desirable to avoid waiting
for the slowest data center to respond.&lt;/p&gt;

&lt;p&gt;The traditional mechanism for transactions that are
distributed across databases is two-phase commit (2PC), but
this has serious drawbacks in a geo-replicated system. 2PC
depends on a reliable coordinator to determine the outcome
of a transaction, so it will block for the duration of a coordinator
failure, and (even worse) the blocked transaction will
be holding locks that prevent other transactions from making
progress until the recovery is completed.&lt;/p&gt;

&lt;p&gt;MDCC requires only a single wide-area message
round-trip to commit a transaction in the common case, and
is “master-bypassing”, meaning it can read or update from
any node in any data center. &lt;/p&gt;

&lt;p&gt;MDCC is the first protocol to use Generalized Paxos [15] as a commit protocol on a per record basis, combining it with techniques from
the database community (escrow transactions [19] and demarcation
[3]). The key idea is to achieve single round-trip
commits by 1) executing parallel Generalized Paxos on each
record, 2) ensuring every prepare has been received by a
fast quorum of replicas, 3) disallowing aborts for successfully
prepared records, and 4) piggybacking notification of
commit state on subsequent transactions.&lt;/p&gt;

&lt;h2&gt;Architecture Overview&lt;/h2&gt;

&lt;p&gt;MDCC uses a library-centric approach similar to the architectures
of DBS3 [5], Megastore [2] or Spanner &lt;a href=&quot;as%0Ashown%20in%20Figure%201&quot;&gt;8&lt;/a&gt;. This architecture separates the stateful
component of a database system as a distributed record manager.
All higher-level functionality (such as query processing
and transaction management) is provided through a stateless
DB library, which can be deployed at the application server.&lt;/p&gt;

&lt;p&gt;As a result, the only stateful component of the architecture,
the storage node, is significantly simplified and scalable
through standard techniques such as range partitioning,
whereas all higher layers of the database can be replicated
freely with the application tier because they are stateless.&lt;/p&gt;

&lt;p&gt;MDCC places storage nodes in geographically distributed
data centers, with every node being responsible for one or more horizontal partitions. we assume
for the remainder of the paper that every data center
contains a full replica of the data, and the data within a single
data center is partitioned across machines.&lt;/p&gt;

&lt;p&gt;The DB library provides a programming model for transactions
and is mainly responsible for coordinating the replication
and consistency of the data using MDCC’s commit
protocol. The DB library also acts as a transaction manager
and is responsible to determine the outcome of a transaction.&lt;/p&gt;

&lt;p&gt;In contrast to many other systems, MDCC supports an individual
master per record, which can either be storage nodes
or app-server and is responsible to coordinate the updates
to a record.&lt;/p&gt;

&lt;h2&gt;The MDCC Protocol&lt;/h2&gt;

&lt;h3&gt;Classic Paxos&lt;/h3&gt;

&lt;p&gt;Paxos distinguishes between clients, proposers, acceptors
and learners. These can be directly mapped to our scenario,
where clients are app-servers, proposers are masters, acceptors
are storage nodes and all nodes are learners.&lt;/p&gt;

&lt;p&gt;The basic idea in Classic Paxos [14], as applied for replicating
a transaction’s updates to data, is as follows: Every
record has a master responsible for coordinating updates to
the record. At the end of a transaction, the app-server sends
the update requests to the masters of each the record. The master informs all
storage nodes responsible for the record that it is the master
for the next update. It is possible that multiple masters exist
for a record, but to make progress, eventually only one
master is allowed. The master processes the client request
by attempting to convince the storage nodes to agree on it.
A storage node accepts an update if and only if it comes
from the most recent master the node knows of, and it has
not already accepted a more recent update for the record.&lt;/p&gt;

&lt;p&gt;the Classic Paxos algorithm operates in
two phases. Phase 1 tries to establish the mastership for
an update for a specific record r. A master P, selects a
proposal number m, also referred to as a ballot number or
round, higher than any known proposal number and sends
a Phase1a request with m to at least a majority of storage
nodes responsible for r. The proposal numbers must be unique for each master because they are used to determine
the latest request. If a storage node receives a Phase1a request
greater than any proposal number it has already responded
to, it responds with a Phase1b message containing
m, the highest-numbered update (if any) including its proposal
number n, and promises not to accept any future requests
less than or equal to m. If P receives responses containing
its proposal number m from a majority QC of storage
nodes, it has been chosen as a master. Now, only P will be
able to commit a value for proposal number m.&lt;/p&gt;

&lt;p&gt;Phase 2 tries to write a value. P sends an accept request
Phase2a to all the storage nodes of Phase 1 with the ballot
number m and value v. v is either the update of the highestnumbered
proposal among the Phase1b responses, or the
requested update from the client if no Phase1b responses
contained a value.&lt;/p&gt;

&lt;h3&gt;Transaction Support&lt;/h3&gt;

&lt;p&gt;We guarantee this consistency level by using a Paxos instance
per record to accept an option to execute the update,
instead of writing the value directly. After the app-server
learns the options for all the records in a transaction, it commits
the transaction and asynchronously notifies the storage
nodes to execute the options. If an option is not yet executed,
it is called an outstanding option.&lt;/p&gt;

&lt;p&gt;Updates to records create new versions, and are represented
in the form vread → vwrite, where vread is the
version of the record read by the transaction and vwrite is
the new version of the record. This allows MDCC to detect
write-write conflicts by comparing the current version of a
record with vread. If they are not equal, the record was modified
between the read and write and a write-write conflict
was encountered.&lt;/p&gt;

&lt;p&gt;The app-server coordinates the transaction by trying to
get the options accepted for all updates. It proposes the options
to the Paxos instances running for each record, with the
participants being the replicas of the record. Every storage
node responds to the app-server with an accept or reject of
the option, depending on if vread is valid. Hence, the storage nodes make an
active decision to accept or reject the option. This is fundamentally
different than existing uses of Paxos , which require to send a fixed value (e.g., the “final” accept or commit
decision) and only decide based on the ballot number if the
value should be accepted. The reason why this change does
not violate the Paxos assumptions is that we defined at the
end of Section 3.1.1 that a new record version can only be
chosen if the previous version was successfully determined.
Thus, all storage nodes will always make the same abort or
commit decision.&lt;/p&gt;

&lt;p&gt;Just as in 2PC, the app-server commits a transaction when
it learns all options as accepted, and aborts a transaction
when it learns any option as rejected. The app-server learns
an option if and only if a majority of storage nodes agrees
on the option. &lt;/p&gt;

&lt;p&gt;In contrast to 2PC we made another important
change: MDCC does not allow clients or app-servers
to abort a transaction once it has been proposed. Decisions
are determined and stored by the distributed storage nodes
with MDCC, instead of being decided by a single coordinator
with 2PC. This ensures that the commit status of a
transaction depends only on the status of the learned options
and hence is always deterministic even with failures.&lt;/p&gt;

&lt;p&gt;If the app-server determines that the transaction is aborted
or committed, it informs involved storage nodes through a
Learned message about the decision. The storage nodes in
turn execute the option (make visible) or mark it as rejected.&lt;/p&gt;

&lt;p&gt;We apply a simple
pessimistic strategy to avoid deadlocks. The core idea is to
relax the requirement that we can only learn a new version if
the previous instance is committed.&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Jan 2018 22:14:04 +0800</pubDate>
        <link>http://masutangu.com/2018/01/mit-6824-note-5/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/01/mit-6824-note-5/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（四）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;ZooKeeper&lt;/h1&gt;

&lt;h2&gt;Abstract&lt;/h2&gt;

&lt;p&gt;ZooKeeper 旨在提供简单高效的内核以供客户端实现更复杂的 coordination primitives。In addition to the wait-free
property, ZooKeeper provides a &lt;strong&gt;per client guarantee of FIFO execution of requests&lt;/strong&gt; and &lt;strong&gt;linearizability for all requests that change the ZooKeeper state&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;为了保证状态更新操作（写请求）的 linearizability，ZooKeeper 实现了 &lt;strong&gt;Zab，一个基于 leader 的原子广播协议&lt;/strong&gt;。In ZooKeeper, &lt;strong&gt;servers process read operations locally, and we do not use Zab to totally order them&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在客户端缓存数据是提高读性能的重要技术，&lt;strong&gt;ZooKeeper 提供了 watch 机制&lt;/strong&gt;，不直接管理客户端缓存。&lt;/p&gt;

&lt;h2&gt;Service overview&lt;/h2&gt;

&lt;p&gt;ZooKeeper 提供给客户端 znode 的抽象，znode 有下列两种类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regular&lt;/strong&gt; : 由客户端显式创建和删除&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ephemeral&lt;/strong&gt;: 由客户端创建，可以由客户端显式删除，当会话终止时系统也会自动删除&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ZooKeeper 实现了 watch 机制，当数据发生改变时通知客户端，而不必通过让客户端轮询服务器的方式。&lt;/p&gt;

&lt;p&gt;客户端连接到 ZooKeeper 时会初始化一个 session。Session 有超时机制，当 ZooKeeper 在超时时间内没有收到来自客户端 session的任何信息时，会判定该客户端已经挂掉。&lt;/p&gt;

&lt;h2&gt;ZooKeeper guarantees&lt;/h2&gt;

&lt;p&gt;ZooKeeper 具备以下两个基础保证：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linearizable writes&lt;/strong&gt;: all requests that update the state of ZooKeeper are serializable and respect precedence;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;FIFO client order&lt;/strong&gt;: all requests from a given client are executed in the order that they were sent by the client.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Examples of primitives&lt;/h2&gt;

&lt;h3&gt;Simple Locks without Herd Effect&lt;/h3&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Lock
1 n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if n is lowest znode in C, exit
4 p = znode in C ordered just before n
5 if exists(p, true) wait for watch event
6 goto 2

Unlock
1 delete(n)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Write Lock
1 n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if n is lowest znode in C, exit
4 p = znode in C ordered just before n
5 if exists(p, true) wait for event
6 goto 2

Read Lock
1 n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if no write znodes lower than n in C, exit
4 p = write znode in C ordered just before n
5 if exists(p, true) wait for event
6 goto 3
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;h2&gt;ZooKeeper Implementation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-4/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;收到请求时，由 request processor 处理。如果是写请求，则使用 zab 协议，最终提交到 ZooKeeper 数据库的修改将会被复制到该系统上所有的服务器。&lt;/p&gt;

&lt;p&gt;复制数据库是一个内存数据库，&lt;strong&gt;在变更被应到到内存数据库之前，我们强制将更新记录刷到磁盘上以实现 recoverability&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;每个 ZooKeeper 服务器都接收处理来自客户端的请求。 读操作由每个服务器的本地数据库来处理，写请求则由 zab 协议处理。&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Request Processor&lt;/h3&gt;

&lt;p&gt;与客户端发送的请求不同，&lt;strong&gt;事务是幂等的&lt;/strong&gt;。When the leader receives a write request, it calculates what the state of the system will be when the write is applied and transforms it into a transaction that captures this new state. The future state must be calculated because there may be outstanding transactions that have not yet been applied to the database. （尽管 transaction 可能执行多次，但通过计算 state 的方式，且保证多次执行的 transaction 也按照原先的顺序，就能保证幂等性）&lt;/p&gt;

&lt;h3&gt;Atomic Broadcast&lt;/h3&gt;

&lt;p&gt;所有更新 ZooKeeper 状态的请求都被转发到 leader。 Leader 执行请求并通过 Zab，一个原子广播协议广播该变更。Zab 采用majority quorums 来决定一个建议，因此 ZooKeeper 在 2F + 1 服务器的场景下最多可以容忍 F 台服务器故障。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Because state changes depend on the application of previous state changes, Zab provides stronger order guarantees than regular atomic broadcast.&lt;/strong&gt; More specifically, Zab guarantees that &lt;strong&gt;changes broadcast by a leader are delivered in the order they were sent&lt;/strong&gt; and &lt;strong&gt;all changes from previous leaders are delivered to an established leader before it broadcasts its own changes&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Replicated Database&lt;/h3&gt;

&lt;p&gt;重放消息以恢复状态非常耗时，因此 ZooKeeper 定期进行快照，恢复时只需要重放快照后的消息。 为了不锁住状态，ZooKeeper 采用&lt;strong&gt;模糊快照&lt;/strong&gt;的方式。对树进行深度优先扫描，原子读取每个 znode 的数据和元数据并写入磁盘。Since the resulting fuzzy snapshot may have applied some subset of the state changes delivered during the generation of the snapshot, the result may not correspond to the state of ZooKeeper at any point in time. However, since state changes are idempotent, we can apply them twice as long as we apply the state changes in order.&lt;/p&gt;

&lt;h3&gt;Client-Server Interactions&lt;/h3&gt;

&lt;p&gt;读请求由服务器本地处理。每个读请求都被一个 zxid 标记，表示服务器看到的最后一个事务。本地读取的方式提高了性能，但可能返回旧数据。客户端可以在读操作后调用 sync 来保证返回的是最新的值。In our implementation, we do not need to atomically broadcast sync as we use a leader-based algorithm, and we simply place the sync operation at the end of the queue of requests between the leader and the server executing the call to sync. In order for this to work, the follower must be sure that the leader is still the leader. If there are pending transactions that commit, then the server does not suspect the leader. If the pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction.&lt;/p&gt;

&lt;p&gt;ZooKeeper 服务器以 FIFO 的顺序来处理客户端的请求。Response 包含了 zxid。Even heartbeat messages during intervals of no activity include the last zxid seen by the server that the client is connected to. If the client connects to a new server, that new server ensures that its view of the ZooKeeper data is at least as recent as the view of the client by checking the last zxid of the client against its last zxid. If the client has a more recent view than the server, the server does not reestablish the session with the client until the server has caught up.&lt;/p&gt;

&lt;h1&gt;Linearizability versus Serializability&lt;/h1&gt;

&lt;p&gt;注：摘自 &lt;a href=&quot;http://www.bailis.org/blog/linearizability-versus-serializability/&quot;&gt;Linearizability versus Serializability&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Linearizability: single-operation, single-object, real-time order&lt;/h2&gt;

&lt;p&gt;Linearizability is a guarantee about single operations on single objects. It provides a real-time  guarantee on the behavior of a set of single operations.&lt;/p&gt;

&lt;p&gt;In plain English, under linearizability, writes should appear to be instantaneous. Imprecisely, once a write completes, all later reads (where “later” is defined by wall-clock start time) should return the value of that write or the value of a later write. Once a read returns a particular value, all later reads should return that value or the value of a later write.&lt;/p&gt;

&lt;p&gt;Linearizability for read and write operations is synonymous with the term “atomic consistency” and is the “C,” or “consistency,” in Gilbert and Lynch’s proof of the CAP Theorem. We say linearizability is composable (or “local”) because, if operations on each object in a system are linearizable, then all operations in the system are linearizable.&lt;/p&gt;

&lt;h2&gt;Serializability: multi-operation, multi-object, arbitrary total order&lt;/h2&gt;

&lt;p&gt;Serializability is a guarantee about transactions, or groups of one or more operations over one or more objects. It guarantees that the execution of a set of transactions (usually containing read and write operations) over multiple items is equivalent to some serial execution (total ordering) of the transactions.&lt;/p&gt;

&lt;p&gt;Serializability is the traditional “I,” or isolation, in ACID. If users’ transactions each preserve application correctness (“C,” or consistency, in ACID), a serializable execution also preserves correctness. Therefore, serializability is a mechanism for guaranteeing database correctness.&lt;/p&gt;

&lt;p&gt;Serializability is not composable. Serializability does not imply any kind of deterministic order—it simply requires that some equivalent serial execution exists.&lt;/p&gt;

&lt;p&gt;关于 Serializability 和 Linearizability，可以读读&lt;a href=&quot;https://36kr.com/p/5037166.html&quot;&gt;分布式系统一致性的发展历史&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Transaction Management in the R* Distributed Database Management System&lt;/h1&gt;

&lt;p&gt;这篇论文的主题是关于分布式数据库系统的事务管理，着重描述了 R* 提交协议：Presumed Abort（PA）和 Presumed Commit（PC）。PA 和 PC 是著名的 two-phase（2P）提交协议的扩展。&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;R* is an experimental, distributed database management system (DDBMS). In a distributed database
system, the actions of a transaction (an atomic unit of consistency and recovery) may occur at more than one site. &lt;strong&gt;A commit protocol is needed to guarantee the uniform commitment of distributed transaction executions.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Some of the desirable characteristics in a commit protocol are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;guaranteed transaction atomicity always&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ability to “forget” outcome of commit processing after a short amount of time（不需要一直记录结果）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;minimal overhead in terms of log writes and message traffic&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;optimized performance in the no-failure case&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;exploitation of completely or partially read-only transactions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;maximizing the ability to perform unilateral aborts&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Multilevel hierarchical commit protocols are suggested to be more natural than the conventional two-level (one coordinator and a set of subordinates) protocols.&lt;/strong&gt; With these goals in mind, we extended the conventional 2P commit protocol to support a tree of processes and defined the &lt;strong&gt;Presumed Abort (PA)&lt;/strong&gt; and the &lt;strong&gt;Presumed Commit (PC)&lt;/strong&gt; protocols to improve the performance of distributed transaction commit. &lt;/p&gt;

&lt;h2&gt;The Two-Phase Commit Protocol&lt;/h2&gt;

&lt;p&gt;在 2P 提交协议中，分布式事务执行模型包括了一个 coordinator 进程，它连接了客户端应用程序和其他称为 subordinates 的进程。Subordinates 进程彼此之间不交互，只和 coordinator 交互。每个事务分配一个全局唯一的名字。&lt;/p&gt;

&lt;h3&gt;2P Under Normal Operation&lt;/h3&gt;

&lt;p&gt;当 coordinator 接收到来自用户的事务提交请求时，并行的发送 PREPARE 消息给 subordinates，此时进入提交协议的第一阶段。如果该事务可以提交，subordinate 先 force-write 一条 prepare log record，之后发送 YES VOTE 给 coordinator，再等待 coordinator 返回最后的决策（commit 还是 abort），之后 subordinate 进入 &lt;strong&gt;prepare&lt;/strong&gt; 状态，无法单方面终止或提交事务。如果 subordinate 决定终止事务，先 force-write 一条 abort record，之后发送 NO VOTE 给 coordinator。因为 NO VOTE 表示否决，subordinate 不需要等待 coordinator，直接终止该事务，释放锁并“忘记”该事务（不需要记录任何信息了）。&lt;/p&gt;

&lt;p&gt;当 coordinator 接收到所有 subordinates 的投票时，进入提交协议的第二阶段。如果所有投票都是 YES，则 coordinator 进入 &lt;strong&gt;committing&lt;/strong&gt; 状态：首先 force-write 一条 commit record，发送 COMMIT 消息给所有 subordinates. 在 force-write 成功执行后，事务即可提交，且通知客户端事务已经成功提交。如果 coordinator 有收到 NO VOTE，则进入 &lt;strong&gt;aborting&lt;/strong&gt; 状态：force-write 一条 abort record，发送 ABORT 消息给处于准备状态的或者没有回 PREPARE 消息的 subordinates。&lt;/p&gt;

&lt;p&gt;如果 subordinate 收到 COMMIT 消息，则进入 &lt;strong&gt;committing&lt;/strong&gt; 状态：先 force-write 一条 commit record，发送 ACK 消息给 coordinator，然后提交事务并且“忘记”该事务。如果收到的是 ABORT 消息，进入 &lt;strong&gt;aborting&lt;/strong&gt; 状态：force-write 一条 abort record，发送 ACK 消息给 coordinator，然后终止事务并且“忘记”该事务。当 coordinator 收到所有 subordinates 的 ACK 消息（不包括 NO VOTE 的 subordinates），写一条 end record 然后“忘记”该事务。&lt;/p&gt;

&lt;p&gt;The general principle on which the protocols described in this paper are based is that if a subordinate acknowledges the receipt of any particular message, then it should make sure (by forcing a log record with the information in that message before sending the ACK) that it will never ask the coordinator about that piece of information. &lt;/p&gt;

&lt;p&gt;log 包含了如下内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;the type (prepare, end, etc.) of the record&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the identity of the process that writes the record&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the name of the transaction&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the identity of the coordinator&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the names of the exclusive locks held by the writer in the case of prepare records&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the identities of the subordinates in the case of the commit/abort records written by the coordinator&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结下，提交事务总共需要：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;subordinate 写两条记录：prepare record 和 commit record，且发送两条消息：YES VOTE 和 ACK&lt;/li&gt;
&lt;li&gt;coordinator 发送两条消息：PREPARE 和 COMMIT，以及写两条记录：commit record（force-write）和 end record（非 force-write） &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;2P and Failures&lt;/h3&gt;

&lt;p&gt;每个站点都存在一个 recovery 进程，处理来自其他站点的 recovery 进程的信息。当从 crash 中恢复时，recovery 进程读取 stable storage 的 log，并且在 virtual storage 中重建。在 virtual storage 的这部分信息有下列用途：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应答其他站点发来的事务查询请求，这些事务的 coordinators 是运行在该挂掉的站点上（answer queries from other sites about transactions that had their coordinators at this site）&lt;/li&gt;
&lt;li&gt;发送事务的查询信息给其他站点的 subordinates，这些事务的 coordinators 是运行在该挂掉的站点上（send unsolicited information to other sites that had subordinates for transactions that had their coordinators at this site）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 virtual storage 构建的好处在于可以快速回应其他站点的查询，而不需要从 stable storage 查询日志。&lt;/p&gt;

&lt;p&gt;当 recovery 进程发现有事务处于 &lt;strong&gt;prepare&lt;/strong&gt; 状态，则会定期向 coordinator 询问该事务应该如何处理（commit 还是 abort）。如果 recovery 进程发现有事务在执行过程中崩溃，没有留下任何日志，则直接回滚操作，写一条 abort record，然后“忘记”。&lt;/p&gt;

&lt;p&gt;如果 recovery 进程发现事务处于 &lt;strong&gt;committing/aborting&lt;/strong&gt; 状态，它将尝试定期发送 COMMIT/ABORT 给所有还没 ACK 过的 subordinates。一旦收到所有 ACK，则写一条 end record 并且“忘记”该事务。&lt;/p&gt;

&lt;p&gt;如果 recovery 进程收到一条事务查询，但 virtual storage 中没有该事务的信息，则直接返回 ABORT。&lt;/p&gt;

&lt;h2&gt;The Presumed Abort Protocol&lt;/h2&gt;

&lt;p&gt;上一节提到，在找不到关于事务的信息时，recovery 进程将返回 ABORT。这意味着当 coordinator 决定要 abort 事务时，可以直接“忘记”该事务，coordinator 和 subordinates 不再需要 force-write 一条 abort record，subordinates 也不需要 ACK 来自 coordinator 的 ABORT 消息。coordinator 也不在需要记录 end record 了。&lt;/p&gt;

&lt;p&gt;只读事务的情况下，leaf 进程直接返回 READ VOTE，释放锁，且“忘记”该事务。 A nonroot, nonleaf sends a READ VOTE only if its own vote and those of its subordinates’ are also READ VOTES. Otherwise, as long as none of the latter is a NO VOTE, it sends a YES VOTE.（存在部分只读事务）&lt;/p&gt;

&lt;p&gt;There will not be a second phase of the protocol if the root process is readonly and it gets only READ VOTES. In this case the root process, just like the other processes, writes no log records for the transaction. &lt;/p&gt;

&lt;p&gt;总结下，完全只读事务，所有进程都不需要写任何 log record。每个 nonleaf 进程发送一条 PREPARE 消息，每个 nonroot 进程发送一条信息 READ VOTE 消息。&lt;/p&gt;

&lt;h2&gt;ZooKeeper Note&lt;/h2&gt;

&lt;p&gt;Zookeeper: &lt;strong&gt;a generic &amp;quot;master&amp;quot; service&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Design challenges:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What API?&lt;/li&gt;
&lt;li&gt;How to make master fault tolerant?&lt;/li&gt;
&lt;li&gt;How to get good performance?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Challenges interact: good performance may influence API e.g., asynchronous interface to allow pipelining&lt;/p&gt;

&lt;p&gt;Sessions: clients sign into zookeeper&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Session allows a client to fail-over to another zookeeper service
Client know the term and index of last completed operationsend it on each request. &lt;strong&gt;Service performs operation only if caught up with what client has seen.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sessions can timeout
Client must refresh a session continuously send a heartbeat to the server (like a lease). &lt;strong&gt;Zookeeper considers client &amp;quot;dead&amp;quot; if doesn&amp;#39;t hear from a client.&lt;/strong&gt; Client may keep doing its thing (e.g., network partition) but cannot perform other zookeeper ops in that session&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Challenge: Duplicates client requests&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Scenario
Primary receives client request, fails
Client resends client request to new primary&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lab 3: 
&lt;strong&gt;Table to detect duplicates&lt;/strong&gt;
&lt;strong&gt;Limitation: one outstanding op per client&lt;/strong&gt;（缓存回包，所以需要等上一个请求处理完，才能处理下一个请求，对比是否重复，重复的话返回已经缓存好的回包）
Problem problem: cannot pipeline client requests&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Zookeeper:
Some ops are idempotent period
Some ops are easy to make idempotent: &lt;strong&gt;test-version-and-then-do-op&lt;/strong&gt;
Some ops &lt;strong&gt;the client is responsible for detecting dups&lt;/strong&gt;
 Consider the lock example.
   Create a file name that includes its session ID
     &amp;quot;app/lock/request-sessionID-seqno&amp;quot;
     zookeeper informs client when switch to new primary
 client runs getChildren()
   if new requests is there, all set
   if not, re-issue create&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Problem: read may return stale data if only master performs it&lt;/p&gt;

&lt;p&gt;Zookeeper solution: don&amp;#39;t promise non-stale data (by default)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reads are allowed to return stale data&lt;/li&gt;
&lt;li&gt;Reads can be executed by any replica&lt;/li&gt;
&lt;li&gt;Read throughput increases as number of servers increases&lt;/li&gt;
&lt;li&gt;Read returns the last zxid it has seen
 So that new primary can catch up to zxid before serving the read
 Avoids reading from past&lt;/li&gt;
&lt;li&gt;Only sync-read() guarantees data is not stale&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sync optimization: &lt;strong&gt;avoid ZAB layer for sync-read&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Must ensure that read observes last committed txn. &lt;strong&gt;Leader puts sync in queue between it and replica.&lt;/strong&gt; If ops ahead of in the queue commit, then leader must be leader, otherwise, issue null transaction.(&lt;strong&gt;In same spirit read optimization in Raft paper&lt;/strong&gt;, see last par section 8 of raft paper)&lt;/p&gt;

&lt;h2&gt;2P Note&lt;/h2&gt;

&lt;p&gt;What about concurrent transactions?&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;x and y are bank balances
x and y start out as $10

T1 is doing a transfer of $1 from x to y
T1:
  add(x, 1)  -- server A
  add(y, -1) -- server B
T2:
  tmp1 = get(x)
  tmp2 = get(y)
  print tmp1, tmp2
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;Problem: &lt;strong&gt;What if T2 runs between the two add() RPCs?&lt;/strong&gt; Then T2 will print 11, 10 money will have been created!  T2 should print 10,10 or 9,11.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The traditional correctness definition is &amp;quot;serializability&amp;quot;.&lt;/strong&gt;  Results should be as if transactions ran one at a time in some order as if T1, then T2; or T2, then T1. The results for the two differ, either is OK.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&amp;quot;Two-phase locking&amp;quot; is one way to implement serializability&lt;/strong&gt;. Each database record has a lock. The lock is stored at the server that stores the record. Transaction must wait for and acquire a record&amp;#39;s lock before using it. Thus add() handler implicitly acquires lock when it uses record. x or y transaction holds its locks until &lt;em&gt;after&lt;/em&gt; commit or abort.&lt;/p&gt;

&lt;p&gt;What are locks really doing?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When transactions conflict, locks delay one to force serial execution.&lt;/li&gt;
&lt;li&gt;When transactions don&amp;#39;t conflict, locks allow fast parallel execution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft and two-phase commit solve different problems!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use Raft to get high availability by replicating&lt;/strong&gt;, i.e. to be able to operate when some servers are crashed. The servers all do the &lt;em&gt;same&lt;/em&gt; thing&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use 2PC when each subordinate does something different&lt;/strong&gt; and &lt;strong&gt;all of them must do their part&lt;/strong&gt;. &lt;strong&gt;2PC does not help availability&lt;/strong&gt; since all servers must be up to get anything done. &lt;strong&gt;Raft does not ensure that all servers do something&lt;/strong&gt; since only a majority have to be alive.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What if you want &lt;strong&gt;high availability and distributed commit&lt;/strong&gt;?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each &amp;quot;server&amp;quot; should be a Raft-replicated service&lt;/li&gt;
&lt;li&gt;And the Transaction Coordinator(TC) should be Raft-replicated&lt;/li&gt;
&lt;li&gt;Run two-phase commit among the replicated services&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then you can tolerate failures and still make progress.&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Jan 2018 19:53:45 +0800</pubDate>
        <link>http://masutangu.com/2018/01/mit-6824-note-4/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/01/mit-6824-note-4/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>再见，2017</title>
        <description>&lt;p&gt;回看起去年的总结 &lt;a href=&quot;http://masutangu.com/2016/12/review/&quot;&gt;我的2016&lt;/a&gt;，心里挺有感慨的。17 年是出现转机的一年，在这最后一天，按照惯例，整理下这一年的历程和想法。&lt;/p&gt;

&lt;h1&gt;关于自己&lt;/h1&gt;

&lt;p&gt;在去年的时候，我总觉得自己在工作上挺失意的，各种原因下达不到自己的期望。还好，今年在工作方面对自己还算满意。&lt;strong&gt;最大的进步是对工作的心态上，不再有一些困惑和纠结。&lt;/strong&gt;工作上努力去争取有挑战的需求，空闲的时候则抽更多的时间在个人学习上，维持好工作和自学的平衡。我挺喜欢这样的节奏，忙的时候全心全意把工作做好，不忙的时候就把更多的时间投入到自己感兴趣的领域。&lt;/p&gt;

&lt;p&gt;在经历 16 年那段纠结的时间，如今自己努力一点点扭转，我也更加相信，&lt;strong&gt;在任何时候都要把主动权握在自己手里&lt;/strong&gt;。不管自己处在一个什么样的环境，对你是有利或者是不利的、有没有机遇、公不公平，&lt;strong&gt;把自己能控制的做好，在任何时候都会有选择权&lt;/strong&gt;。但另一方面，环境也是至关重要的一个因素，是金子总会发光，我觉得是悖论。或者说，&lt;strong&gt;是金子的话，应该主动寻找能让自己发光的机会&lt;/strong&gt;，而不是呆在沙堆中，等着别人来发掘你，该争取的东西就一定要去争取。&lt;/p&gt;

&lt;p&gt;个人学习上，下半年的主要精力都放在分布式系统上，跟着 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 的课程，读论文，做实验，记&lt;a href=&quot;http://masutangu.com/2017/11/mit-6824-note-1/&quot;&gt;笔记&lt;/a&gt;。这个课程受益匪浅，有些思路我也能顺利运用到工作上，这样的感觉挺不错的。&lt;/p&gt;

&lt;h1&gt;关于团队&lt;/h1&gt;

&lt;p&gt;平时喜欢篮球，也经常关注 NBA。在我看来，团队氛围和文化及其重要。勇士、马刺的成功，和他们的球队文化、管理层的理念是分不开的。&lt;strong&gt;球队的凝聚力、球员之间的化学反应，是无法体现在冷冰冰的数据上的。&lt;/strong&gt;读到这篇文章&lt;a href=&quot;https://zhuanlan.zhihu.com/p/31997176&quot;&gt;《你可能见证了勇士队的成功，却很少了解他们背后的球队文化》&lt;/a&gt;的时候，我内心有点被触动：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;即便克拉克离开了勇士队，勇士的主场球馆内依然挂着他身穿勇士球衣的照片，而克拉克自己也表示，虽然自己不在勇士，但是还是会和这些球员保持紧密的关系。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一支球队中，有不同的角色：全明星、先发、替补、饮水机管理员。但相信我，&lt;strong&gt;无论你在球队中扮演什么样的角色，每个人都希望自己是球队的一部分，是比赛的一部分，每个人都希望球队的胜利，也有自己贡献的那部分因素。&lt;/strong&gt;在这一年，我参与了项目组新项目的研发，大家一起对需求、讨论方案、前后台定协议、联调、改bug，没有什么撕逼、甩锅，我感觉到自己在团队中的角色，我知道自己能做些什么，这些都让我更加有动力去扮演好团队中的一份子。&lt;/p&gt;

&lt;h1&gt;一些不足&lt;/h1&gt;

&lt;p&gt;一直以来，我都比较难以保持稳定的状态。有时状态好的时候，就一头扎进去。前阵子有一段时间，每天都 6 点就自然醒，那段时间就天天很早到公司，吃完早餐后开始读论文，就这样每天都抽更多的时间去学习。但持续了大概一个月之后就累垮了，低迷了一两周。而且状态低迷的时候，自己也经常会焦虑，导致有些恶性循环。新的一年，希望自己能维持稳定些的状态，对自己也更有耐心些。&lt;strong&gt;细水长流&lt;/strong&gt;，做长远打算。&lt;/p&gt;

&lt;h1&gt;新年展望&lt;/h1&gt;

&lt;p&gt;新的一年，希望自己能在&lt;strong&gt;工作上能有更大的突破&lt;/strong&gt;，个人学习上继续按自己规划的路线走，&lt;strong&gt;扩展更多领域的知识，提升自己的核心竞争力&lt;/strong&gt;。还是相信那句话：&lt;strong&gt;Pursue excellence, and success will follow！&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 31 Dec 2017 16:28:17 +0800</pubDate>
        <link>http://masutangu.com/2017/12/2017-review/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/12/2017-review/</guid>
        
        
        <category>随笔</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（三）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;Spinnaker&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Spinnaker is an experimental datastore that is designed to run on a large cluster of commodity servers in a single datacenter. This paper describes Spinnaker’s Paxos-based replication protocol. The use of Paxos ensures that a data partition in Spinnaker will be available for reads and writes as long a majority of its replicas are alive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;实现持续可用性的一个解决方案是&lt;strong&gt;主从复制&lt;/strong&gt;，但主从复制存在以下缺陷：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在上图的例子中，主从节点都从 LSN=10 开始（a），之后 slave 节点挂了（b），master 节点继续接收写请求，一直到 LSN=20。之后 master 节点也挂了（c），之后 slave 节点恢复（d），然而，在此时 slave 节点不能接收任何读写请求因为它缺失了 LSN=11 到LSN＝20 之间的记录。如果要避免这种情况，只有在任意节点挂掉的时候，都阻塞写请求。但这样就降低了整个系统的 availability。&lt;/p&gt;

&lt;p&gt;分布式系统中，&lt;strong&gt;一致性模型描述了如何使不同的 relicas 保持同步&lt;/strong&gt;。&lt;strong&gt;强一致性&lt;/strong&gt;保证了所有的 replicas 都是一致的，但要实现强一致性需要牺牲 availability 或网络分区容忍性。CAP 理论提出 &lt;strong&gt;Consistency&lt;/strong&gt;、&lt;strong&gt;Availability&lt;/strong&gt; 和 &lt;strong&gt;Partition tolerance&lt;/strong&gt; 三者最多只能同时满足两项。&lt;/p&gt;

&lt;p&gt;比如 Dynamo 这样的系统，使用&lt;strong&gt;最终一致性&lt;/strong&gt;模型来提供高可用性和分区容忍性。Dynamo 是一个 AP 系统，牺牲了 Consistency。&lt;/p&gt;

&lt;p&gt;Spinnaker 使用基于 Paxos 的协议来实现日志提交和故障恢复。Paxos 确保了系统在大多数节点存活的情况下可以运作。Spinnaker 是一个 CA 系统，用于单一的 datacenter，并使用另外的 replication strategy 来保证跨 datacenter 的容错性。 &lt;/p&gt;

&lt;h2&gt;Related work&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Two-phase commit (2PC)&lt;/strong&gt; 是保持 replicas 一致的一种方式。但 2PC 更偏向于将每个 participant 当作一个独立的资源管理者，而不仅仅是 replica。使用 2PC 来实现 replication 有些 overkill，并且还有不少缺陷。首先单一节点失败会导致系统 abort。其次每个 transaction 都发起 2PC 会导致极差的性能。每次 2PC 都需要两次磁盘强制写和传输两条信息的时延。最后，2PC 在 coordinator 挂掉时无法运作。&lt;/p&gt;

&lt;p&gt;Amazon 的 Dynamo 通过&lt;strong&gt;分布式时钟&lt;/strong&gt;来解决最终一致性的问题。&lt;/p&gt;

&lt;p&gt;Google 的 Bigtable 提供了强一致性，和 spinnaker 不同的是，Bigtable 依赖 GFS 来存储数据和日志，还有实现 replication。这样每个 transaction 的 workload 就加重了（需要和 gfs 的 master 交互）。&lt;/p&gt;

&lt;h2&gt;Architecture&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Like Bigtable and PNUTS, Spinnaker distributes the rows of a table across its cluster using range partitioning. Each node is assigned a base key range, which is replicated on the next N − 1 nodes (N = 3 by default).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Each group of nodes involved in replicating a key range is denoted as a &lt;strong&gt;cohort&lt;/strong&gt;. Note that cohorts overlap.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个日志由一个 LSN 唯一的标记。Commit queue 是在内存的数据结构，用于存放 pending writes。写操作只有在接收到大多数 cohort 的 ack 之后才能提交。在此之前都存放在 commit queue 中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-3.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;已经提交的写操作存于 memtable 中，并被定期刷到被称为 SSTable 的 immutable disk structure。SSTable 会被定期合并以提升读性能并删除不需要的数据。&lt;/p&gt;

&lt;h2&gt;The replication protocol&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-4.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;提交一个写操作需要三次日志强制写和四条信息交互，不过大多数操作都是重叠的（可以并行）。&lt;/p&gt;

&lt;h2&gt;Recovery&lt;/h2&gt;

&lt;p&gt;Follower 的恢复需要两个阶段：&lt;strong&gt;local recovery&lt;/strong&gt; 和 &lt;strong&gt;catch up&lt;/strong&gt;。定义 f.cmt 表示 follower 的最后一个提交日志的 LSN，f.lst 表示 follower 的最后一个日志的 LSN。Local recovery 阶段，follower 从最近一次 checkpoint 开始重新执行日志直到 f.cmt，之后进入 catch up 阶段。Catch up 阶段，follower 通知 leader 自己的 f.cmt，leader 回复 f.cmt 之后所有的 commit writes。Leader 将会阻塞所有新的写请求直到 follower 已经跟上。&lt;/p&gt;

&lt;p&gt;当 leader 挂掉，新的 leader 将被选举，并且会确保新的 leader 会包含所有已提交的写操作。在老的 leader 挂掉时，有可能其提交的写操作在某些 followers 还处于 pending 的状态。新 leader 将使用下图的算法，继续提交所有 unresolved 写操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-5.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Leader election&lt;/h2&gt;

&lt;p&gt;选举算法如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-6.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Dynamo&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;一个系统的&lt;strong&gt;可靠性&lt;/strong&gt;和&lt;strong&gt;可扩展性&lt;/strong&gt;取决于如何管理应用状态。Amazon 采用一种高度去中心化，松耦合，由数百个服务组成的面向服务架构。在这种环境中尤其需要一个高可用的存储技术。&lt;/p&gt;

&lt;p&gt;Dynamo 被设计用于管理服务状态，要求具备非常高的可靠性，而且需要严格控制&lt;strong&gt;可用性、一致性、成本效益和性能&lt;/strong&gt;的之间的权衡。&lt;/p&gt;

&lt;p&gt;Amazon 服务平台中许多服务只需要主键访问数据存储。通常关系数据库模式会导致效率低下，且可扩展性和可用性也有限。Dynamo 提供了一个简单且唯一的主键接口，以满足这些应用的要求。&lt;/p&gt;

&lt;p&gt;Dynamo 综合一些著名的技术来实现可伸缩性和可用性：使用&lt;strong&gt;一致性哈希&lt;/strong&gt;划分和复制数据，一致性由&lt;strong&gt;对象版本&lt;/strong&gt;来实现。更新时副本之间的一致性是由&lt;strong&gt;类似仲裁（quorum-like）&lt;/strong&gt;的技术和&lt;strong&gt;去中心化的副本同步协议&lt;/strong&gt;来保证。Dynamo 采用了&lt;strong&gt;基于 gossip&lt;/strong&gt; 的分布式故障检测和 membership protocol。&lt;/p&gt;

&lt;h2&gt;System Assumptions and Requirements&lt;/h2&gt;

&lt;p&gt;Dynamo 的目标应用程序是通过较弱的一致性（ACID中的“C”）来达到高可用性。Dynamo 不提供任何数据隔离（ACID中的“I”）保证，只允许单一主键更新。&lt;/p&gt;

&lt;h2&gt;Design Considerations&lt;/h2&gt;

&lt;p&gt;对于容易出现服务器和网络故障的系统，可使用&lt;strong&gt;乐观复制&lt;/strong&gt;来提高系统的可用性，数据变化允许在后台发送到副本，同时容忍网络断开。这种做法带来的挑战是如何检测和协调由此引发的冲突。协调的过程引入了两个问题：&lt;strong&gt;何时协调&lt;/strong&gt;以及&lt;strong&gt;谁负责协调&lt;/strong&gt;。Dynamo 被设计成&lt;strong&gt;最终一致性（eventually consistent）&lt;/strong&gt;的数据存储，所有的更新操作最终会分发到所有副本。&lt;/p&gt;

&lt;p&gt;一个重要的设计考虑的因素是何时去协调更新操作冲突，例如是在读还是写过程中协调冲突。许多传统 data store 在写的过程中协调冲突，从而保持读的复杂度相对简单。另一方面，&lt;strong&gt;Dynamo 的目标是“永远可写”&lt;/strong&gt;。对于 Amazon 许多服务来讲，拒绝客户的更新操作可能导致糟糕的客户体验。这迫使我们&lt;strong&gt;将协调冲突的复杂性推给“读”&lt;/strong&gt;，以确保“写”永远不会拒绝。&lt;/p&gt;

&lt;p&gt;其他重要的设计原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;增量的可扩展性（Incremental scalability）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dynamo 需要能够水平扩展一台存储主机，而对系统操作者和系统本身的影响很小。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;对称性（Symmetry）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;每个 Dynamo 节点应该有一样的责任，不应该存在有区别或具备特殊的角色或额外的责任的节点。根据我们的经验，对称性(symmetry)简化了系统的配置和维护。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;去中心化（Decentralization）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;是对对称性的延伸，设计应采用有利于去中心化而不是集中控制的技术。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;异质性（Heterogeneity）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;系统必须能够利用其运行所在的基础设施的异质性。例如，负载的分配必须与各个独立的服务器的能力成比例。这样就可以一次只增加一个高处理能力的节点，而无需一次升级所有的主机。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Partitioning Algorithm&lt;/h2&gt;

&lt;p&gt;Dynamo 的关键设计要求之一是&lt;strong&gt;增量可扩展性&lt;/strong&gt;，这需要一个机制来将数据动态划分到系统中的节点上。Dynamo 的分区方案依赖于一致哈希。&lt;strong&gt;一致性哈希的主要优点是节点的新增或减少只影响其最直接的邻居&lt;/strong&gt;，而对其他节点没影响。&lt;/p&gt;

&lt;p&gt;基本的一致性哈希算法存在一些不足：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;环上​​的节点随机分配位置导致数据和负载的不均匀分布&lt;/li&gt;
&lt;li&gt;基本算法无法满足节点性能的&lt;strong&gt;异质性（Heterogeneity）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决这些问题，&lt;strong&gt;Dynamo 引入虚拟节点&lt;/strong&gt;：每个节点被分配到环上多点，而不是只映射到环上的一个单点。系统中每个节点对多个虚拟节点负责。&lt;/p&gt;

&lt;p&gt;使用虚拟节点具有以下优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果一个节点不可用，这个节点的负载将均匀地分散给其余的可用节点&lt;/li&gt;
&lt;li&gt;当一个节点再次可用，或者是新添加到系统中，the newly available node accepts a roughly equivalent amount of load from each of the other available nodes. &lt;/li&gt;
&lt;li&gt;一个节点负责的虚拟节点数目由其处理能力来决定（heterogeneity）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Replication&lt;/h2&gt;

&lt;p&gt;每个键 K 被分配到一个协调器节点。协调器节点负责其管辖范围内的数据的复制。除了在本地存储每个 key 外，协调器节点也将 key 复制到环上顺时针方向的 N-1 后继节点。系统中每个节点负责环上的从其自己到第 N 个前继节点间的区域。&lt;/p&gt;

&lt;p&gt;负责存储一个特定的键的节点列表被称为&lt;strong&gt;首选列表&lt;/strong&gt;。&lt;/p&gt;

&lt;h2&gt;Data Versioning&lt;/h2&gt;

&lt;p&gt;Dynamo 提供最终一致性，允许更新操作异步地传播到所有副本。put 调用可能在更新操作被所有的副本执行之前就返回给调用者，这会导致在随后的 get 操作可能会返回一个不是最新的对象。&lt;/p&gt;

&lt;p&gt;Dynamo 将每次数据修改的结果当作一个全新且不可修改的数据版本。它允许同一时间存在多个版本的对象。大多数情况，新版本包括了老的版本，系统自己可以决定权威版本（语法协调 syntactic reconciliation）。然而，在出现失败且伴随并发更新操作的时候，可能导致对象的版本冲突。在这种情况下，系统无法协调，需要由客户端必须执行协调，将多个分支演化后的数据折叠成一个合并的版本（语义协调 semantic reconciliation）。&lt;/p&gt;

&lt;p&gt;Dynamo 使用&lt;strong&gt;矢量时钟&lt;/strong&gt;来捕捉同一对象不同版本的因果关系。矢量时钟实际上是一系列（节点，计数器）对。一个矢量时钟和每个对象的每个版本相关联。通过审查向量时钟，可以判断一个对象的两个版本是平行或因果顺序。&lt;strong&gt;如果第一个时钟对象上的计数器小于或等于第二个时钟对象上的所有节点，那么第一个是第二个的祖先&lt;/strong&gt;，可以被忽略。否则这两个变化是冲突的，需要协调。&lt;/p&gt;

&lt;p&gt;在 Dynamo 中，当客户端更新一个对象时必须指定它正要更新哪个版本，这通过从早期的读操作中获得的上下文对象来指定。它包含了向量时钟信息。当处理一个读请求，如果 Dynamo 访问到多个不能语法协调的分支，它将返回处于分支叶子的所有对象。&lt;/p&gt;

&lt;h2&gt;Execution of get () and put () operations&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;处理读或写操作的节点被称为协调员。&lt;/strong&gt;通常，这是首选列表中前 N 个节点中的第一个。如果请求是通过 load balancer 收到，访问 key 的请求可能被路由到环上任一节点。在这种情况下，如果该节点不是请求的 key 的首选列表中前 N 个节点之一，该节点将请求转发到首选列表前 N 个中的第一个节点。&lt;/p&gt;

&lt;p&gt;为了保持副本的一致性，Dynamo 使用的一致性协议类似于仲裁（Quorum NRW 模型）。该协议有两个关键配置值：R 和 W。R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. N 则是数据的副本数量。设定 R 和 W，使得 R + W &amp;gt; N，即&lt;strong&gt;读操作副本加上写操作副本必须大于数据的副本量&lt;/strong&gt;。在此模型中，一个 get 或者 put 操作延时是由最慢的 R 或 W 副本决定的。因此 R 和 W 通常配置为小于 Ｎ，以减小时延。&lt;/p&gt;

&lt;p&gt;当收到对 key 的 put 请求时，协调员为新版本生成向量时钟并写入本地，然后将新版本（带上向量时钟）发送给首选列表中的前 Ｎ 个可达节点。如果至少 Ｗ-1 个节点返回了响应，那么这个写操作被认为是成功的。&lt;/p&gt;

&lt;p&gt;同样，对于 get 请求，协调员从首选列表中前 Ｎ 个可达节点处请求该 key 所有版本的数据，然后等待 Ｒ 个响应，然后返回结果给客户端。如果协调员收到多个版本的数据，它返回所有它认为没有因果关系的版本。&lt;/p&gt;

&lt;h2&gt;Handling Failures: Hinted Handoff&lt;/h2&gt;

&lt;p&gt;Dynamo 如果使用传统的仲裁方式，在服务器故障和网络分裂的情况下将不可用。因此 Dynamo 使用了&lt;strong&gt;“马虎仲裁”&lt;/strong&gt;机制，所有的读，写操作是由首选列表上的前 N 个健康的节点执行的，它们可能不总是在散列环上遇到的那前 N 个节点。&lt;/p&gt;

&lt;p&gt;举个例子，如果写操作过程中节点 A 暂时挂掉或不可达，本来发往 A 的副本现在被发送到节点 D。发送到 D 的副本将在其原数据中表明 A 节点才是该副本的预期接收者。接收到的暗示副本将被保存在单独的本地存储中定期扫描。在检测到了 A 已经复苏，D 会尝试发送副本到 A。一旦传送成功，D 可将数据从本地存储中删除，不会降低系统中的副本总数。&lt;/p&gt;

&lt;p&gt;数据中心有可能因为断电、冷却装置故障、网络故障或自然灾害发生故障。Dynamo 将每个对象跨多个数据中心地进行复制。从本质上讲，每个个 key 的首选列表的分布在多个数据中心。这些数据中心通过高速网络连接。这种&lt;strong&gt;跨多个数据中心的复制方案&lt;/strong&gt;使我们能够处理整个数据中心故障。&lt;/p&gt;

&lt;h2&gt;Handling permanent failures: Replica synchronization&lt;/h2&gt;

&lt;p&gt;Dynamo 实现了&lt;strong&gt;反熵（anti-entropy 或副本同步）协议&lt;/strong&gt;来保持副本同步。&lt;/p&gt;

&lt;p&gt;为了更快地检测副本之间的不一致性并减少传输的数据量，&lt;strong&gt;Dynamo 使用 MerkleTree&lt;/strong&gt;。MerkleTree 是一个哈希树，其叶子是各个 key 的哈希值，父节点为其各自孩子节点的哈希。MerkleTree 的主要优点是树的每个分支可以独立地检查。另外，MerkleTree 有助于减少为检查副本间不一致而传输的数据的大小。如果两树的根哈希值相等，那么树的叶节点值也相等，该节点不需要同步。&lt;/p&gt;

&lt;h2&gt;Membership and Failure Detection&lt;/h2&gt;

&lt;p&gt;去中心化的故障检测协议使用一个简单的 Gossip 式的协议，使系统中的每个节点可以了解其他节点加入或离开。详细可以参考论文《On
scalable and efficient distributed failure detectors》&lt;/p&gt;

&lt;h2&gt;Ensuring Uniform Load distribution&lt;/h2&gt;

&lt;p&gt;随着时间和负载分布的影响，Dynamo 的划分方案也逐渐演化：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-3/illustration-7.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;策略 1：每个节点随机分配 T 个 token ，且基于 token 值进行分割&lt;/p&gt;

&lt;p&gt;使用这一策略会出现以下问题。首先，当新的节点加入系统时，它需要从其他节点上“窃取”其负责的 key ranges。然而，这些需要移交key ranges 给新节点的节点必须扫描他们的本地持久化存储。第二，当一个节点加入/离开系统，许多节点的 key range 发生变化，MertkleTree 需要重新计算，在生产系统上，这不是一个简单的操作。最后，由于 key range 的随机性，没有简单的办法为整个 key space 做快照，这使得归档过程变得复杂。在这个方案中，归档整个 key space 需要分别检索每个节点的 key，这是非常低效的。&lt;/p&gt;

&lt;p&gt;这个策略的根本问题是，&lt;strong&gt;数据划分&lt;/strong&gt;和&lt;strong&gt;数据安置&lt;/strong&gt;交织在一起。 &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;策略 2：每个节点随机分配 T 个 token，每个分区同等大小&lt;/p&gt;

&lt;p&gt;在此策略中，哈希空间被划分为 Q 个同样大小的分区/范围，每个节点随机分配 T 个 token。通常设置 Q 使得 Q &amp;gt;&amp;gt; N 且 Q &amp;gt;&amp;gt; S * T，其中 S 为系统的节点个数。在这一策略中，token 只是用来构造映射函数，该函数将哈希空间的值映射到一个有序列的节点列表，而不决定分区。A partition is placed on the first N unique nodes that are encountered while walking the consistent hashing ring clockwise from the end of the partition. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;策略 3：每个节点分配 Q/S 个 token，每个分区同等大小&lt;/p&gt;

&lt;p&gt;Each node is assigned Q/S tokens where S is the number of nodes in the system. When a node leaves the system, its tokens are randomly distributed to the remaining nodes such that these properties are preserved. Similarly, when a node joins the system it &amp;quot;steals&amp;quot; tokens from nodes in the system in a way that preserves these properties. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相对于策略 1，策略 3 达到更好的效率，每个节点需要维持的信息的大小降低了三个数量级。虽然存储不是一个主要问题，但节点间周期地gossip 成员信息，因此尽可能简化信息。除此之外，策略 3 更易于部署，理由如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;更快的 bootstrapping/recovery&lt;/p&gt;

&lt;p&gt;由于分区范围是固定的，它们可以保存在单独的文件，这意味着转移分区仅需要简单地转移文件。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;易于归档&lt;/p&gt;

&lt;p&gt;对数据集定期归档是 Amazon 存储服务的强制性要求。策略 3 归档整个数据集很简单，因为分区的文件可以被分别归档。相反，策略1 中 token 是随机选取的，归档存储的数据需要分别检索各个节点的 key，这通常是低效和缓慢的。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 15:19:31 +0800</pubDate>
        <link>http://masutangu.com/2017/12/mit-6824-note-3/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/12/mit-6824-note-3/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>MIT 6.824 学习笔记（二）</title>
        <description>&lt;p&gt;本系列文章是对 &lt;a href=&quot;https://pdos.csail.mit.edu/6.824/schedule.html&quot;&gt;MIT 6.824&lt;/a&gt; 课程的学习笔记。&lt;/p&gt;

&lt;h1&gt;Raft&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Raft 是用于管理复制日志的一致性算法。为了方便理解，Raft 将一致性算法分解为几个关键模块：&lt;strong&gt;Leader 选举&lt;/strong&gt;、&lt;strong&gt;日志复制&lt;/strong&gt;和&lt;strong&gt;安全性&lt;/strong&gt;，同时&lt;strong&gt;通过更强的一致性来减少需要考虑的状态&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;一致性算法允许一组机器像一个整体一样工作，即使其中一些机器挂掉。一致性算法在构建大规模可信赖系统中扮演重要的角色。&lt;/p&gt;

&lt;p&gt;Raft 和许多一致性算法类似，但他也有自己的新特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strong leader&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用比其他一致性算法更强的 leadership 形式。举例来说，log entries 只能从 leader 发往其他 server。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leader election&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用随机 timer 来选举 leader，简洁地解决了冲突的问题。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Membership changes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用 joint consensus 的方式来处理成员变化的问题。在变迁的过程中，两个不同的配置可以共存，以此来实现在变迁过程中集群仍然能持续运转。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Replicated state machines 通常用 replicated log 实现。一致性算法起的作用是保证多个 replicas 上的 replicated log 一致。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;实际系统中使用的一致性算法通常有下列特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;安全性保证&lt;/strong&gt;（绝对不会返回一个错误的结果）&lt;/p&gt;

&lt;p&gt;在非拜占庭错误情况下，网络延迟、分区、丢包、冗余和乱序等错误都可以保证不返回错误的结果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;可用性&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;集群中只要大多数的机器可运行并且能够相互通信、和客户端通信，服务就可用。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;不依赖时序来保证一致性&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通常情况下，小部分 slow servers 不会影响系统整体的性能&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Raft basics&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft 在任何时候都保证以下的特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Election Safety&lt;/strong&gt;: at most one leader can be elected in a given term.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leader Append-Only&lt;/strong&gt;: a leader never overwrites or deletes entries in its log; it only appends new entries. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Log Matching&lt;/strong&gt;: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leader Completeness&lt;/strong&gt;: if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;State Machine Safety&lt;/strong&gt;: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft 中每个 server 在任意时刻都处于 &lt;strong&gt;leader&lt;/strong&gt;、&lt;strong&gt;follower&lt;/strong&gt;、&lt;strong&gt;candidate&lt;/strong&gt; 这三种状态之一。Followers 被动的接收 leader 或 candidate 的请求，leader 处理所有客户端请求，candidate 则起了选举 leader 的角色。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-1.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Raft 把时间划分为&lt;strong&gt;任期（Terms）&lt;/strong&gt;，并保证每个任期内只有一个 leader。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Terms act as a logical clock in Raft&lt;/strong&gt;, and they allow servers to detect obsolete information such as stale leaders. &lt;strong&gt;Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft 服务器之间使用 RPC 进行通信，&lt;strong&gt;RequestVote RPC 用于选举，AppendEntries 用于 leader 向 followers 复制日志和心跳。&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Log replication&lt;/h2&gt;

&lt;p&gt;Raft 的日志机制维护下面两个特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If two entries in different logs have the same index and term, then they store the same command.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Safty&lt;/h2&gt;

&lt;p&gt;在任何 leader-based 的一致性算法中，leader 都必须存储所有已经提交的日志条目。Raft 使用了一种简单的办法，保证之前任期中已经提交的日志都会出现在新的 leader 中，不需要将这些日志传给新的 leader。这意味着&lt;strong&gt;日志的传送是单向的从 leader 发往 followers，并且 leader 从不覆盖本地已经存在的日志。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft 在投票过程中只允许&lt;strong&gt;包含全部已提交日志的 server 当选 leader&lt;/strong&gt;。Raft 通过比较日志中最后一条日志条目的索引值和任期来判断哪份日志更新。&lt;strong&gt;任期大的日志更加新，如果任期相同，那么日志索引值更大的新。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Leader 不能断定之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。&lt;/strong&gt;（请看论文的 figure 8）Raft 永远不会通过计算副本数目的方式提交一个之前任期内的日志条目。&lt;strong&gt;只有当前任期里的日志条目是通过计算副本数目的方式被提交。&lt;/strong&gt;一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。&lt;/p&gt;

&lt;h2&gt;Timing and availability&lt;/h2&gt;

&lt;p&gt;Raft 要求系统满足以下时间限制：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;broadcastTime ≪ electionTimeout ≪ MTBF
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;BroadcastTime 指的是并行发送 RPCs 给集群中的其他服务器并接收到响应的平均时间。MTBF 是单台服务器两次故障之间的平均时间。&lt;/p&gt;

&lt;p&gt;Raft 的 RPCs 通常耗时在 0.5ms 到 20ms 之间。因此 electionTimeout 最好处于 10ms 到 500ms 之间。&lt;/p&gt;

&lt;h2&gt;Cluster membership changes&lt;/h2&gt;

&lt;p&gt;为了保证配置修改机制的安全，在转换的过程中同一个任期中任何一个时间点都不能出现超过一个 leader。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-3.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Raft 使用 two-phase 的方式来保证安全性。集群首先切换到过渡的配置，我们称其为 &lt;strong&gt;joint consensus&lt;/strong&gt;。一旦 joint consensus 被提交，集群再切换到新的配置。&lt;strong&gt;Joint consensus&lt;/strong&gt; 是新老配置的结合：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日志条目被复制给集群中新、老配置的所有服务器&lt;/li&gt;
&lt;li&gt;新、旧配置的服务器都可以成为领导人&lt;/li&gt;
&lt;li&gt;Agreement (for elections and entry commitment) 需要分别在两种配置上获得大多数的支持&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群配置以特殊的日志条目保存在 replicated log 中来存储和通信，下图展示了配置转换的过程。一旦一个服务器将新的配置日志条目添加到本地日志中，它就会用这个配置来做决定（服务器总是使用最新的配置，无论该配置是否已经被提交）。这意味着 leader 要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mit-6824-note-2/illustration-4.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new 都不能单独做出决定，并且 Leader Completeness 的特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为 leader。这时 leader 创建一条 C-new 配置的日志条目并复制给集群就是安全的了，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，不在新的配置内的服务器就可以关闭了。&lt;/p&gt;

&lt;h2&gt;Client interaction&lt;/h2&gt;

&lt;p&gt;Raft 的目标是要实现 linearizable semantics(each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response) 但 Raft 是可能执行同一条命令多次的。解决方法是&lt;strong&gt;客户端对于每一条指令都赋予一个唯一的序列号，状态机记录每条指令最新的序列号和相应的响应。&lt;/strong&gt;如果接收到一条指令其序列号已经被执行了，那么就立即返回响应，无需重复执行指令。&lt;/p&gt;

&lt;p&gt;只读的操作可以直接处理而不需要写入日志，但可能返回脏数据。Raft 采用额外的机制在不写入日志的情况下保证读操作的正确性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader 必须知道最新的被提交日志。Raft 中通过 leader 在任期开始的时候提交一个空白的日志条目到日志中去来实现。&lt;/li&gt;
&lt;li&gt;Leader 在处理只读的请求之前需要检查自己是否已经被废黜了。Raft 中通过让 leader 在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Raft makes a few design choices that sacrifice performance for simplicity:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Raft follower rejects out-of-order AppendEntries RPCs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Rather than saving for use after hole is filled. Might be important if network re-orders packets a lot
And makes leader-&amp;gt;follower RPC pipelining harder&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Snapshotting is wasteful for big slowly-changing states&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A slow leader may hurt Raft, e.g. in geo-replication&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Experience suggests these have a big effect on performance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disk writes for persistence&lt;/li&gt;
&lt;li&gt;Message/packet/RPC overhead&lt;/li&gt;
&lt;li&gt;Need to execute logged commands sequentially&lt;/li&gt;
&lt;li&gt;Fast path for read-only operations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Appendix&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://thesecretlivesofdata.com/raft/&quot;&gt;http://thesecretlivesofdata.com/raft/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://thesquareplanet.com/blog/students-guide-to-raft/&quot;&gt;https://thesquareplanet.com/blog/students-guide-to-raft/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 15:19:31 +0800</pubDate>
        <link>http://masutangu.com/2017/12/mit-6824-note-2/</link>
        <guid isPermaLink="true">http://masutangu.com/2017/12/mit-6824-note-2/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
  </channel>
</rss>
