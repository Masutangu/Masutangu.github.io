<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Masutangu</title>
    <description>也許我這一生　始終在追逐那顆九號球</description>
    <link>http://masutangu.com/</link>
    <atom:link href="http://masutangu.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 28 Sep 2018 22:11:57 +0800</pubDate>
    <lastBuildDate>Fri, 28 Sep 2018 22:11:57 +0800</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>Designing Data-Intensive Applications 读书笔记（三）</title>
        <description>&lt;h1&gt;Chapter 6. Partitioning&lt;/h1&gt;

&lt;p&gt;For very large datasets, or very high query throughput, that is not sufficient: we need to break the data up into &lt;strong&gt;partitions&lt;/strong&gt;, also known as &lt;strong&gt;sharding&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The main reason for wanting to partition data is &lt;strong&gt;scalability&lt;/strong&gt;. Different partitions can be placed on different nodes in a shared-nothing cluster.&lt;/p&gt;

&lt;p&gt;In this chapter we will first look at different approaches for partitioning large datasets and observe &lt;strong&gt;how the indexing of data interacts with partitioning&lt;/strong&gt;. We’ll then talk about &lt;strong&gt;rebalancing&lt;/strong&gt;, which is necessary if you want to add or remove nodes in your cluster. Finally, we’ll get an overview of &lt;strong&gt;how databases route requests to the right partitions and execute queries&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Partitioning and Replication&lt;/h2&gt;

&lt;p&gt;Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes. &lt;strong&gt;Even though each record belongs to exactly one partition, it may still be stored on several different nodes for fault tolerance.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A node may store more than one partition. If a leader–follower replication model is used, the combination of partitioning and replication can look like bellow figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;Partitioning of Key-Value Data&lt;/h2&gt;

&lt;p&gt;If the partitioning is unfair, so that some partitions have more data or queries than others, we call it skewed. &lt;/p&gt;

&lt;h3&gt;Partitioning by Key Range&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Within each partition, we can keep keys in sorted order. This has the advantage that range scans are easy, and you can treat the key as a concatenated index in order to fetch several related records in one query.&lt;/p&gt;

&lt;h3&gt;Partitioning by Hash of Key&lt;/h3&gt;

&lt;p&gt;Because of this risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key.&lt;/p&gt;

&lt;p&gt;Once you have a suitable hash function for keys, you can assign each partition a range of hashes (rather than a range of keys), and every key whose hash falls within a partition’s range will be stored in that partition. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately however, by using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries. Keys that were once adjacent are now scattered across all the partitions. In MongoDB, if you have enabled hash-based sharding mode, any range query has to be sent to all partitions.&lt;/p&gt;

&lt;p&gt;Cassandra achieves a compromise between the two partitioning strategies. A table in Cassandra can be declared with a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra’s SSTables. A query therefore cannot search for a range of values within the first column of a compound key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key.&lt;/p&gt;

&lt;h3&gt;Skewed Workloads and Relieving Hot Spots&lt;/h3&gt;

&lt;p&gt;As discussed, hashing a key to determine its partition can help reduce hot spots. However, it can’t avoid them entirely: in the extreme case where all reads and writes are for the same key, you still end up with all requests being routed to the same partition.&lt;/p&gt;

&lt;p&gt;Today, most data systems are not able to automatically compensate for such a highly skewed workload, so it’s the responsibility of the application to reduce the skew. For example, if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. However, having split the writes across different keys, any reads now have to do additional work, as they have to read the data from all 100 keys and combine it.&lt;/p&gt;

&lt;h2&gt;Partitioning and Secondary Indexes&lt;/h2&gt;

&lt;p&gt;If records are only ever accessed via their primary key, we can determine the partition from that key and use it to route read and write requests to the partition responsible for that key.&lt;/p&gt;

&lt;p&gt;The situation becomes more complicated if secondary indexes are involved. &lt;strong&gt;The problem with secondary indexes is that they don’t map neatly to partitions.&lt;/strong&gt; There are two main approaches to partitioning a database with secondary indexes: &lt;strong&gt;document-based partitioning&lt;/strong&gt; and &lt;strong&gt;term-based partitioning&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Partitioning Secondary Indexes by Document&lt;/h3&gt;

&lt;p&gt;Imagine you are operating a website for selling used cars. Each listing has a unique ID—call it the document ID—and you partition the database by the document ID.&lt;/p&gt;

&lt;p&gt;You want to let users search for cars, allowing them to filter by color and by make, so you need a secondary index on color and make. If you have declared the index, the database can perform the indexing automatically. For example, whenever a red car is added to the database, the database partition automatically adds it to the list of document IDs for the index entry color:red.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In this indexing approach, each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition. A document-partitioned index is also known as a &lt;strong&gt;local index&lt;/strong&gt; (as opposed to a &lt;strong&gt;global index&lt;/strong&gt;, described in the next section).&lt;/p&gt;

&lt;p&gt;If you want to search for red cars, you need to send the query to all partitions, and combine all the results you get back.&lt;/p&gt;

&lt;p&gt;This approach to querying a partitioned database is sometimes known as &lt;strong&gt;scatter/ gather&lt;/strong&gt;, and it can make read queries on secondary indexes quite expensive. Even if you query the partitions in parallel, scatter/gather is prone to &lt;strong&gt;tail latency amplification&lt;/strong&gt;(多个并行请求，请求时延由最慢的请求决定).&lt;/p&gt;

&lt;h3&gt;Partitioning Secondary Indexes by Term&lt;/h3&gt;

&lt;p&gt;Rather than each partition having its own secondary index (a local index), we can construct a global index that covers data in all partitions. A global index must also be partitioned, but it can be partitioned differently from the primary key index.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-5.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;We call this kind of index term-partitioned, because the term we’re looking for determines the partition of the index. &lt;/p&gt;

&lt;p&gt;The advantage of a global (term-partitioned) index over a document-partitioned index is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. However, the downside of a global index is that writes are slower and more complicated, because a write to a single document may now affect multiple partitions of the index (every term in the document might be on a different partition, on a different node).&lt;/p&gt;

&lt;p&gt;In practice, updates to global secondary indexes are often asynchronous.&lt;/p&gt;

&lt;h2&gt;Rebalancing Partitions&lt;/h2&gt;

&lt;p&gt;The process of moving load from one node in the cluster to another is called rebalancing.&lt;/p&gt;

&lt;p&gt;No matter which partitioning scheme is used, rebalancing is usually expected to meet some minimum requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After rebalancing, the load (data storage, read and write requests) should be shared fairly between the nodes in the cluster.&lt;/li&gt;
&lt;li&gt;While rebalancing is happening, the database should continue accepting reads and writes.&lt;/li&gt;
&lt;li&gt;No more data than necessary should be moved between nodes, to make rebalanc‐ ing fast and to minimize the network and disk I/O load.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Strategies for Rebalancing&lt;/h3&gt;

&lt;h4&gt;How not to do it: hash mod N&lt;/h4&gt;

&lt;p&gt;The problem with the mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another. &lt;/p&gt;

&lt;h4&gt;Fixed number of partitions&lt;/h4&gt;

&lt;p&gt;Fortunately, there is a fairly simple solution: create many more partitions than there are nodes, and assign several partitions to each node. &lt;/p&gt;

&lt;p&gt;Now, if a node is added to the cluster, the new node can steal a few partitions from every existing node until partitions are fairly distributed once again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-6.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Only entire partitions are moved between nodes. The number of partitions does not change, nor does the assignment of keys to partitions. The only thing that changes is the assignment of partitions to nodes. This change of assignment is not immediate-it takes some time to transfer a large amount of data over the network—so the old assignment of partitions is used for any reads and writes that happen while the transfer is in progress.&lt;/p&gt;

&lt;p&gt;Choosing the right number of partitions is difficult if the total size of the dataset is highly variable.&lt;/p&gt;

&lt;h4&gt;Dynamic partitioning&lt;/h4&gt;

&lt;p&gt;key range–partitioned databases such as HBase and RethinkDB create partitions dynamically. When a partition grows to exceed a configured size, it is split into two partitions so that approximately half of the data ends up on each side of the split. Conversely, if lots of data is deleted and a partition shrinks below some threshold, it can be merged with an adjacent partition. &lt;/p&gt;

&lt;h4&gt;Partitioning proportionally to nodes&lt;/h4&gt;

&lt;p&gt;With dynamic partitioning, the number of partitions is proportional to the size of the dataset, since the splitting and merging processes keep the size of each partition between some fixed minimum and maximum. On the other hand, with a fixed number of partitions, the size of each partition is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes.&lt;/p&gt;

&lt;p&gt;A third option, used by Cassandra and Ketama, is to make the number of partitions proportional to the number of nodes.&lt;/p&gt;

&lt;p&gt;When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. &lt;/p&gt;

&lt;h3&gt;Operations: Automatic or Manual Rebalancing&lt;/h3&gt;

&lt;p&gt;Fully automated rebalancing can be convenient, however, it can be unpredictable. Rebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. If it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.&lt;/p&gt;

&lt;h2&gt;Request Routing&lt;/h2&gt;

&lt;p&gt;On a high level, there are a few different approaches to this problem:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Allow clients to contact any node (e.g., via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Require that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-7.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Many distributed data systems rely on a separate coordination service such as Zoo‐ Keeper to keep track of this cluster metadata:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-8.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.&lt;/p&gt;

&lt;p&gt;Cassandra and Riak take a different approach: they use a gossip protocol among the nodes to disseminate any changes in cluster state. Requests can be sent to any node, and that node forwards them to the appropriate node for the requested partition.&lt;/p&gt;

&lt;p&gt;When using a routing tier or when sending requests to a random node, clients still need to find the IP addresses to connect to. These are not as fast-changing as the assignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.&lt;/p&gt;

&lt;h1&gt;Chapter 7. Transactions&lt;/h1&gt;

&lt;p&gt;A transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback). If it fails, the application can safely retry. With transactions, error handling becomes much simpler for an application, because it doesn’t need to worry about partial failure.&lt;/p&gt;

&lt;h2&gt;The Slippery Concept of a Transaction&lt;/h2&gt;

&lt;h3&gt;The Meaning of ACID&lt;/h3&gt;

&lt;p&gt;The safety guarantees provided by transactions are often described by the well-known acronym ACID, which stands for Atomicity, Consistency, Isolation, and Durability. &lt;/p&gt;

&lt;h4&gt;Atomicity&lt;/h4&gt;

&lt;p&gt;In general, atomic refers to something that cannot be broken down into smaller parts. ACID atomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. If the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.&lt;/p&gt;

&lt;p&gt;The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. &lt;/p&gt;

&lt;h4&gt;Consistency&lt;/h4&gt;

&lt;p&gt;The idea of ACID consistency is that you have certain statements about your data (invariants) that must always be true--for example, in an accounting system, credits and debits across all accounts must always be balanced.&lt;/p&gt;

&lt;p&gt;Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID sense) is a property of the application. The application may rely on the database’s atomicity and isolation properties in order to achieve consistency, but it’s not up to the database alone. Thus, the letter C doesn’t really belong in ACID.&lt;/p&gt;

&lt;h4&gt;Isolation&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-9.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Isolation in the sense of ACID means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes. The classic database textbooks formalize isolation as &lt;strong&gt;serializability&lt;/strong&gt;, which means that each transaction can pretend that it is the only transaction running on the entire database. The database ensures that when the transactions have committed, the result is the same as if they had run serially (one after another), even though in reality they may have run concurrently.&lt;/p&gt;

&lt;p&gt;However, in practice, serializable isolation is rarely used, because it carries a performance penalty.&lt;/p&gt;

&lt;h4&gt;Durability&lt;/h4&gt;

&lt;p&gt;Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.&lt;/p&gt;

&lt;h3&gt;Single-Object and Multi-Object Operations&lt;/h3&gt;

&lt;p&gt;In ACID, atomicity and isolation describe what the database should do if a client makes several writes within the same transaction:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Atomicity
If an error occurs halfway through a sequence of writes, the transaction should be aborted, and the writes made up to that point should be discarded. In other words, the database saves you from having to worry about partial failure, by giving an all-or-nothing guarantee.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Isolation
Concurrently running transactions shouldn’t interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-10.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;User 2 experiences an anomaly: the mailbox listing shows an unread message, but the counter shows zero unread messages because the counter increment has not yet happened.ii Isolation would have prevented this issue by ensuring that user 2 sees either both the inserted email and the updated counter, or neither, but not an inconsistent halfway point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-11.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;If an error occurs somewhere over the course of the transaction, the contents of the mailbox and the unread counter might become out of sync. In an atomic transaction, if the update to the counter fails, the transaction is aborted and the inserted email is rolled back.&lt;/p&gt;

&lt;p&gt;Multi-object transactions require some way of determining which read and write operations belong to the same transaction. In relational databases, that is typically done based on the client’s TCP connection to the database server: on any particular connection, &lt;strong&gt;everything between a BEGIN TRANSACTION and a COMMIT statement is considered to be part of the same transaction&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;Single-object writes&lt;/h4&gt;

&lt;p&gt;Atomicity and isolation also apply when a single object is being changed. For exam‐ ple, imagine you are writing a 20 KB JSON document to a database:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the network connection is interrupted after the first 10 KB have been sent, does the database store that unparseable 10 KB fragment of JSON?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the power fails while the database is in the middle of overwriting the previous value on disk, do you end up with the old and new values spliced together?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If another client reads that document while the write is in progress, will it see a partially updated value?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Atomicity can be implemented using a log for crash recovery and isolation can be implemented using a lock on each object (allowing only one thread to access an object at any one time).&lt;/p&gt;

&lt;p&gt;Some databases also provide more complex atomic operations, such as an increment operation, which removes the need for a read-modify-write cycle. Similarly popular is a compare-and-set operation, which allows a write to happen only if the value has not been concurrently changed by someone else.&lt;/p&gt;

&lt;p&gt;These single-object operations are useful, as they can prevent lost updates when several clients try to write to the same object concurrently.&lt;/p&gt;

&lt;h4&gt;Handling errors and aborts&lt;/h4&gt;

&lt;p&gt;ACID databases are based on this philosophy: if the database is in danger of violating its guarantee of atomicity, isolation, or durability, it would rather aban‐ don the transaction entirely than allow it to remain half-finished.&lt;/p&gt;

&lt;p&gt;Not all systems follow that philosophy, though. In particular, datastores with leader‐less replication work much more on a “best effort” basis, which could be summarized as “the database will do as much as it can, and if it runs into an error, it won’t undo something it has already done”—so it’s the application’s responsibility to recover from errors.&lt;/p&gt;

&lt;p&gt;Although retrying an aborted transaction is a simple and effective error handling mechanism, it isn’t perfect:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the transaction actually succeeded, but the network failed while the server tried to acknowledge the successful commit to the client (so the client thinks it failed), then retrying the transaction causes it to be performed twice—unless you have an additional application-level deduplication mechanism in place.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the error is due to overload, retrying the transaction will make the problem worse, not better. To avoid such feedback cycles, you can limit the number of retries, use exponential backoff, and handle overload-related errors differently from other errors (if possible).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is only worth retrying after transient errors (for example due to deadlock, isolation violation, temporary network interruptions, and failover); after a permanent error (e.g., constraint violation) a retry would be pointless.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the transaction also has side effects outside of the database, those side effects may happen even if the transaction is aborted. For example, if you’re sending an email, you wouldn’t want to send the email again every time you retry the transaction. If you want to make sure that several different systems either commit or abort together, two-phase commit can help.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the client process fails while retrying, any data it was trying to write to the database is lost.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Weak Isolation Levels&lt;/h2&gt;

&lt;p&gt;Databases have long tried to hide concurrency issues from application developers by providing &lt;strong&gt;transaction isolation&lt;/strong&gt;. &lt;strong&gt;Serializable isolation&lt;/strong&gt; means that the database guarantees that transactions have the same effect as if they ran serially.&lt;/p&gt;

&lt;p&gt;Serializable isolation has a performance cost, and many databases don’t want to pay that price. It’s therefore common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all.&lt;/p&gt;

&lt;h3&gt;Read Committed&lt;/h3&gt;

&lt;p&gt;The most basic level of transaction isolation is read committed. It makes two guarantees:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When reading from the database, you will only see data that has been committed (no dirty reads).&lt;/li&gt;
&lt;li&gt;When writing to the database, you will only overwrite data that has been committed (no dirty writes).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;No dirty reads&lt;/h4&gt;

&lt;p&gt;Imagine a transaction has written some data to the database, but the transaction has not yet committed or aborted. Can another transaction see that uncommitted data? If yes, that is called a &lt;strong&gt;dirty read&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;No dirty writes&lt;/h4&gt;

&lt;p&gt;If the earlier write is part of a transaction that has not yet committed, so the later write overwrites an uncommitted value? This is called a dirty write.&lt;/p&gt;

&lt;p&gt;Transactions running at the read committed isolation level must prevent dirty writes, usually by delaying the second write until the first write’s transaction has committed or aborted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-12.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;However, read committed does not prevent the race condition between two counter increments.&lt;/p&gt;

&lt;h4&gt;Implementing read committed&lt;/h4&gt;

&lt;p&gt;Most commonly, databases prevent dirty writes by using &lt;strong&gt;row-level locks&lt;/strong&gt;: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object. &lt;strong&gt;It must then hold that lock until the transaction is committed or aborted.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;How do we prevent dirty reads? One option would be to use the same lock, and to require any transaction that wants to read an object to briefly acquire the lock and then release it again immediately after reading. This would ensure that a read couldn’t happen while an object has a dirty, uncommitted value.&lt;/p&gt;

&lt;p&gt;However, the approach of requiring read locks does not work well in practice, because one long-running write transaction can force many read-only transactions to wait until the long-running transaction has completed.&lt;/p&gt;

&lt;p&gt;For that reason, most databasesvi prevent dirty reads using the following approach: for every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.&lt;/p&gt;

&lt;h3&gt;Snapshot Isolation and Repeatable Read&lt;/h3&gt;

&lt;p&gt;Figure below illustrates a problem that can occur with read committed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-13.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;This anomaly is called a &lt;strong&gt;nonrepeatable read&lt;/strong&gt; or &lt;strong&gt;read skew&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Some situations cannot tolerate such temporary inconsistency:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Backups
Taking a backup requires making a copy of the entire database, which may take hours on a large database. During the time that the backup process is running, writes will continue to be made to the database. Thus, you could end up with some parts of the backup &lt;strong&gt;containing an older version of the data, and other parts containing a newer version&lt;/strong&gt;. If you need to restore from such a backup, the inconsistencies (such as disappearing money) become permanent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analytic queries and integrity checks
Sometimes, you may want to run a query that scans over large parts of the database. Such queries are common in analytics, or may be part of a periodic integrity check that everything is in order (monitoring for data corruption). These queries are likely to return nonsensical results if they observe parts of the database at different points in time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Snapshot isolation is the most common solution to this problem. The idea is that each transaction reads from a consistent snapshot of the database—that is, the transaction sees all the data that was committed in the database at the start of the transaction. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.&lt;/p&gt;

&lt;h4&gt;Implementing snapshot isolation&lt;/h4&gt;

&lt;p&gt;Like read committed isolation, implementations of snapshot isolation typically use write locks to prevent dirty writes.&lt;/p&gt;

&lt;p&gt;From a performance point of view, a key principle of snapshot isolation is &lt;strong&gt;readers never block writers, and writers never block readers&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as &lt;strong&gt;multi-version concurrency control (MVCC)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If a database only needed to provide read committed isolation, but not snapshot isolation, it would be sufficient to keep two versions of an object: the committed version and the overwritten-but-not-yet-committed version. However, storage engines that support snapshot isolation typically use MVCC for their read committed isolation level as well. &lt;/p&gt;

&lt;p&gt;Following figure illustrates how MVCC-based snapshot isolation is implemented in PostgreSQL(other implementations are similar). When a transaction is started, it is given a unique, always-increasing transaction ID (txid). Whenever a transaction writes anything to the database, the data it writes is tagged with the transaction ID of the writer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-14.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Each row in a table has a &lt;strong&gt;created_by&lt;/strong&gt; field, containing the ID of the transaction that inserted this row into the table. Moreover, each row has a &lt;strong&gt;deleted_by&lt;/strong&gt; field, which is initially empty. If a transaction deletes a row, the row isn’t actually deleted from the database, but it is marked for deletion by setting the deleted_by field to the ID of the transaction that requested the deletion. At some later time, when it is certain that no transaction can any longer access the deleted data, a garbage collection process in the database removes any rows marked for deletion and frees their space.&lt;/p&gt;

&lt;h4&gt;Visibility rules for observing a consistent snapshot&lt;/h4&gt;

&lt;p&gt;By carefully defining visibility rules, the database can present a consistent snapshot of the database to the application. This works as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;At the start of each transaction, the database makes a list of all the other transactions that are in progress (not yet committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Any writes made by aborted transactions are ignored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Any writes made by transactions with a later transaction ID (i.e., which started after the current transaction started) are ignored, regardless of whether those transactions have committed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All other writes are visible to the application’s queries.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Put another way, an object is visible if both of the following conditions are true:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;At the time when the reader’s transaction started, the transaction that created the object had already committed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader’s transaction started.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By never updating values in place but instead creating a new version every time a value is changed, the database can provide a consistent snapshot while incurring only a small overhead.&lt;/p&gt;

&lt;h4&gt;Indexes and snapshot isolation&lt;/h4&gt;

&lt;p&gt;How do indexes work in a multi-version database? One option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction.&lt;/p&gt;

&lt;p&gt;PostgreSQL has optimizations for avoid‐ ing index updates if different versions of the same object can fit on the same page.&lt;/p&gt;

&lt;p&gt;Another approach is used in CouchDB, Datomic, and LMDB. Although they also use B-trees, they use an append-only/copy-on-write variant that does not overwrite pages of the tree when they are updated, but instead creates a new copy of each modified page. Parent pages, up to the root of the tree, are copied and updated to point to the new versions of their child pages. Any pages that are not affected by a write do not need to be copied, and remain immutable.&lt;/p&gt;

&lt;p&gt;With append-only B-trees, every write transaction (or batch of transactions) creates a new B-tree root, and a particular root is a consistent snapshot of the database at the point in time when it was created. There is no need to filter out objects based on transaction IDs because subsequent writes cannot modify an existing B-tree; they can only create new tree roots. However, this approach also requires a background process for compaction and garbage collection.&lt;/p&gt;

&lt;h4&gt;Repeatable read and naming confusion&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Snapshot isolation&lt;/strong&gt; is a useful isolation level, especially for read-only transactions. However, many databases that implement it call it by different names. In Oracle it is called &lt;strong&gt;serializable&lt;/strong&gt;, and in PostgreSQL and MySQL it is called &lt;strong&gt;repeatable read&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Preventing Lost Updates&lt;/h3&gt;

&lt;p&gt;The lost update problem can occur if an application reads some value from the data‐ base, modifies it, and writes back the modified value (a read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.&lt;/p&gt;

&lt;h4&gt;Atomic write operations&lt;/h4&gt;

&lt;p&gt;Many databases provide atomic update operations, which remove the need to implement read-modify-write cycles in application code.&lt;/p&gt;

&lt;p&gt;Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. This technique is sometimes known as &lt;strong&gt;cursor stability&lt;/strong&gt;. Another option is to simply force all atomic operations to be executed on a single thread.&lt;/p&gt;

&lt;h4&gt;Explicit locking&lt;/h4&gt;

&lt;p&gt;Another option for preventing lost updates, is for the application to explicitly lock objects that are going to be updated.&lt;/p&gt;

&lt;h4&gt;Automatically detecting lost updates&lt;/h4&gt;

&lt;p&gt;An alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.&lt;/p&gt;

&lt;p&gt;An advantage of this approach is that databases can perform this check efficiently in conjunction with snapshot isolation. &lt;/p&gt;

&lt;h4&gt;Compare-and-set&lt;/h4&gt;

&lt;p&gt;The purpose of this operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it.&lt;/p&gt;

&lt;p&gt;However, &lt;strong&gt;if the database allows the WHERE clause to read from an old snapshot, this statement may not prevent lost updates, because the condition may be true even though another concurrent write is occurring&lt;/strong&gt;. Check whether your database’s compare-and-set operation is safe before relying on it.&lt;/p&gt;

&lt;h4&gt;Conflict resolution and replication&lt;/h4&gt;

&lt;p&gt;In replicated databases, preventing lost updates takes on another dimension: since they have copies of the data on multiple nodes, and the data can potentially be modified concurrently on different nodes, some additional steps need to be taken to prevent lost updates.&lt;/p&gt;

&lt;p&gt;That is the idea behind Riak 2.0 datatypes, which prevent lost updates across replicas. When a value is concurrently updated by different clients, Riak automatically merges together the updates in such a way that no updates are lost.&lt;/p&gt;

&lt;p&gt;On the other hand, the last write wins (LWW) conflict resolution method is prone to lost updates, as discussed in “Last write wins (discarding concurrent writes)”. Unfortunately, LWW is the default in many replicated databases.&lt;/p&gt;

&lt;h3&gt;Write Skew and Phantoms&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-15.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h4&gt;More examples of write skew&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-17.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, snapshot isolation does not prevent another user from concurrently inserting a conflicting meeting. In order to guarantee you won’t get scheduling conflicts, you once again need serializable isolation.&lt;/p&gt;

&lt;h4&gt;Characterizing write skew&lt;/h4&gt;

&lt;p&gt;This anomaly is called &lt;strong&gt;write skew&lt;/strong&gt;. It is neither a dirty write nor a lost update, because the two transactions are updating two different objects.&lt;/p&gt;

&lt;p&gt;With write skew, our options are more restricted:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Atomic single-object operations don’t help, as multiple objects are involved.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The automatic detection of lost updates that you find in some implementations of snapshot isolation unfortunately doesn’t help either: write skew is not automatically detected in PostgreSQL’s repeatable read, MySQL/InnoDB’s repeatable read, Oracle’s serializable, or SQL Server’s snapshot isolation level. &lt;strong&gt;Auto‐matically preventing write skew requires true serializable isolation.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to specify that at least one doctor must be on call, you would need a constraint that involves multiple objects. Most databases do not have built-in support for such constraints, but you may be able to implement them with triggers or materialized views, depending on the database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you can’t use a serializable isolation level, the second-best option in this case is probably to explicitly lock the rows that the transaction depends on. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Phantoms causing write skew&lt;/h4&gt;

&lt;p&gt;All of these examples follow a similar pattern:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A SELECT query checks whether some requirement is satisfied by searching for rows that match some search condition (there are at least two doctors on call, there are no existing bookings for that room at that time, the position on the board doesn’t already have another figure on it, the username isn’t already taken, there is still money in the account).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Depending on the result of the first query, the application code decides how to continue (perhaps to go ahead with the operation, or perhaps to report an error to the user and abort).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the application decides to go ahead, it makes a write (INSERT, UPDATE, or DELETE) to the database and commits the transaction.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the case of the doctor on call example, the row being modified in step 3 was one of the rows returned in step 1, so we could make the transaction safe and avoid write skew by locking the rows in step 1 (SELECT FOR UPDATE). However, the other four examples are different: &lt;strong&gt;they check for the absence of rows matching some search condition, and the write adds a row matching the same condition. If the query in step 1 doesn’t return any rows, SELECT FOR UPDATE can’t attach locks to anything.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This effect, where a write in one transaction changes the result of a search query in another transaction, is called a &lt;strong&gt;phantom&lt;/strong&gt;. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.&lt;/p&gt;

&lt;h4&gt;Materializing conflicts&lt;/h4&gt;

&lt;p&gt;If the problem of phantoms is that there is no object to which we can attach the locks, perhaps we can artificially introduce a lock object into the database?&lt;/p&gt;

&lt;p&gt;For example, in the meeting room booking case you could imagine creating a table of time slots and rooms. Each row in this table corresponds to a particular room for a particular time period. Now a transaction that wants to create a booking can lock (SELECT FOR UPDATE) the rows in the table that correspond to the desired room and time period. After it has acquired the locks, it can check for overlapping bookings and insert a new booking as before. &lt;/p&gt;

&lt;p&gt;This approach is called materializing conflicts, because it takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database. Unfortunately, it can be hard and error-prone to figure out how to materialize conflicts, and it’s ugly to let a concurrency control mechanism leak into the application data model. For those reasons, materializing conflicts should be considered a last resort if no alternative is possible. A serializable isolation level is much preferable in most cases.&lt;/p&gt;

&lt;h2&gt;Serializability&lt;/h2&gt;

&lt;p&gt;It’s a sad situation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Isolation levels are hard to understand, and inconsistently implemented in different databases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you look at your application code, it’s difficult to tell whether it is safe to run at a particular isolation level.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are no good tools to help us detect race conditions. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, &lt;strong&gt;serially&lt;/strong&gt;, without any concurrency.&lt;/p&gt;

&lt;p&gt;Most databases that provide serializability today use one of three techniques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Literally executing transactions in a serial order&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two-phase locking, which for several decades was the only viable option&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Optimistic concurrency control techniques such as serializable snapshot isolation&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Actual Serial Execution&lt;/h3&gt;

&lt;p&gt;The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.&lt;/p&gt;

&lt;h4&gt;Encapsulating transactions in stored procedures&lt;/h4&gt;

&lt;p&gt;In this interactive style of transaction, a lot of time is spent in network communication between the application and the database. If you were to disallow concurrency in the database and only process one transaction at a time, the throughput would be dreadful because the database would spend most of its time waiting for the applica‐ tion to issue the next query for the current transaction. &lt;/p&gt;

&lt;p&gt;For this reason, systems with single-threaded serial transaction processing don’t allow interactive multi-statement transactions. Instead, the application must submit the entire transaction code to the database ahead of time, as a &lt;strong&gt;stored procedure&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-16.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h4&gt;Partitioning&lt;/h4&gt;

&lt;p&gt;If you can find a way of partitioning your dataset so that each transaction only needs to read and write data within a single partition, then each partition can have its own transaction processing thread running independently from the others.&lt;/p&gt;

&lt;p&gt;However, for any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions that it touches. The stored procedure needs to be performed in lock-step across all partitions to ensure serializability across the whole system.&lt;/p&gt;

&lt;p&gt;Whether transactions can be single-partition depends very much on the structure of the data used by the application. Simple key-value data can often be partitioned very easily, but data with multiple secondary indexes is likely to require a lot of cross-partition coordination.&lt;/p&gt;

&lt;h3&gt;Two-Phase Locking (2PL)&lt;/h3&gt;

&lt;p&gt;Several transactions are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as anyone wants to write (modify or delete) an object, exclusive access is required:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue. (This ensures that B can’t change the object unexpectedly behind A’s back.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue. (Reading an old version of the object, is not acceptable under 2PL.)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In 2PL, writers don’t just block other writers; they also block readers and vice versa. Snapshot isolation has the mantra &lt;strong&gt;readers never block writers, and writers never block readers&lt;/strong&gt;, which captures this key difference between snapshot isolation and two-phase locking.&lt;/p&gt;

&lt;p&gt;On the other hand, because 2PL provides serializability, it protects against all the race conditions discussed earlier, including lost updates and write skew.&lt;/p&gt;

&lt;h4&gt;Implementation of two-phase locking&lt;/h4&gt;

&lt;p&gt;2PL is used by the serializable isolation level in MySQL (InnoDB) and SQL Server, and the repeatable read isolation level in DB2.&lt;/p&gt;

&lt;p&gt;The blocking of readers and writers is implemented by a having a lock on each object in the database. The lock can either be in shared mode or in exclusive mode. The lock is used as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If a transaction wants to read an object, it must first acquire the lock in &lt;strong&gt;shared mode&lt;/strong&gt;. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, these transactions must wait.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a transaction wants to write to an object, it must first acquire the lock in &lt;strong&gt;exclusive mode&lt;/strong&gt;. No other transaction may hold the lock at the same time (either in shared or in exclusive mode), so if there is any existing lock on the object, the transaction must wait.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a transaction first reads and then writes an object, it may &lt;strong&gt;upgrade its shared lock to an exclusive lock&lt;/strong&gt;. The upgrade works the same as getting an exclusive lock directly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: &lt;strong&gt;the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since so many locks are in use, it can happen quite easily that transaction A is stuck waiting for transaction B to release its lock, and vice versa. This situation is called deadlock. The database automatically detects deadlocks between transactions and aborts one of them so that the others can make progress. The aborted transaction needs to be retried by the application.&lt;/p&gt;

&lt;h4&gt;Performance of two-phase locking&lt;/h4&gt;

&lt;p&gt;The big downside of two-phase locking, and the reason why it hasn’t been used by everybody since the 1970s, is performance: transaction throughput and response times of queries are significantly worse under two-phase locking than under weak isolation.&lt;/p&gt;

&lt;h4&gt;Predicate locks&lt;/h4&gt;

&lt;p&gt;We discussed the problem of phantoms—that is, one transaction changing the results of another transaction’s search query. A database with serializable isolation must prevent phantoms.&lt;/p&gt;

&lt;p&gt;In the meeting room booking example this means that if one transaction has searched for existing bookings for a room within a certain time window, another transaction is not allowed to concurrently insert or update another booking for the same room and time range. &lt;/p&gt;

&lt;p&gt;How do we implement this? Conceptually, we need a &lt;strong&gt;predicate lock&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A predicate lock restricts access as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If transaction A wants to read objects matching some condition, like in that SELECT query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If transaction A wants to insert, update, or delete any object, &lt;strong&gt;it must first check whether either the old or the new value matches any existing predicate lock&lt;/strong&gt;. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Index-range locks&lt;/h4&gt;

&lt;p&gt;Unfortunately, predicate locks do not perform well: if there are many locks by active transactions, checking for matching locks becomes time-consuming. For that reason, most databases with 2PL actually implement &lt;strong&gt;index-range locking&lt;/strong&gt;(also known as &lt;strong&gt;next-key locking&lt;/strong&gt;), which is a simplified approximation of predicate locking.&lt;/p&gt;

&lt;p&gt;It’s safe to simplify a predicate by making it match a greater set of objects.&lt;/p&gt;

&lt;p&gt;In the room bookings database you would probably have an index on the room_id column, and/or indexes on start_time and end_time:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Say your index is on room_id, and the database uses this index to find existing bookings for room 123. &lt;strong&gt;Now the database can simply attach a shared lock to this index entry&lt;/strong&gt;, indicating that a transaction has searched for bookings of room 123.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Alternatively, if the database uses a time-based index to find existing bookings, it can attach a shared lock to a range of values in that index, indicating that a transaction has searched for bookings that overlap with the time period of noon to 1 p.m. on January 1, 2018.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Either way, an approximation of the search condition is attached to one of the indexes. Now, if another transaction wants to insert, update, or delete a booking for the same room and/or an overlapping time period, it will have to update the same part of the index. In the process of doing so, it will encounter the shared lock, and it will be forced to wait until the lock is released.&lt;/p&gt;

&lt;p&gt;Index-range locks are not as precise as predicate locks would be (they may lock a bigger range of objects than is strictly necessary to maintain serializability), but since they have much lower overheads, they are a good compromise.&lt;/p&gt;

&lt;h3&gt;Serializable Snapshot Isolation (SSI)&lt;/h3&gt;

&lt;p&gt;An algorithm called serializable snapshot isolation (SSI) is very promising. It provides full serializability, but has only a small performance penalty compared to snapshot isolation.&lt;/p&gt;

&lt;p&gt;As SSI is so young compared to other concurrency control mechanisms, it is still proving its performance in practice, but it has the possibility of being fast enough to become the new default in the future.&lt;/p&gt;

&lt;h4&gt;Pessimistic versus optimistic concurrency control&lt;/h4&gt;

&lt;p&gt;Two-phase locking is a so-called &lt;strong&gt;pessimistic concurrency control mechanism&lt;/strong&gt;: it is based on the principle that if anything might possibly go wrong (as indicated by a lock held by another transaction), it’s better to wait until the situation is safe again before doing anything. It is like mutual exclusion, which is used to protect data structures in multi-threaded programming.&lt;/p&gt;

&lt;p&gt;By contrast, serializable snapshot isolation is an &lt;strong&gt;optimistic concurrency control technique&lt;/strong&gt;. Optimistic in this context means that instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. &lt;/p&gt;

&lt;p&gt;When a transaction wants to commit, the database checks whether anything bad happened (i.e., whether isolation was violated); if so, the transaction is aborted and has to be retried. Only transactions that executed serializably are allowed to commit.&lt;/p&gt;

&lt;p&gt;As the name suggests, SSI is based on snapshot isolation—that is, all reads within a transaction are made from a consistent snapshot of the database. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.&lt;/p&gt;

&lt;h4&gt;Decisions based on an outdated premise&lt;/h4&gt;

&lt;p&gt;Under snapshot isolation, the result from the original query may no longer be up-to-date by the time the transaction commits, because the data may have been modified in the meantime.&lt;/p&gt;

&lt;p&gt;Put another way, the transaction is taking an action based on a premise (a fact that was true at the beginning of the transaction, e.g., “There are currently two doctors on call”). Later, when the transaction wants to commit, the original data may have changed—the premise may no longer be true.&lt;/p&gt;

&lt;p&gt;How does the database know if a query result might have changed? There are two cases to consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)&lt;/li&gt;
&lt;li&gt;Detecting writes that affect prior reads (the write occurs after the read)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Detecting stale MVCC reads&lt;/h4&gt;

&lt;p&gt;In Figure, transaction 43 sees Alice as having on_call = true, because transaction 42 (which modified Alice’s on-call status) is uncommitted. However, by the time transaction 43 wants to commit, transaction 42 has already committed. This means that the write that was ignored when reading from the consistent snapshot has now taken effect, and transaction 43’s premise is no longer true.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-18.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In order to prevent this anomaly, the database needs to track when a transaction ignores another transaction’s writes due to MVCC visibility rules. When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why wait until committing? Why not abort transaction 43 immediately when the stale read is detected?&lt;/strong&gt; Well, if transaction 43 was a read-only transaction, it wouldn’t need to be aborted, because there is no risk of write skew. At the time when transaction 43 makes its read, the database doesn’t yet know whether that transaction is going to later perform a write. Moreover, transaction 42 may yet abort or may still be uncommitted at the time when transaction 43 is committed, and so the read may turn out not to have been stale after all. By avoiding unnecessary aborts, SSI preserves snapshot isolation’s support for long-running reads from a consistent snapshot.&lt;/p&gt;

&lt;h4&gt;Detecting writes that affect prior reads&lt;/h4&gt;

&lt;p&gt;The second case to consider is when another transaction modifies data after it has been read. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-3/illustration-19.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Transactions 42 and 43 both search for on-call doctors during shift 1234. If there is an index on shift_id, the database can use the index entry 1234 to record the fact that transactions 42 and 43 read this data.&lt;/p&gt;

&lt;p&gt;When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. This process is similar to acquiring a write lock on the affected key range, but rather than blocking until the readers have committed, the lock acts as a tripwire: it simply notifies the transactions that the data they read may no longer be up to date.&lt;/p&gt;

&lt;h4&gt;Performance of serializable snapshot isolation&lt;/h4&gt;

&lt;p&gt;Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction. Like under snapshot isolation, writers don’t block readers, and vice versa.&lt;/p&gt;

&lt;p&gt;Compared to serial execution, serializable snapshot isolation is not limited to the throughput of a single CPU core.&lt;/p&gt;

&lt;p&gt;The rate of aborts significantly affects the overall performance of SSI. However, SSI is probably less sensitive to slow transactions than two-phase locking or serial execution.&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Sep 2018 22:11:46 +0800</pubDate>
        <link>http://masutangu.com/2018/09/data-intensive-note-3/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/09/data-intensive-note-3/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Core dump 原理探究学习笔记（二）</title>
        <description>&lt;p&gt;本系列文章是读&lt;a href=&quot;https://blog.csdn.net/xuzhina/article/category/1322964&quot;&gt;《coredump问题原理探究》&lt;/a&gt;的读书笔记。&lt;/p&gt;

&lt;h2&gt;函数逆向&lt;/h2&gt;

&lt;p&gt;这是将要逆向的样例源码：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
int main(int argc, char* argv[]) {
  for (int i = 0; i &amp;lt; argc; i++) {
    int len = strlen(argv[i]);
    switch (len) {
    case 0:
      printf(&amp;quot;%c\n&amp;quot;, argv[i][0]);
      break;
    case 1:
      printf(&amp;quot;%s\n&amp;quot;, argv[i+1]);
      break;
    case 2:
      printf(&amp;quot;%d\n&amp;quot;, i);
      break;
    default:
      printf(&amp;quot;%s\n&amp;quot;, i);
      break;
    }
  }  
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;./test helloworld&lt;/code&gt; 运行程序 core 掉。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) bt
#0  0x00007f1128e62694 in vfprintf () from /lib64/libc.so.6
#1  0x00007f1128e6b879 in printf () from /lib64/libc.so.6
#2  0x000000000040074d in main ()
(gdb) disassemble main
Dump of assembler code for function main:
   0x0000000000400680 &amp;lt;+0&amp;gt;:     push   %rbp
   0x0000000000400681 &amp;lt;+1&amp;gt;:     mov    %rsp,%rbp
   0x0000000000400684 &amp;lt;+4&amp;gt;:     sub    $0x20,%rsp
   0x0000000000400688 &amp;lt;+8&amp;gt;:     mov    %edi,-0x14(%rbp)
   0x000000000040068b &amp;lt;+11&amp;gt;:    mov    %rsi,-0x20(%rbp)
   0x000000000040068f &amp;lt;+15&amp;gt;:    movl   $0x0,-0x4(%rbp)
   0x0000000000400696 &amp;lt;+22&amp;gt;:    jmpq   0x400752 &amp;lt;main+210&amp;gt;
   0x000000000040069b &amp;lt;+27&amp;gt;:    mov    -0x4(%rbp),%eax
   0x000000000040069e &amp;lt;+30&amp;gt;:    cltq   
   0x00000000004006a0 &amp;lt;+32&amp;gt;:    lea    0x0(,%rax,8),%rdx
   0x00000000004006a8 &amp;lt;+40&amp;gt;:    mov    -0x20(%rbp),%rax
   0x00000000004006ac &amp;lt;+44&amp;gt;:    add    %rdx,%rax
   0x00000000004006af &amp;lt;+47&amp;gt;:    mov    (%rax),%rax
   0x00000000004006b2 &amp;lt;+50&amp;gt;:    mov    %rax,%rdi
   0x00000000004006b5 &amp;lt;+53&amp;gt;:    callq  0x400580 &amp;lt;strlen@plt&amp;gt;
   0x00000000004006ba &amp;lt;+58&amp;gt;:    mov    %eax,-0x8(%rbp)
   0x00000000004006bd &amp;lt;+61&amp;gt;:    mov    -0x8(%rbp),%eax
   0x00000000004006c0 &amp;lt;+64&amp;gt;:    cmp    $0x1,%eax
   0x00000000004006c3 &amp;lt;+67&amp;gt;:    je     0x4006fe &amp;lt;main+126&amp;gt;
   0x00000000004006c5 &amp;lt;+69&amp;gt;:    cmp    $0x2,%eax
   0x00000000004006c8 &amp;lt;+72&amp;gt;:    je     0x400723 &amp;lt;main+163&amp;gt;
   0x00000000004006ca &amp;lt;+74&amp;gt;:    test   %eax,%eax
   0x00000000004006cc &amp;lt;+76&amp;gt;:    jne    0x400739 &amp;lt;main+185&amp;gt;
   0x00000000004006ce &amp;lt;+78&amp;gt;:    mov    -0x4(%rbp),%eax
   0x00000000004006d1 &amp;lt;+81&amp;gt;:    cltq   
   0x00000000004006d3 &amp;lt;+83&amp;gt;:    lea    0x0(,%rax,8),%rdx
   0x00000000004006db &amp;lt;+91&amp;gt;:    mov    -0x20(%rbp),%rax
   0x00000000004006df &amp;lt;+95&amp;gt;:    add    %rdx,%rax
   0x00000000004006e2 &amp;lt;+98&amp;gt;:    mov    (%rax),%rax
   0x00000000004006e5 &amp;lt;+101&amp;gt;:   movzbl (%rax),%eax
   0x00000000004006e8 &amp;lt;+104&amp;gt;:   movsbl %al,%eax
   0x00000000004006eb &amp;lt;+107&amp;gt;:   mov    %eax,%esi
   0x00000000004006ed &amp;lt;+109&amp;gt;:   mov    $0x400800,%edi
   0x00000000004006f2 &amp;lt;+114&amp;gt;:   mov    $0x0,%eax
   0x00000000004006f7 &amp;lt;+119&amp;gt;:   callq  0x400540 &amp;lt;printf@plt&amp;gt;
   0x00000000004006fc &amp;lt;+124&amp;gt;:   jmp    0x40074e &amp;lt;main+206&amp;gt;
   0x00000000004006fe &amp;lt;+126&amp;gt;:   mov    -0x4(%rbp),%eax
   0x0000000000400701 &amp;lt;+129&amp;gt;:   cltq   
   0x0000000000400703 &amp;lt;+131&amp;gt;:   add    $0x1,%rax
   0x0000000000400707 &amp;lt;+135&amp;gt;:   lea    0x0(,%rax,8),%rdx
   0x000000000040070f &amp;lt;+143&amp;gt;:   mov    -0x20(%rbp),%rax
   0x0000000000400713 &amp;lt;+147&amp;gt;:   add    %rdx,%rax
   0x0000000000400716 &amp;lt;+150&amp;gt;:   mov    (%rax),%rax
   0x0000000000400719 &amp;lt;+153&amp;gt;:   mov    %rax,%rdi
   0x000000000040071c &amp;lt;+156&amp;gt;:   callq  0x400560 &amp;lt;puts@plt&amp;gt;
   0x0000000000400721 &amp;lt;+161&amp;gt;:   jmp    0x40074e &amp;lt;main+206&amp;gt;
   0x0000000000400723 &amp;lt;+163&amp;gt;:   mov    -0x4(%rbp),%eax
   0x0000000000400726 &amp;lt;+166&amp;gt;:   mov    %eax,%esi
   0x0000000000400728 &amp;lt;+168&amp;gt;:   mov    $0x400804,%edi
   0x000000000040072d &amp;lt;+173&amp;gt;:   mov    $0x0,%eax
   0x0000000000400732 &amp;lt;+178&amp;gt;:   callq  0x400540 &amp;lt;printf@plt&amp;gt;
   0x0000000000400737 &amp;lt;+183&amp;gt;:   jmp    0x40074e &amp;lt;main+206&amp;gt;
   0x0000000000400739 &amp;lt;+185&amp;gt;:   mov    -0x4(%rbp),%eax
   0x000000000040073c &amp;lt;+188&amp;gt;:   mov    %eax,%esi
   0x000000000040073e &amp;lt;+190&amp;gt;:   mov    $0x400808,%edi
   0x0000000000400743 &amp;lt;+195&amp;gt;:   mov    $0x0,%eax
   0x0000000000400748 &amp;lt;+200&amp;gt;:   callq  0x400540 &amp;lt;printf@plt&amp;gt;
   0x000000000040074d &amp;lt;+205&amp;gt;:   nop
   0x000000000040074e &amp;lt;+206&amp;gt;:   addl   $0x1,-0x4(%rbp)
   0x0000000000400752 &amp;lt;+210&amp;gt;:   mov    -0x4(%rbp),%eax
   0x0000000000400755 &amp;lt;+213&amp;gt;:   cmp    -0x14(%rbp),%eax
   0x0000000000400758 &amp;lt;+216&amp;gt;:   jl     0x40069b &amp;lt;main+27&amp;gt;
   0x000000000040075e &amp;lt;+222&amp;gt;:   mov    $0x0,%eax
   0x0000000000400763 &amp;lt;+227&amp;gt;:   leaveq 
   0x0000000000400764 &amp;lt;+228&amp;gt;:   retq   
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;由 
&lt;code&gt;0x0000000000400758 &amp;lt;+216&amp;gt;:   jl     0x40069b &amp;lt;main+27&amp;gt;&lt;/code&gt;
推断出 &amp;lt;main+27&amp;gt; 到 &amp;lt;main+216&amp;gt; 构成一个循环。&lt;/p&gt;

&lt;p&gt;看看循环前的判断语句：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x000000000040074e &amp;lt;+206&amp;gt;:   addl   $0x1,-0x4(%rbp)
0x0000000000400752 &amp;lt;+210&amp;gt;:   mov    -0x4(%rbp),%eax
0x0000000000400755 &amp;lt;+213&amp;gt;:   cmp    -0x14(%rbp),%eax
0x0000000000400758 &amp;lt;+216&amp;gt;:   jl     0x40069b &amp;lt;main+27&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;从main 开头的汇编可以看出，edi 寄存器保存了 argc, rsi 寄存器保存了 argv：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x0000000000400688 &amp;lt;+8&amp;gt;:     mov    %edi,-0x14(%rbp)
0x000000000040068b &amp;lt;+11&amp;gt;:    mov    %rsi,-0x20(%rbp)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;因此循环的判断语句是对比 0x4(%rbp) 这个变量和 argc，命名这个变量为 cnt。上面判断的语句语义为：执行 cnt++，如果 cnt 小于 argc 则跳转到 &amp;lt;main+27&amp;gt;。&lt;/p&gt;

&lt;p&gt;由：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x000000000040068f &amp;lt;+15&amp;gt;:    movl   $0x0,-0x4(%rbp)
0x0000000000400696 &amp;lt;+22&amp;gt;:    jmpq   0x400752 &amp;lt;main+210&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看出，程序初始时 cnt 被赋值为 0，然后跳转到循环开始处 &amp;lt;main+210&amp;gt; 与 argc 比较。&lt;/p&gt;

&lt;p&gt;由：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x000000000040075e &amp;lt;+222&amp;gt;:   mov    $0x0,%eax
0x0000000000400763 &amp;lt;+227&amp;gt;:   leaveq 
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;知道 main 函数始终返回 0（eax 寄存器保存函数返回值）。&lt;/p&gt;

&lt;p&gt;因此目前我们推算出的源码架构为：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;int main(int argc, char* argv[]) {
  int cnt = 0;
  for (; cnt &amp;lt; args; cnt++) {
      &amp;lt;main+32&amp;gt; - &amp;lt;main+206&amp;gt;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;继续看看 &amp;lt;main+32&amp;gt; 到 &amp;lt;main+206&amp;gt; 的汇编。重点关注跳转指令：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x00000000004006b5 &amp;lt;+53&amp;gt;:    callq  0x400580 &amp;lt;strlen@plt&amp;gt;
0x00000000004006ba &amp;lt;+58&amp;gt;:    mov    %eax,-0x8(%rbp)
0x00000000004006bd &amp;lt;+61&amp;gt;:    mov    -0x8(%rbp),%eax
0x00000000004006c0 &amp;lt;+64&amp;gt;:    cmp    $0x1,%eax
0x00000000004006c3 &amp;lt;+67&amp;gt;:    je     0x4006fe &amp;lt;main+126&amp;gt;
0x00000000004006c5 &amp;lt;+69&amp;gt;:    cmp    $0x2,%eax
0x00000000004006c8 &amp;lt;+72&amp;gt;:    je     0x400723 &amp;lt;main+163&amp;gt;
0x00000000004006ca &amp;lt;+74&amp;gt;:    test   %eax,%eax
0x00000000004006cc &amp;lt;+76&amp;gt;:    jne    0x400739 &amp;lt;main+185&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;上面的代码根据 eax 寄存器的值来跳转对应的代码，而 eax 保存的是调用 strlen 函数的返回值。看看 strlen 的入参：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x000000000040069b &amp;lt;+27&amp;gt;:    mov    -0x4(%rbp),%eax
0x000000000040069e &amp;lt;+30&amp;gt;:    cltq   
0x00000000004006a0 &amp;lt;+32&amp;gt;:    lea    0x0(,%rax,8),%rdx
0x00000000004006a8 &amp;lt;+40&amp;gt;:    mov    -0x20(%rbp),%rax
0x00000000004006ac &amp;lt;+44&amp;gt;:    add    %rdx,%rax
0x00000000004006af &amp;lt;+47&amp;gt;:    mov    (%rax),%rax
0x00000000004006b2 &amp;lt;+50&amp;gt;:    mov    %rax,%rdi
0x00000000004006b5 &amp;lt;+53&amp;gt;:    callq  0x400580 &amp;lt;strlen@plt&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;由上面知道 -0x4(%rbp) 是变量 cnt，-0x20(%rbp) 保存 rsi 的值也就是 argv，这段汇编表示取 argv[cnt] 的值，并存入寄存器 rdi 当作 strlen 函数的参数。&lt;/p&gt;

&lt;p&gt;此时推算出的 main 函数如下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;int main(int argc, char* argv[]) {
  int cnt = 0;
  for (; cnt &amp;lt; args; cnt++) {
      int i = strlen(argv[cnt]);
      switch (i) {
      case 1:
        &amp;lt;main+126&amp;gt; - &amp;lt;main+156&amp;gt;
        break;
      case 2:
        &amp;lt;main+163&amp;gt; - &amp;lt;main+178&amp;gt;
        break;
      case 0:
        &amp;lt;main+78&amp;gt; - &amp;lt;main+119&amp;gt;
        break;
      default:
        &amp;lt;main+185&amp;gt; - &amp;lt;main+205&amp;gt;
      }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;依次查看每个 case 的汇编。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;case 1：&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x00000000004006fe &amp;lt;+126&amp;gt;:   mov    -0x4(%rbp),%eax
0x0000000000400701 &amp;lt;+129&amp;gt;:   cltq   
0x0000000000400703 &amp;lt;+131&amp;gt;:   add    $0x1,%rax
0x0000000000400707 &amp;lt;+135&amp;gt;:   lea    0x0(,%rax,8),%rdx
0x000000000040070f &amp;lt;+143&amp;gt;:   mov    -0x20(%rbp),%rax
0x0000000000400713 &amp;lt;+147&amp;gt;:   add    %rdx,%rax
0x0000000000400716 &amp;lt;+150&amp;gt;:   mov    (%rax),%rax
0x0000000000400719 &amp;lt;+153&amp;gt;:   mov    %rax,%rdi
0x000000000040071c &amp;lt;+156&amp;gt;:   callq  0x400560 &amp;lt;puts@plt&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;取 argv[cnt + 1] 的值入栈，调用 puts@plt。即：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;puts(argv[cnt + 1]);
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;case 2：&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x0000000000400723 &amp;lt;+163&amp;gt;:   mov    -0x4(%rbp),%eax
0x0000000000400726 &amp;lt;+166&amp;gt;:   mov    %eax,%esi
0x0000000000400728 &amp;lt;+168&amp;gt;:   mov    $0x400804,%edi
0x000000000040072d &amp;lt;+173&amp;gt;:   mov    $0x0,%eax
0x0000000000400732 &amp;lt;+178&amp;gt;:   callq  0x400540 &amp;lt;printf@plt&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;调用 printf 函数，esi 保存参数 cnt，edi 保存参数 $0x400804，gdb 下 $0x400804 指向的内容：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /s 0x400804
0x400804:       &amp;quot;%d\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;即：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;printf(&amp;quot;%d\n&amp;quot;, cnt);
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;case 0：&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x00000000004006ce &amp;lt;+78&amp;gt;:    mov    -0x4(%rbp),%eax
0x00000000004006d1 &amp;lt;+81&amp;gt;:    cltq   
0x00000000004006d3 &amp;lt;+83&amp;gt;:    lea    0x0(,%rax,8),%rdx
0x00000000004006db &amp;lt;+91&amp;gt;:    mov    -0x20(%rbp),%rax
0x00000000004006df &amp;lt;+95&amp;gt;:    add    %rdx,%rax
0x00000000004006e2 &amp;lt;+98&amp;gt;:    mov    (%rax),%rax
0x00000000004006e5 &amp;lt;+101&amp;gt;:   movzbl (%rax),%eax
0x00000000004006e8 &amp;lt;+104&amp;gt;:   movsbl %al,%eax
0x00000000004006eb &amp;lt;+107&amp;gt;:   mov    %eax,%esi
0x00000000004006ed &amp;lt;+109&amp;gt;:   mov    $0x400800,%edi
0x00000000004006f2 &amp;lt;+114&amp;gt;:   mov    $0x0,%eax
0x00000000004006f7 &amp;lt;+119&amp;gt;:   callq  0x400540 &amp;lt;printf@plt&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&amp;lt;+78&amp;gt; 到 &amp;lt;+98&amp;gt; 是取 argv[cnt]。&amp;lt;+101&amp;gt; 到 &amp;lt;+104&amp;gt; 则为 argv[cnt][0]。gdb 0x400800 地址：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /s 0x400800
0x400800:       &amp;quot;%c\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;即为：
&lt;code&gt;
printf(&amp;quot;%c\n&amp;quot;, argv[cnt][0]);
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;default：&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x0000000000400739 &amp;lt;+185&amp;gt;:   mov    -0x4(%rbp),%eax
0x000000000040073c &amp;lt;+188&amp;gt;:   mov    %eax,%esi
0x000000000040073e &amp;lt;+190&amp;gt;:   mov    $0x400808,%edi
0x0000000000400743 &amp;lt;+195&amp;gt;:   mov    $0x0,%eax
0x0000000000400748 &amp;lt;+200&amp;gt;:   callq  0x400540 &amp;lt;printf@plt&amp;gt;
0x000000000040074d &amp;lt;+205&amp;gt;:   nop
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;gdb 0x400808：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /s 0x400808
0x400808:       &amp;quot;%s\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;即&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;printf(&amp;quot;%s\n&amp;quot;, cnt);
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;因此逆向出来的 main 函数如下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;int main(int argc, char* argv[]) {
  int cnt = 0;
  for (; cnt &amp;lt; args; cnt++) {
      int i = strlen(argv[cnt]);
      switch (i) {
      case 1:
        puts(argv[cnt + 1]);
        break;
      case 2:
        printf(&amp;quot;%d\n&amp;quot;, cnt);
        break;
      case 0:
        printf(&amp;quot;%c\n&amp;quot;, argv[cnt][0]);
        break;
      default:
        printf(&amp;quot;%s\n&amp;quot;, cnt);
        break;
      }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;和上面我们给的源码几乎一致。&lt;/p&gt;

&lt;p&gt;由 于core 时栈地址 0x000000000040074d 位于 &amp;lt;+188&amp;gt; 到 &amp;lt;+205&amp;gt; 的范围，即 default 分支的代码。根据我们逆向的 default 分支的代码：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;printf(&amp;quot;%s\n&amp;quot;, cnt);
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;确实会引起崩溃。&lt;/p&gt;
</description>
        <pubDate>Wed, 19 Sep 2018 21:15:05 +0800</pubDate>
        <link>http://masutangu.com/2018/09/coredump-note-2/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/09/coredump-note-2/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Core dump 原理探究学习笔记（一）</title>
        <description>&lt;p&gt;本系列文章是读&lt;a href=&quot;https://blog.csdn.net/xuzhina/article/category/1322964&quot;&gt;《coredump问题原理探究》&lt;/a&gt;的读书笔记。&lt;/p&gt;

&lt;h2&gt;函数调用帧&lt;/h2&gt;

&lt;p&gt;先看一个例子&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;// test.cpp

int func(int c, char* s, int off) {
  int a = 0x12345678;
  int *p = &amp;amp;a;
  int res = c + *(s + off);
  return *p + res;
}

int main() {
  int b = 0x87654321;
  return b + func(0x100, &amp;quot;hello&amp;quot;, 3);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;g++ -o test test.cpp&lt;/code&gt; 编译后，gdb test 看下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) disassemble func
Dump of assembler code for function _Z4funciPci:
  0x00000000004005b0 &amp;lt;+0&amp;gt;: push %rbp
  0x00000000004005b1 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
  0x00000000004005b4 &amp;lt;+4&amp;gt;: mov %edi,-0x14(%rbp)
  0x00000000004005b7 &amp;lt;+7&amp;gt;: mov %rsi,-0x20(%rbp)
  0x00000000004005bb &amp;lt;+11&amp;gt;: mov %edx,-0x18(%rbp)
  0x00000000004005be &amp;lt;+14&amp;gt;: movl $0x12345678,-0x10(%rbp)
  0x00000000004005c5 &amp;lt;+21&amp;gt;: lea -0x10(%rbp),%rax
  0x00000000004005c9 &amp;lt;+25&amp;gt;: mov %rax,-0x8(%rbp)
  0x00000000004005cd &amp;lt;+29&amp;gt;: mov -0x18(%rbp),%eax
  0x00000000004005d0 &amp;lt;+32&amp;gt;: movslq %eax,%rdx
  0x00000000004005d3 &amp;lt;+35&amp;gt;: mov -0x20(%rbp),%rax
  0x00000000004005d7 &amp;lt;+39&amp;gt;: add %rdx,%rax
  0x00000000004005da &amp;lt;+42&amp;gt;: movzbl (%rax),%eax
  0x00000000004005dd &amp;lt;+45&amp;gt;: movsbl %al,%edx
  0x00000000004005e0 &amp;lt;+48&amp;gt;: mov -0x14(%rbp),%eax
  0x00000000004005e3 &amp;lt;+51&amp;gt;: add %edx,%eax
  0x00000000004005e5 &amp;lt;+53&amp;gt;: mov %eax,-0xc(%rbp)
  0x00000000004005e8 &amp;lt;+56&amp;gt;: mov -0x8(%rbp),%rax
  0x00000000004005ec &amp;lt;+60&amp;gt;: mov (%rax),%edx
  0x00000000004005ee &amp;lt;+62&amp;gt;: mov -0xc(%rbp),%eax
  0x00000000004005f1 &amp;lt;+65&amp;gt;: add %edx,%eax
  0x00000000004005f3 &amp;lt;+67&amp;gt;: pop %rbp
  0x00000000004005f4 &amp;lt;+68&amp;gt;: retq
End of assembler dump.
(gdb) disassemble main
Dump of assembler code for function main:
  0x00000000004005f5 &amp;lt;+0&amp;gt;: push %rbp
  0x00000000004005f6 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
  0x00000000004005f9 &amp;lt;+4&amp;gt;: sub $0x10,%rsp
  0x00000000004005fd &amp;lt;+8&amp;gt;: movl $0x87654321,-0x4(%rbp)
  0x0000000000400604 &amp;lt;+15&amp;gt;: mov $0x3,%edx
  0x0000000000400609 &amp;lt;+20&amp;gt;: mov $0x4006b0,%esi
  0x000000000040060e &amp;lt;+25&amp;gt;: mov $0x100,%edi
  0x0000000000400613 &amp;lt;+30&amp;gt;: callq 0x4005b0 &amp;lt;_Z4funciPci&amp;gt;
  0x0000000000400618 &amp;lt;+35&amp;gt;: mov -0x4(%rbp),%edx
  0x000000000040061b &amp;lt;+38&amp;gt;: add %edx,%eax
  0x000000000040061d &amp;lt;+40&amp;gt;: leaveq
  0x000000000040061e &amp;lt;+41&amp;gt;: retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到，每个函数调用的开始和结束都有如下指令：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;push %rbp     // 入栈 rbp 保存下旧的 rbp，此时 rsp -= 8 (64 位下)
mov %rsp,%rbp // 设置 rbp = rsp
...
pop %rbp      // 调用结束 出栈恢复 rbp，此时 rsp += 8 (64 位下)
retq          // 把 rbp 指向地址下一个单元的内容放到 rip，此时 rsp += 8 (64 位下)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;打个断点验证下：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) tbreak *0x0000000000400613
Temporary breakpoint 1 at 0x400613
(gdb) r
Starting program: test3
Temporary breakpoint 1, 0x0000000000400613 in main ()
Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.tl2.2.x86_64 libgcc-4.8.5-4.el7.x86_64 libstdc++-4.8.5-4.el7.x86_64
(gdb) i r rsp rbp
rsp 0x7fffffffe460 0x7fffffffe460
rbp 0x7fffffffe470 0x7fffffffe470
(gdb) x /4x $rsp
0x7fffffffe460: 0xffffe550 0x00007fff 0x00000000 0x87654321
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;si 跟进去，发现 rsp 比之前小了 8 字节（从 &lt;strong&gt;0x7fffffffe460&lt;/strong&gt; 变成了 &lt;strong&gt;0x7fffffffe458&lt;/strong&gt;）。这是因为调用 call 指令会把返回地址（也就是 &lt;code&gt;callq 0x4005b0 &amp;lt;_Z4funciPci&amp;gt;&lt;/code&gt; 的下一条指令 &lt;code&gt;mov -0x4(%rbp),%edx&lt;/code&gt; 的地址）入栈。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) si
0x00000000004005b0 in func(int, char*, int) ()
(gdb) i r rsp rbp
rsp 0x7fffffffe458 0x7fffffffe458
rbp 0x7fffffffe470 0x7fffffffe470
(gdb) x /4x $rsp
0x7fffffffe458: 0x00400618 0x00000000 0xffffe550 0x00007fff
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;继续执行 &lt;code&gt;push %rbp&lt;/code&gt; 之后：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) ni
0x00000000004005b1 in func(int, char*, int) ()
(gdb) i r rsp rbp
rsp 0x7fffffffe450 0x7fffffffe450
rbp 0x7fffffffe470 0x7fffffffe470
(gdb) x /4x $rsp
0x7fffffffe450: 0xffffe470 0x00007fff 0x00400618 0x00000000
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看出 &lt;code&gt;push %rbp&lt;/code&gt; 之后 rsp 指向的地址存放了 rbp 的值：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x7fffffffe450: 0xffffe470 0x00007fff
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;继续执行 &lt;code&gt;mov %rsp,%rbp&lt;/code&gt;，此时 rsp 和 rbp 的值一致。rbp 指向上个函数帧的 rbp（本样例为 &lt;code&gt;0xffffe470&lt;/code&gt;），rbp 的下个单元指向返回地址（本样例为 &lt;code&gt;0x00400618&lt;/code&gt;） ，info symbol 该返回地址可以打印出函数名称（本样例为 &lt;code&gt;main + 35 in section .text of test3&lt;/code&gt;）。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) ni
0x00000000004005b4 in func(int, char*, int) ()
(gdb) i r rsp rbp
rsp 0x7fffffffe450 0x7fffffffe450
rbp 0x7fffffffe450 0x7fffffffe450
(gdb) x /4x $rbp
0x7fffffffe450: 0xffffe470 0x00007fff 0x00400618 0x00000000
(gdb) info symbol 0x00400618
main + 35 in section .text of test3
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看出函数调用时帧布局:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/coredump-note-1/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;rbp 地址大于等于 rsp&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rbp 下一个单元的内容存放返回地址，info symbol 会显示出函数名称&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rbp 所指向的单元是上一帧的 rbp，亦遵守上述两个规则&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;栈溢出调试&lt;/h2&gt;

&lt;p&gt;构造一个栈溢出的例子：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;// test2.cpp 
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

int overflow(int level, char* str) {
  char buff[16];
  strcpy(buff, str);
  printf(&amp;quot;buffer:%s&amp;quot;, buff);
  return ++level;
}

int wrapper1(int level, char* str) {
  return overflow( ++level, str );
}

int wrapper2(int level, char* str) {
  return wrapper1(++level, str);
}

int wrapper3(int level, char* str) {
  return wrapper2(++level, str);
}

int main(int argc, char* argv[]) {
  if ( argc &amp;lt; 2 ) {
    return -1;
  }
  return wrapper3(0, argv[1]);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;执行 &lt;code&gt;./test2 WeAreHumanBeingsNothingCanNotStopUsWe&lt;/code&gt; core 掉。gdb coredump 文件：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) bt
#0 0x0000000000400680 in overflow(int, char*) ()
#1 0x6f7453746f4e6e61 in ?? ()
#2 0x0000006557735570 in ?? ()
#3 0x00000003e167f5b6 in ?? ()
#4 0x00007ffeccfd8200 in ?? ()
#5 0x00000000004006cb in wrapper2(int, char*) ()
Cannot access memory at address 0x43676e6968746f56
(gdb) i r rsp rbp
rsp 0x7ffeccfd81c8 0x7ffeccfd81c8
rbp 0x43676e6968746f4e 0x43676e6968746f4e
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;查看 rsp rbp 发现 rbp 小于 rsp，根据上节的总结，我们知道 rbp 的值是非法的。&lt;/p&gt;

&lt;p&gt;查看下 rsp 的内容，尝试找到正确的 rbp：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /32x $rsp
0x7ffeccfd81c8: 0x6f4e6e61 0x6f745374 0x57735570 0x00000065
0x7ffeccfd81d8: 0xe167f5b6 0x00000003 0xccfd8200 0x00007ffe
0x7ffeccfd81e8: 0x004006cb 0x00000000 0xccfd979d 0x00007ffe
0x7ffeccfd81f8: 0xe0a97890 0x00000002 0xccfd8220 0x00007ffe
0x7ffeccfd8208: 0x004006f1 0x00000000 0xccfd979d 0x00007ffe
0x7ffeccfd8218: 0x00000000 0x00000001 0xccfd8240 0x00007ffe
0x7ffeccfd8228: 0x00400727 0x00000000 0xccfd8328 0x00007ffe
0x7ffeccfd8238: 0x00000000 0x00000002 0x00000000 0x00000000
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;0x7ffeccfd8200, 0x00007ffeccfd979d, 0x00007ffeccfd8220, 0x00007ffeccfd8240, 0x00007ffeccfd8328 都大于 0x7ffeccfd81c8，info symbol 这三个地址的下个单元地址看看能否正常显示函数：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) info symbol 0x004006cb
wrapper2(int, char*) + 36 in section .text of test
(gdb) info symbol 0xe0a97890
No symbol matches 0xe0a97890.
(gdb) info symbol 0x004006f1
wrapper3(int, char*) + 36 in section .text of test
(gdb) info symbol 0x00400727
main + 52 in section .text of test
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到 0x7ffeccfd8200, 0x00007ffeccfd8220, 0x00007ffeccfd8240 都可以正常显示下个单元的地址，结论 1 和 2都满足，接下来看看第 3 个结论是否也满足：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /2x 0x7ffeccfd8200
0x7ffeccfd8200: 0xccfd8220 0x00007ffe
(gdb) x /2x 0x00007ffeccfd8220
0x7ffeccfd8220: 0xccfd8240 0x00007ffe
(gdb) x /2x 0x00007ffeccfd8240
0x7ffeccfd8240: 0x00000000 0x00000000
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;0x7ffeccfd8200 刚好指向 0x00007ffeccfd8220，0x00007ffeccfd8220 也刚好指向 0x00007ffeccfd8240。所以这三个都是合法的 rbp。这样我们已经恢复了真实的调用栈：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;wrapper2(int, char*) + 36 in section .text of test
wrapper3(int, char*) + 36 in section .text of test
main + 52 in section .text of test
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看看 wrapper2 的汇编：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) disassemble wrapper2
Dump of assembler code for function _Z8wrapper2iPc:
0x00000000004006a7 &amp;lt;+0&amp;gt;: push %rbp
0x00000000004006a8 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
0x00000000004006ab &amp;lt;+4&amp;gt;: sub $0x10,%rsp
0x00000000004006af &amp;lt;+8&amp;gt;: mov %edi,-0x4(%rbp)
0x00000000004006b2 &amp;lt;+11&amp;gt;: mov %rsi,-0x10(%rbp)
0x00000000004006b6 &amp;lt;+15&amp;gt;: addl $0x1,-0x4(%rbp)
0x00000000004006ba &amp;lt;+19&amp;gt;: mov -0x10(%rbp),%rdx
0x00000000004006be &amp;lt;+23&amp;gt;: mov -0x4(%rbp),%eax
0x00000000004006c1 &amp;lt;+26&amp;gt;: mov %rdx,%rsi
0x00000000004006c4 &amp;lt;+29&amp;gt;: mov %eax,%edi
0x00000000004006c6 &amp;lt;+31&amp;gt;: callq 0x400681 &amp;lt;_Z8wrapper1iPc&amp;gt;
0x00000000004006cb &amp;lt;+36&amp;gt;: leaveq
0x00000000004006cc &amp;lt;+37&amp;gt;: retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;可以看到这里返回地址为 &lt;code&gt;0x00000000004006cb &amp;lt;+36&amp;gt;: leaveq&lt;/code&gt;。
从前面&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;0x7ffeccfd81e8: 0x004006cb 0x00000000 0xccfd979d 0x00007ffe
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看到栈地址 0x7ffeccfd81e8 保存 wrapper1 的返回地址为 0x004006cb ，和 wrapper2 的汇编相符
，故继续看看 &lt;code&gt;callq 0x400681 &amp;lt;_Z8wrapper1iPc&amp;gt;&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;shell c++filt 查看下函数的原型：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) shell c++filt _Z8wrapper1iPc
wrapper1(int, char*)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看下 wrapper1 的汇编：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) disassemble wrapper1
Dump of assembler code for function _Z8wrapper1iPc:
0x0000000000400681 &amp;lt;+0&amp;gt;: push %rbp
0x0000000000400682 &amp;lt;+1&amp;gt;: mov %rsp,%rbp
0x0000000000400685 &amp;lt;+4&amp;gt;: sub $0x10,%rsp
0x0000000000400689 &amp;lt;+8&amp;gt;: mov %edi,-0x4(%rbp)
0x000000000040068c &amp;lt;+11&amp;gt;: mov %rsi,-0x10(%rbp)
0x0000000000400690 &amp;lt;+15&amp;gt;: addl $0x1,-0x4(%rbp)
0x0000000000400694 &amp;lt;+19&amp;gt;: mov -0x10(%rbp),%rdx
0x0000000000400698 &amp;lt;+23&amp;gt;: mov -0x4(%rbp),%eax
0x000000000040069b &amp;lt;+26&amp;gt;: mov %rdx,%rsi
0x000000000040069e &amp;lt;+29&amp;gt;: mov %eax,%edi
0x00000000004006a0 &amp;lt;+31&amp;gt;: callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;
0x00000000004006a5 &amp;lt;+36&amp;gt;: leaveq
0x00000000004006a6 &amp;lt;+37&amp;gt;: retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;&lt;/code&gt; 之前都是 rbp rsp 操作，推测出执行 &lt;code&gt;callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;&lt;/code&gt; 时引起的栈溢出。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) x /16x $rsp
0x7ffeccfd81c8: 0x6f4e6e61 0x6f745374 0x57735570 0x00000065
0x7ffeccfd81d8: 0xe167f5b6 0x00000003 0xccfd8200 0x00007ffe
0x7ffeccfd81e8: 0x004006cb 0x00000000 0xccfd979d 0x00007ffe
0x7ffeccfd81f8: 0xe0a97890 0x00000002 0xccfd8220 0x00007ffe
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;0x7ffeccfd81c8 的内容本应该是 &lt;code&gt;0x004006a5 0x00000000&lt;/code&gt; （指向返回地址 &lt;code&gt;x00000000004006a5 &amp;lt;+36&amp;gt;: leaveq&lt;/code&gt;），但由于栈溢出被覆盖了。因此猜测是 _Z8overflowiPc 函数引起的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：为什么是 0x7ffeccfd81c8？前面知道 0x7ffeccfd81e8 保存调用 wrapper1 的返回地址。由上图可以看出，调用 wrapper1 时， 减去 8 个字节的 &lt;code&gt;push %rbp&lt;/code&gt;，减去 16 字节的局部变量（&lt;code&gt;sub $0x10,%rsp&lt;/code&gt;），再减去 8 个字节的 &lt;code&gt;callq 0x400640 &amp;lt;_Z8overflowiPc&amp;gt;&lt;/code&gt;，即 0x7ffeccfd81e8 - 32 = 0x7ffeccfd81c8f 保存调用 _Z8overflowiPc 的返回地址0x00000000004006a5&lt;/em&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(gdb) shell c++filt _Z8overflowiPc
overflow(int, char*)
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;因此定位到 &lt;code&gt;overflow&lt;/code&gt; 函数代码中用到 &lt;code&gt;strcpy&lt;/code&gt; 导致溢出。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 22:47:56 +0800</pubDate>
        <link>http://masutangu.com/2018/09/coredump-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/09/coredump-note-1/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Designing Data-Intensive Applications 读书笔记（二）</title>
        <description>&lt;h1&gt;Chapter 4. Encoding and Evolution&lt;/h1&gt;

&lt;p&gt;When a data format or schema changes, a corresponding change to application code often needs to happen. However, &lt;strong&gt;in a large application, code changes often cannot happen instantaneously&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;With server-side applications you may want to perform a rolling upgrade, deploying the new version to a few nodes at a time, checking whether the new version is running smoothly, and gradually working your way through all the nodes. This allows new versions to be deployed without service downtime, and thus encourages more frequent releases and better evolvability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With client-side applications you’re at the mercy of the user, who may not install the update for some time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This means that &lt;strong&gt;old and new versions of the code, and old and new data formats, may potentially all coexist in the system at the same time&lt;/strong&gt;. In order for the system to continue running smoothly, we need to maintain compatibility in both directions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Backward compatibility&lt;/strong&gt;: Newer code can read data that was written by older code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Forward compatibility&lt;/strong&gt;: Older code can read data that was written by newer code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this chapter we will look at several formats for encoding data, including JSON, XML, Protocol Buffers, Thrift, and Avro. In particular, we will look at &lt;strong&gt;how they handle schema changes and how they support systems where old and new data and code need to coexist&lt;/strong&gt;. We will then discuss how those formats are used &lt;strong&gt;for data storage and for communication&lt;/strong&gt;: in web services, Representational State Transfer (REST), and remote procedure calls (RPC), as well as message-passing systems such as actors and message queues.&lt;/p&gt;

&lt;h2&gt;Formats for Encoding Data&lt;/h2&gt;

&lt;p&gt;Programs usually work with data in (at least) two different representations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so on. These data structures are optimized for efficient access and manipulation by the CPU (typically using pointers).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you want to write data to a file or send it over the network, &lt;strong&gt;you have to encode it as some kind of self-contained sequence of bytes&lt;/strong&gt; (for example, a JSON document). Since a pointer wouldn’t make sense to any other process, this sequence-of-bytes representation looks quite different from the data structures that are normally used in memory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, we need some kind of translation between the two representations. The translation from the in-memory representation to a byte sequence is called &lt;strong&gt;encoding (also known as serialization or marshalling)&lt;/strong&gt;, and the reverse is called &lt;strong&gt;decoding (parsing, deserialization, unmarshalling)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Language-Specific Formats&lt;/h3&gt;

&lt;p&gt;Many programming languages come with built-in support for encoding in-memory objects into byte sequences. However, they also have a number of deep problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The encoding is often tied to a particular programming language&lt;/strong&gt;, and reading the data in another language is very difficult.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to restore data in the same object types, the decoding process needs to be able to instantiate arbitrary classes. &lt;strong&gt;This is frequently a source of security problems: if an attacker can get your application to decode an arbitrary byte sequence, they can instantiate arbitrary classes, which in turn often allows them to do terrible things such as remotely executing arbitrary code.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Versioning data is often an afterthought in these libraries&lt;/strong&gt;: as they are intended for quick and easy encoding of data, they often neglect the inconvenient problems of forward and backward compatibility.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Efficiency (CPU time taken to encode or decode, and the size of the encoded structure) is also often an afterthought. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;For these reasons it’s generally a bad idea to use your language’s built-in encoding for anything other than very transient purposes.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;JSON, XML, and Binary Variants&lt;/h3&gt;

&lt;p&gt;JSON, XML, and CSV are textual formats, and thus somewhat human-readable. Besides the superficial syntactic issues, they also have some subtle problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;There is a lot of ambiguity around the encoding of numbers.&lt;/strong&gt; This is a problem when dealing with large numbers; for example, integers greater than 253 cannot be exactly represented in an IEEE 754 double-precision floating-point number, so such numbers become inaccurate when parsed in a language that uses floating-point numbers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;JSON and XML have good support for Unicode character strings (i.e., human- readable text), but &lt;strong&gt;they don’t support binary strings&lt;/strong&gt; (sequences of bytes without a character encoding). Binary strings are a useful feature, so people get around this limitation by encoding the binary data as text using Base64. This works, but it’s somewhat hacky and increases the data size by 33%.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Thrift and Protocol Buffers&lt;/h3&gt;

&lt;p&gt;Both Thrift and Protocol Buffers require a schema for any data that is encoded.&lt;/p&gt;

&lt;h2&gt;Modes of Dataflow&lt;/h2&gt;

&lt;p&gt;In this chapter we will explore some of the most common ways how data flows between processes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Via databases &lt;/li&gt;
&lt;li&gt;Via service calls&lt;/li&gt;
&lt;li&gt;Via asynchronous message passing&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Dataflow Through Databases&lt;/h3&gt;

&lt;p&gt;Backward compatibility is clearly necessary here; otherwise your future self won’t be able to decode what you previously wrote.&lt;/p&gt;

&lt;p&gt;A value in the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. Thus, forward compatibility is also often required for databases.&lt;/p&gt;

&lt;p&gt;If you decode a database value into model objects in the application, and later reencode those model objects, &lt;strong&gt;the unknown field might be lost in that translation process&lt;/strong&gt;. Solving this is not a hard problem; you just need to be aware of it.&lt;/p&gt;

&lt;h3&gt;Dataflow Through Services: REST and RPC&lt;/h3&gt;

&lt;p&gt;There are two popular approaches to web services: &lt;strong&gt;REST and SOAP&lt;/strong&gt;. They are almost diametrically opposed in terms of philosophy.&lt;/p&gt;

&lt;p&gt;REST is not a protocol, but rather a design philosophy that builds upon the principles of HTTP. &lt;strong&gt;It emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication, and content type negotiation.&lt;/strong&gt; An API designed according to the principles of REST is called RESTful.&lt;/p&gt;

&lt;p&gt;By contrast, SOAP is an XML-based protocol for making network API requests. Although it is most commonly used over HTTP, it aims to be independent from HTTP and avoids using most HTTP features.&lt;/p&gt;

&lt;p&gt;The API of a SOAP web service is described using an XML-based language called the Web Services Description Language, or WSDL. WSDL enables code generation so that a client can access a remote service using local classes and method calls. This is useful in statically typed programming languages, but less so in dynamically typed ones.&lt;/p&gt;

&lt;p&gt;RESTful APIs tend to favor simpler approaches, typically involving less code generation and automated tooling. &lt;/p&gt;

&lt;h4&gt;Data encoding and evolution for RPC&lt;/h4&gt;

&lt;p&gt;The RPC model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process (&lt;strong&gt;this abstraction is called location transparency&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;It is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, you only need &lt;strong&gt;backward compatibility on requests, and forward compatibility on responses&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The backward and forward compatibility properties of an RPC scheme are inherited from whatever encoding it uses.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service compatibility is made harder by the fact that RPC is often used for communication across organizational boundaries&lt;/strong&gt;, so the provider of a service often has no control over its clients and cannot force them to upgrade. Thus, compatibility needs to be maintained for a long time, perhaps indefinitely. &lt;/p&gt;

&lt;h3&gt;Message-Passing Dataflow&lt;/h3&gt;

&lt;p&gt;Asynchronous message-passing systems are similar to RPC in that a client’s request (usually called a message) is delivered to another process with low latency. They are similar to databases in that the message is not sent via a direct network connection, but goes via an intermediary called a message broker (also called a message queue or message-oriented middleware), which stores the message temporarily.&lt;/p&gt;

&lt;p&gt;Using a message broker has several advantages compared to direct RPC:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It can act as a buffer if the recipient is unavailable or overloaded, and thus improve system reliability.&lt;/li&gt;
&lt;li&gt;It can automatically redeliver messages to a process that has crashed, and thus prevent messages from being lost.&lt;/li&gt;
&lt;li&gt;It avoids the sender needing to know the IP address and port number of the recipient (which is particularly useful in a cloud deployment where virtual machines often come and go).&lt;/li&gt;
&lt;li&gt;It allows one message to be sent to several recipients.&lt;/li&gt;
&lt;li&gt;It logically decouples the sender from the recipient (the sender just publishes messages and doesn’t care who consumes them).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, a difference compared to RPC is that &lt;strong&gt;message-passing communication is usually one-way: a sender normally doesn’t expect to receive a reply to its messages&lt;/strong&gt;. It is possible for a process to send a response, but this would usually be done on a separate channel. This communication pattern is &lt;strong&gt;asynchronous&lt;/strong&gt;: the sender doesn’t wait for the message to be delivered, but simply sends it and then forgets about it.&lt;/p&gt;

&lt;h4&gt;Distributed actor frameworks&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;The actor model is a programming model for concurrency in a single process.&lt;/strong&gt; Rather than dealing directly with threads (and the associated problems of race conditions, locking, and deadlock), logic is encapsulated in actors. &lt;strong&gt;Each actor typically represents one client or entity, it may have some local state (which is not shared with any other actor), and it communicates with other actors by sending and receiving asynchronous messages.&lt;/strong&gt; Since each actor processes only one message at a time, it doesn’t need to worry about threads, and each actor can be scheduled independently by the framework.&lt;/p&gt;

&lt;p&gt;In distributed actor frameworks, this programming model is used to scale an application across multiple nodes. &lt;strong&gt;The same message-passing mechanism is used, no matter whether the sender and recipient are on the same node or different nodes.&lt;/strong&gt; If they are on different nodes, the message is transparently encoded into a byte sequence, sent over the network, and decoded on the other side.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Many services need to support rolling upgrades, where a new version of a service is gradually deployed to a few nodes at a time, rather than deploying to all nodes simultaneously. Rolling upgrades allow new versions of a service to be released without downtime. &lt;strong&gt;These properties are hugely beneficial for evolvability, the ease of making changes to an application.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During rolling upgrades, or for various other reasons, we must assume that &lt;strong&gt;different nodes are running the different versions of our application’s code&lt;/strong&gt;. Thus, it is important that &lt;strong&gt;all data flowing around the system is encoded in a way that provides backward compatibility (new code can read old data) and forward compatibility (old code can read new data)&lt;/strong&gt;.&lt;/p&gt;

&lt;h1&gt;Chapter 5. Replication&lt;/h1&gt;

&lt;p&gt;There are various reasons why you might want to distribute a database across multiple machines:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your data volume, read load, or write load grows bigger than a single machine can handle, you can potentially spread the load across multiple machines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fault tolerance/high availability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your application needs to continue working even if one machine (or several machines, or the network, or an entire datacenter) goes down, you can use multiple machines to give you redundancy. When one fails, another one can take over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you have users around the world, you might want to have servers at various locations worldwide so that each user can be served from a datacenter that is geographically close to them. That avoids the users having to wait for network packets to travel halfway around the world.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Replication means keeping a copy of the same data on multiple machines that are connected via a network.&lt;/strong&gt; There are several reasons why you might want to replicate data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To keep data geographically close to your users&lt;/strong&gt; (and thus reduce latency)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To allow the system to continue working even if some of its parts have failed&lt;/strong&gt; (and thus increase availability)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To scale out the number of machines that can serve read queries&lt;/strong&gt; (and thus increase read throughput)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will discuss three popular algorithms for replicating changes between nodes: &lt;strong&gt;single-leader, multi-leader, and leaderless replication&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Leaders and Followers&lt;/h2&gt;

&lt;p&gt;Each node that stores a copy of the database is called a replica. Every write to the database needs to be processed by every replica. &lt;strong&gt;The most common solution for this is called leader-based replication&lt;/strong&gt; (also known as active/passive or master–slave replication):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;One of the replicas is designated the leader&lt;/strong&gt; (also known as master or primary). When clients want to write to the database, they must send their requests to the leader, which first writes the new data to its local storage.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The other replicas are known as followers (read replicas, slaves, secondaries, or hot standbys). Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of a replication log or change stream. &lt;strong&gt;Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader (the followers are read-only from the client’s point of view).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Synchronous Versus Asynchronous Replication&lt;/h3&gt;

&lt;p&gt;Often, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable.&lt;/p&gt;

&lt;h3&gt;Setting Up New Followers&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Take a consistent snapshot of the leader’s database at some point in time—if possible, without taking a lock on the entire database. Most databases have this feature, as it is also required for backups.&lt;/li&gt;
&lt;li&gt;Copy the snapshot to the new follower node.&lt;/li&gt;
&lt;li&gt;The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in the leader’s replication log.&lt;/li&gt;
&lt;li&gt;When the follower has processed the backlog of data changes since the snapshot, we say it has caught up. It can now continue to process data changes from the leader as they happen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Handling Node Outages&lt;/h3&gt;

&lt;p&gt;How do you achieve high availability with leader-based replication?&lt;/p&gt;

&lt;h4&gt;Follower failure: Catch-up recovery&lt;/h4&gt;

&lt;p&gt;The follower can recover quite easily: from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. &lt;/p&gt;

&lt;h4&gt;Leader failure: Failover&lt;/h4&gt;

&lt;p&gt;Handling a failure of the leader is trickier: &lt;strong&gt;one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader&lt;/strong&gt;. This process is called failover.&lt;/p&gt;

&lt;p&gt;Failover can happen manually or automatically. An automatic failover process usually consists of the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Determining that the leader has failed.&lt;/li&gt;
&lt;li&gt;Choosing a new leader.&lt;/li&gt;
&lt;li&gt;Reconfiguring the system to use the new leader.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Implementation of Replication Logs&lt;/h3&gt;

&lt;h4&gt;Statement-based replication&lt;/h4&gt;

&lt;p&gt;In the simplest case, the leader logs every write request (statement) that it executes and sends that statement log to its followers.&lt;/p&gt;

&lt;p&gt;There are various ways in which this approach to replication can break down:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Any statement that calls a nondeterministic function&lt;/strong&gt;, such as NOW() to get the current date and time or RAND() to get a random number, is likely to generate a different value on each replica.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;If statements use an autoincrementing column&lt;/strong&gt;, or if they depend on the existing data in the database (e.g., UPDATE ... WHERE &lt;some condition&gt;), they must be executed in exactly the same order on each replica, or else they may have a different effect. This can be limiting when there are multiple concurrently executing transactions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statements that have side effects&lt;/strong&gt; (e.g., triggers, stored procedures, user-defined functions) may result in different side effects occurring on each replica, unless the side effects are absolutely deterministic.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Statement-based replication was used in MySQL before version 5.1.&lt;/p&gt;

&lt;h4&gt;Write-ahead log (WAL) shipping&lt;/h4&gt;

&lt;p&gt;In Chapter 3 we discussed how storage engines represent data on disk, and we found that usually every write is appended to a log. &lt;strong&gt;In either case, the log is an append-only sequence of bytes containing all writes to the database.&lt;/strong&gt; We can use the exact same log to build a replica on another node: besides writing the log to disk, the leader also sends it across the network to its followers. When the follower processes this log, it builds a copy of the exact same data structures as found on the leader.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The main disadvantage is that the log describes the data on a very low level&lt;/strong&gt;: a WAL contains details of which bytes were changed in which disk blocks. &lt;strong&gt;This makes replication closely coupled to the storage engine.&lt;/strong&gt; If the database changes its storage format from one version to another, it is typically not possible to run different versions of the database software on the leader and the followers.&lt;/p&gt;

&lt;h4&gt;Logical (row-based) log replication&lt;/h4&gt;

&lt;p&gt;An alternative is to use different log formats for replication and for the storage engine, which allows the replication log to be &lt;strong&gt;decoupled from the storage engine internals&lt;/strong&gt;. This kind of replication log is called a logical log, to distinguish it from the storage engine’s (physical) data representation.&lt;/p&gt;

&lt;p&gt;A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For an inserted row, the log contains the new values of all columns.&lt;/li&gt;
&lt;li&gt;For a deleted row, the log contains enough information to uniquely identify the row that was deleted. Typically this would be the primary key, but if there is no primary key on the table, the old values of all columns need to be logged.&lt;/li&gt;
&lt;li&gt;For an updated row, the log contains enough information to uniquely identify the updated row, and the new values of all columns (or at least the new values of all columns that changed).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A transaction that modifies several rows generates several such log records, followed by a record indicating that the transaction was committed. MySQL’s binlog (when configured to use row-based replication) uses this approach.&lt;/p&gt;

&lt;h4&gt;Trigger-based replication&lt;/h4&gt;

&lt;p&gt;The replication approaches described so far are implemented by the database system, without involving any application code. &lt;strong&gt;If you want to only replicate a subset of the data, or want to replicate from one kind of database to another, or if you need conflict resolution logic, then you may need to move replication up to the application layer.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;An alternative is to use features that are available in many relational databases: &lt;strong&gt;triggers and stored procedures&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A trigger lets you register custom application code that is automatically executed when a data change (write transaction) occurs in a database system. &lt;/p&gt;

&lt;h2&gt;Problems with Replication Lag&lt;/h2&gt;

&lt;p&gt;Being able to tolerate node failures is just one reason for wanting replication. Other reasons are &lt;strong&gt;scalability&lt;/strong&gt; (processing more requests than a single machine can handle) and &lt;strong&gt;latency&lt;/strong&gt; (placing replicas geo‐graphically closer to users).&lt;/p&gt;

&lt;p&gt;Leader-based replication requires all writes to go through a single node, but read-only queries can go to any replica. &lt;/p&gt;

&lt;p&gt;However, this approach only realistically works with asynchronous replication—if you tried to synchronously replicate to all followers, a single node failure or network outage would make the entire system unavailable for writing.&lt;/p&gt;

&lt;p&gt;Unfortunately, &lt;strong&gt;if an application reads from an asynchronous follower, it may see outdated information if the follower has fallen behind&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This inconsistency is just a temporary state—if you stop writing to the database and wait a while, the followers will eventually catch up and become consistent with the leader. For that reason, this effect is known as &lt;strong&gt;eventual consistency&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Reading Your Own Writes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Read-after-write consistency&lt;/strong&gt;, also known as &lt;strong&gt;read-your-writes consistency&lt;/strong&gt;, is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves.&lt;/p&gt;

&lt;p&gt;There are various possible techniques to implement read-after-write consistency in a system with leader-based replication:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;When reading something that the user may have modified, read it from the leader&lt;/strong&gt;; otherwise, read it from a follower. This requires that you have some way of knowing whether something might have been modified, without actually querying it. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If most things in the application are potentially editable by the user, that approach won’t be effective, as most things would have to be read from the leader.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The client can remember the timestamp of its most recent write—then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp.&lt;/strong&gt; If a replica is not sufficiently up to date, either the read can be handled by another replica or the query can wait until the replica has caught up. The timestamp could be a logical timestamp (something that indicates ordering of writes, such as the log sequence number) or the actual system clock.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If your replicas are distributed across multiple datacenters (for geographical proximity to users or for availability), there is additional complexity. Any request that needs to be served by the leader must be routed to the datacenter that contains the leader.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another complication arises when the same user is accessing your service from multiple devices, in this case, there are some additional issues to consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Approaches that require remembering the timestamp of the user’s last update become more difficult, because the code running on one device doesn’t know what updates have happened on the other device. This metadata will need to be centralized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If your replicas are distributed across different datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. If your approach requires reading from the leader, you may first need to route requests from all of a user’s devices to the same datacenter.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Monotonic Reads&lt;/h3&gt;

&lt;p&gt;Our second example of an anomaly that can occur when reading from asynchronous followers is that it’s possible for a user to see things &lt;strong&gt;moving backward in time&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monotonic reads&lt;/strong&gt; is a guarantee that this kind of anomaly does not happen. &lt;strong&gt;It’s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency.&lt;/strong&gt; When you read data, you may see an old value; &lt;strong&gt;monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One way of achieving monotonic reads is to make sure that each user always makes their reads from the same replica.&lt;/strong&gt; However, if that replica fails, the user’s queries will need to be rerouted to another replica.&lt;/p&gt;

&lt;h3&gt;Consistent Prefix Reads&lt;/h3&gt;

&lt;p&gt;Our third example of replication lag anomalies concerns violation of causality. Preventing this kind of anomaly requires another type of guarantee: &lt;strong&gt;consistent prefix reads&lt;/strong&gt;. &lt;strong&gt;This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a particular problem in partitioned (sharded) databases. If the database always applies writes in the same order, reads always see a consistent prefix, so this anomaly cannot happen. However, &lt;strong&gt;in many distributed databases, different partitions operate independently, so there is no global ordering of writes&lt;/strong&gt;: when a user reads from the database, they may see some parts of the database in an older state and some in a newer state.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One solution is to make sure that any writes that are causally related to each other are written to the same partition&lt;/strong&gt;—but in some applications that cannot be done efficiently. There are also algorithms that explicitly keep track of causal dependencies.&lt;/p&gt;

&lt;h3&gt;Solutions for Replication Lag&lt;/h3&gt;

&lt;p&gt;As discussed earlier, there are ways in which an application can provide a stronger guarantee than the underlying database—for example, by performing certain kinds of reads on the leader. However, dealing with these issues in application code is complex and easy to get wrong.&lt;/p&gt;

&lt;p&gt;It would be better if application developers didn’t have to worry about subtle replication issues and could just trust their databases to “do the right thing.” &lt;strong&gt;This is why transactions exist: they are a way for a database to provide stronger guarantees so that the application can be simpler.&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Multi-Leader Replication&lt;/h2&gt;

&lt;p&gt;Leader-based replication has one major downside: there is only one leader, and all writes must go through it. If you can’t connect to the leader for any reason, you can’t write to the database. &lt;strong&gt;A natural extension of the leader-based replication model is to allow more than one node to accept writes. We call this a multi-leader configuration.&lt;/strong&gt; In this setup, each leader simultaneously acts as a follower to the other leaders.&lt;/p&gt;

&lt;h3&gt;Use Cases for Multi-Leader Replication&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;It rarely makes sense to use a multi-leader setup within a single datacenter, because the benefits rarely outweigh the added complexity.&lt;/strong&gt; However, there are some situations in which this configuration is reasonable.&lt;/p&gt;

&lt;h4&gt;Multi-datacenter operation&lt;/h4&gt;

&lt;p&gt;Imagine you have a database with replicas in several different datacenters (perhaps so that you can tolerate failure of an entire datacenter, or perhaps in order to be closer to your users). With a normal leader-based replication setup, the leader has to be in one of the datacenters, and all writes must go through that datacenter.&lt;/p&gt;

&lt;p&gt;In a multi-leader configuration, you can have a leader in each datacenter. &lt;strong&gt;Within each datacenter, regular leader–follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although multi-leader replication has advantages, it also has a big downside: &lt;strong&gt;the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As multi-leader replication is a somewhat retrofitted feature in many databases, there are often subtle configuration pitfalls and surprising interactions with other database features. For example, autoincrementing keys, triggers, and integrity constraints can be problematic.&lt;/p&gt;

&lt;h4&gt;Clients with offline operation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Another situation in which multi-leader replication is appropriate is if you have an application that needs to continue to work while it is disconnected from the internet.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example, consider the calendar apps on your mobile phone, your laptop, and other devices. If you make any changes while you are offline, they need to be synced with a server and your other devices when the device is next online.&lt;/p&gt;

&lt;p&gt;In this case, &lt;strong&gt;every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas of your calendar on all of your devices&lt;/strong&gt;. The replication lag may be hours or even days, depending on when you have internet access available.&lt;/p&gt;

&lt;h4&gt;Collaborative editing&lt;/h4&gt;

&lt;p&gt;Real-time collaborative editing applications allow several people to edit a document simultaneously. When one user edits a document, the changes are instantly applied to their local replica (the state of the document in their web browser or client application) and asynchronously replicated to the server and any other users who are editing the same document.&lt;/p&gt;

&lt;p&gt;If you want to guarantee that there will be no editing conflicts, the application must obtain a lock on the document before a user can edit it. This collaboration model is equivalent to single-leader
replication with transactions on the leader.&lt;/p&gt;

&lt;h3&gt;Handling Write Conflicts&lt;/h3&gt;

&lt;p&gt;The biggest problem with multi-leader replication is that write conflicts can occur, which means that conflict resolution is required.&lt;/p&gt;

&lt;h4&gt;Synchronous versus asynchronous conflict detection&lt;/h4&gt;

&lt;p&gt;In principle, you could make the conflict detection synchronous—i.e., wait for the write to be replicated to all replicas before telling the user that the write was successful. However, by doing so, you would lose the main advantage of multi-leader replication: allowing each replica to accept writes independently. If you want synchronous conflict detection, you might as well just use single-leader replication.&lt;/p&gt;

&lt;h4&gt;Conflict avoidance&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;The simplest strategy for dealing with conflicts is to avoid them: if the application can ensure that all writes for a particular record go through the same leader&lt;/strong&gt;, then conflicts cannot occur. Since many implementations of multi-leader replication handle conflicts quite poorly, &lt;strong&gt;avoiding conflicts is a frequently recommended approach&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;Converging toward a consistent state&lt;/h4&gt;

&lt;p&gt;There are various ways of achieving convergent conflict resolution:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Give each write a unique ID&lt;/strong&gt; (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner, and throw away the other writes. If a timestamp is used, this technique is known as last write wins (LWW). Although this approach is popular, it is dangerously prone to data loss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Give each replica a unique ID&lt;/strong&gt;, and let writes that originated at a higher-numbered replica always take precedence over writes that originated at a lower-numbered replica. This approach also implies data loss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Somehow merge the values together&lt;/strong&gt;—e.g., order them alphabetically and then concatenate them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Record the conflict in an explicit data structure&lt;/strong&gt; that preserves all information, and write application code that resolves the conflict at some later time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Custom conflict resolution logic&lt;/h4&gt;

&lt;p&gt;As the most appropriate way of resolving a conflict may depend on the application, most multi-leader replication tools let you write conflict resolution logic using application code. That code may be executed on write or on read:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On write&lt;/p&gt;

&lt;p&gt;As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On read&lt;/p&gt;

&lt;p&gt;When a conflict is detected, all the conflicting writes are stored. The next time the data is read, these multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict, and write the result back to the database. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There has been some interesting research into automatically resolving conflicts caused by concurrent data modifications. A few lines of research are worth mentioning:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Conflict-free replicated datatypes (CRDTs) are a family of data structures for sets, maps, ordered lists, counters, etc. that can be concurrently edited by multiple users, and which automatically resolve conflicts in sensible ways.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mergeable persistent data structures track history explicitly, similarly to the Git version control system, and use a three-way merge function (whereas CRDTs use two-way merges).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Operational transformation is the conflict resolution algorithm behind collaborative editing applications such as Etherpad and Google Docs. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute a text document.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Multi-Leader Replication Topologies&lt;/h2&gt;

&lt;p&gt;A replication topology describes the communication paths along which writes are propagated from one node to another. If you have two leaders, there is only one plausible topology: leader 1 must send all of its writes to leader 2, and vice versa. With more than two leaders, various different topologies are possible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The most general topology is all-to-all (Figure [c]), in which every leader sends its writes to every other leader.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A problem with circular and star topologies is that if just one node fails, it can interrupt the flow of replication messages between other nodes&lt;/strong&gt;, causing them to be unable to communicate until the node is fixed. The fault tolerance of a more densely connected topology (such as all-to-all) is better because it allows messages to travel along different paths, avoiding a single point of failure.&lt;/p&gt;

&lt;p&gt;On the other hand, all-to-all topologies can have issues too. In particular, &lt;strong&gt;some network links may be faster than others (e.g., due to network congestion), with the result that some replication messages may “overtake” others.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;To order these events correctly, a technique called version vectors can be used, which we will discuss later in this chapter. However, conflict detection techniques are poorly implemented in many multi-leader replication systems. &lt;/p&gt;

&lt;h2&gt;Leaderless Replication&lt;/h2&gt;

&lt;p&gt;In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes. &lt;/p&gt;

&lt;h3&gt;Writing to the Database When a Node Is Down&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;When a client reads from the database, it doesn’t just send its request to one replica: read requests are also sent to several nodes in parallel. The client may get different responses from different nodes; i.e., the up-to-date value from one node and a stale value from another. &lt;strong&gt;Version numbers are used to determine which value is newer.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Read repair and anti-entropy&lt;/h4&gt;

&lt;p&gt;Two mechanisms are often used in Dynamo-style datastores:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Read repair&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When a client makes a read from several nodes in parallel, it can detect any stale responses. The client sees that replica has a stale value and writes the newer value back to that replica. This approach works well for values that are frequently read.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Anti-entropy process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In addition, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Quorums for reading and writing&lt;/h4&gt;

&lt;p&gt;If there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. &lt;strong&gt;As long as w + r &amp;gt; n, we expect to get an up-to-date value when reading&lt;/strong&gt;, because at least one of the r nodes we’re reading from must be up to date. Reads and writes that obey these r and w values are called &lt;strong&gt;quorum reads and writes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A workload with few writes and many reads may benefit from setting w = n and r = 1. This makes reads faster, but has the disadvantage that just one failed node causes all database writes to fail.&lt;/p&gt;

&lt;h3&gt;Limitations of Quorum Consistency&lt;/h3&gt;

&lt;p&gt;However, even with w + r &amp;gt; n, there are likely to be edge cases where stale values are returned. These depend on the implementation, but possible scenarios include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If a sloppy quorum is used, the w writes may end up on different nodes than the r reads, so there is no longer a guaranteed overlap between the r nodes and the w nodes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If two writes occur concurrently, it is not clear which one happened first. In this case, the only safe solution is to merge the concurrent writes. If a winner is picked based on a timestamp (last write wins), writes can be lost due to clock skew.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a write happens concurrently with a read, the write may be reflected on only some of the replicas. In this case, it’s undetermined whether the read returns the old or the new value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a node carrying a new value fails, and its data is restored from a replica carry‐ ing an old value, the number of replicas storing the new value may fall below w, breaking the quorum condition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even if everything is working correctly, there are edge cases in which you can get unlucky with the timing. See “Linearizability and quorums”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Monitoring staleness&lt;/h4&gt;

&lt;p&gt;For leader-based replication, the database typically exposes metrics for the replication lag, which you can feed into a monitoring system. &lt;strong&gt;By subtracting a follower’s current position from the leader’s current position, you can measure the amount of replication lag.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, in systems with leaderless replication, there is no fixed order in which writes are applied, which makes monitoring more difficult. &lt;/p&gt;

&lt;h3&gt;Detecting Concurrent Writes&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h4&gt;Last write wins (discarding concurrent writes)&lt;/h4&gt;

&lt;p&gt;We say the writes are concurrent, so their order is undefined. Even though the writes don’t have a natural ordering, we can force an arbitrary order on them. For example, we can attach a timestamp to each write, pick the biggest timestamp as the most “recent,” and discard any writes with an earlier timestamp. &lt;strong&gt;This conflict resolution algorithm, called last write wins (LWW).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LWW achieves the goal of eventual convergence, but at the cost of durability: &lt;strong&gt;if there are several concurrent writes to the same key, even if they were all reported as successful to the client (because they were written to w replicas), only one of the writes will survive and the others will be silently discarded&lt;/strong&gt;. Moreover, LWW may even drop writes that are not concurrent, as we shall discuss in “Timestamps for ordering events”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If losing data is not acceptable, LWW is a poor choice for conflict resolution.&lt;/strong&gt; The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key.&lt;/p&gt;

&lt;h4&gt;The “happens-before” relationship and concurrency&lt;/h4&gt;

&lt;p&gt;An operation A happens before another operation B if B knows about A, or depends on A, or builds upon A in some way. In fact, we can simply say that two operations are concurrent if neither happens before the other.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If one operation happened before another, the later operation should overwrite the earlier operation, but if the operations are concurrent, we have a conflict that needs to be resolved.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For defining concurrency, exact time doesn’t matter: we simply call two operations concurrent if they are both unaware of each other, regardless of the physical time at which they occurred. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4&gt;Capturing the happens-before relationship&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note-2/illustration-5.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When a client reads a key, the server returns all values that have not been over‐ written, as well as the latest version number. A client must read a key before writing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. (The response from a write request can be like a read, returning all current values, which allows us to chain several writes like in the shopping cart example.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the server receives a write with a particular version number, &lt;strong&gt;it can overwrite all values with that version number or below&lt;/strong&gt; (since it knows that they have been merged into the new value), &lt;strong&gt;but it must keep all values with a higher version number&lt;/strong&gt; (because those values are concurrent with the incoming write).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;When a write includes the version number from a prior read, that tells us which previous state the write is based on.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Version vectors&lt;/h4&gt;

&lt;p&gt;How does the algorithm change when there are multiple replicas, but no leader?&lt;/p&gt;

&lt;p&gt;Previous algorithm uses a single version number to capture dependencies between operations, but that is not sufficient when there are multiple replicas accepting writes concurrently. &lt;strong&gt;Instead, we need to use a version number per replica as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas.&lt;/strong&gt; This information indicates which values to overwrite and which values to keep as siblings.&lt;/p&gt;

&lt;p&gt;The collection of version numbers from all the replicas is called a &lt;strong&gt;version vector&lt;/strong&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 12 Sep 2018 22:03:19 +0800</pubDate>
        <link>http://masutangu.com/2018/09/data-intensive-note-2/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/09/data-intensive-note-2/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>Designing Data-Intensive Applications 读书笔记（一）</title>
        <description>&lt;h1&gt;Chapter 1. Reliable, Scalable, and Maintainable Applications&lt;/h1&gt;

&lt;h2&gt;Thinking About Data Systems&lt;/h2&gt;

&lt;p&gt;Increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and storage needs. Instead, &lt;strong&gt;the work is broken down into tasks that can be performed efficiently on a single tool, and those different tools are stitched together using application code.（在应用层代码整合多种不同的工具来实现需求）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In this book, we focus on three concerns that are important in most software systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reliability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Maintainability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Over time, many different people will work on the system (engineering and operations, both maintaining current behavior and adapting the system to new use cases), and they should all be able to work on it productively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Reliability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Continuing to work correctly, even when things go wrong.&lt;/strong&gt; A fault is not the same as a failure. A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. &lt;strong&gt;It is impossible to reduce the probability of a fault to zero; therefore it is usually best to design fault-tolerance mechanisms that prevent faults from causing failures.（实现 fault－tolerance 以避免 faults 引发 failures）&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Scalability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Scalability is the term we use to describe a system’s ability to cope with increased load.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Describing Load&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Load can be described with a few numbers which we call load parameters.&lt;/strong&gt; The best choice of parameters depends on the architecture of your system: it may be requests per second to a web server, the ratio of reads to writes in a database, the number of simultaneously active users in a chat room, the hit rate on a cache, or something else.&lt;/p&gt;

&lt;h3&gt;Approaches for Coping with Load&lt;/h3&gt;

&lt;p&gt;People often talk of a dichotomy between &lt;strong&gt;scaling up (vertical scaling, moving to a more powerful machine)&lt;/strong&gt; and &lt;strong&gt;scaling out (horizontal scaling, distributing the load across multiple smaller machines)&lt;/strong&gt;. Distributing load across multiple machines is also known as a &lt;strong&gt;shared-nothing architecture&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An architecture that scales well for a particular application is built around &lt;strong&gt;assumptions of which operations will be common and which will be rare—the load parameters（扩展性良好的系统是基于 load parameter 来构建的）&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Maintainability&lt;/h2&gt;

&lt;h3&gt;Operability: Making Life Easy for Operations&lt;/h3&gt;

&lt;p&gt;Make it easy for operations teams to keep the system running smoothly.&lt;/p&gt;

&lt;h3&gt;Simplicity:Managing Complexity&lt;/h3&gt;

&lt;p&gt;Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system. (Note this is not the same as simplicity of the user interface.) &lt;strong&gt;One of the best tools we have for removing accidental complexity is abstraction.&lt;/strong&gt; A good abstraction can hide a great deal of implementation detail behind a clean, simple-to-understand façade. &lt;strong&gt;SQL is an abstraction that hides complex on-disk and in-memory data structures.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Evolvability: Making Change Easy&lt;/h3&gt;

&lt;p&gt;Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity.&lt;/p&gt;

&lt;h1&gt;Chapter 2. Data Models and Query Languages&lt;/h1&gt;

&lt;p&gt;Data models are perhaps the most important part of developing software, because they have such a profound effect: not only on how the software is written, &lt;strong&gt;but also on how we think about the problem that we are solving&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Most applications are built by layering one data model on top of another. For each layer, &lt;strong&gt;the key question is: how is it represented in terms of the next-lower layer?&lt;/strong&gt; In a complex application there may be more intermediary levels, such as APIs built upon APIs, but the basic idea is still the same: &lt;strong&gt;each layer hides the complexity of the layers below it by providing a clean data model&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Relational Model Versus Document Model&lt;/h2&gt;

&lt;p&gt;The best-known data model today is probably that of SQL, data is organized into relations (called tables in SQL), where each relation is an unordered collection of tuples (rows in SQL).&lt;/p&gt;

&lt;h3&gt;The Birth of NoSQL&lt;/h3&gt;

&lt;p&gt;There are several driving forces behind the adoption of NoSQL databases, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A need for greater scalability than relational databases can easily achieve, including very large datasets or very high write throughput&lt;/li&gt;
&lt;li&gt;A widespread preference for free and open source software over commercial database products&lt;/li&gt;
&lt;li&gt;Specialized query operations that are not well supported by the relational model&lt;/li&gt;
&lt;li&gt;Frustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Relational Versus Document Databases Today&lt;/h3&gt;

&lt;p&gt;The main arguments in favor of the document data model are &lt;strong&gt;schema flexibility, better performance due to locality, and that for some applications it is closer to the data structures used by the application&lt;/strong&gt;.（&lt;em&gt;不需要和关系型数据库一样使用 ORM 做转换：If data is stored in relational tables, an awkward translation layer is required between the objects in the application code and the database model of tables, rows, and columns. The disconnect between the models is sometimes called an&lt;/em&gt; &lt;strong&gt;impedance mismatch&lt;/strong&gt;）The relational model counters by providing better support for &lt;strong&gt;joins, and many-to-one and many-to-many relationships&lt;/strong&gt;.&lt;/p&gt;

&lt;h4&gt;Which data model leads to simpler application code?&lt;/h4&gt;

&lt;p&gt;If the data in your application has a document-like structure, then it’s probably a good idea to use a document model. &lt;strong&gt;However, if your application does use many-to-many relationships, the document model becomes less appealing. It’s possible to reduce the need for joins by denormalizing, but then the application code needs to do additional work to keep the denormalized data consistent.&lt;/strong&gt; Joins can be emulated in application code by making multiple requests to the database, but that also moves complexity into the application and is usually slower than a join performed by specialized code inside the database. &lt;strong&gt;In such cases, using a document model can lead to significantly more complex application
code and worse performance.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Schema flexibility in the document model&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Document databases are sometimes called schemaless, there is an implicit schema, but it is not enforced by the database.&lt;/strong&gt; A more accurate term is &lt;strong&gt;schema-on-read&lt;/strong&gt; (the structure of the data is implicit, and only interpreted when the data is read), in contrast with &lt;strong&gt;schema-on-write&lt;/strong&gt; (the traditional approach of relational databases, where the schema is explicit and the database ensures all written data conforms to it).&lt;/p&gt;

&lt;h4&gt;Data locality for queries&lt;/h4&gt;

&lt;p&gt;A document is usually stored as a single continuous string, encoded as JSON, XML, or a binary variant thereof. If your application often needs to access the entire document, there is a performance advantage to this &lt;strong&gt;storage locality&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The locality advantage only applies if you need large parts of the document at the same time. On updates to a document, the entire document usually needs to be rewritten—only modifications that don’t change the encoded size of a document can easily be performed in place. For these reasons, it is generally recommended that you keep documents fairly small and &lt;strong&gt;avoid writes that increase the size of a document&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Query Languages for Data&lt;/h2&gt;

&lt;p&gt;When the relational model was introduced, it included a new way of querying data: &lt;strong&gt;SQL is a declarative query language, whereas IMS and CODASYL queried the database using imperative code&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In a declarative query language, like SQL or relational algebra, you just specify the pattern of the data you want, but not how to achieve that goal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Declarative languages often lend themselves to parallel execution. Imperative code is very hard to parallelize across multiple cores and multiple machines&lt;/strong&gt;, because it specifies instructions that must be performed in a particular order.&lt;/p&gt;

&lt;h2&gt;Graph-Like Data Models&lt;/h2&gt;

&lt;p&gt;The relational model can handle simple cases of many-to-many relationships, but as the connections within your data become more complex, it becomes more natural to start modeling your data as a graph.&lt;/p&gt;

&lt;h3&gt;Property Graphs&lt;/h3&gt;

&lt;p&gt;In the property graph model, each vertex consists of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A unique identifier&lt;/li&gt;
&lt;li&gt;A set of outgoing edges&lt;/li&gt;
&lt;li&gt;A set of incoming edges&lt;/li&gt;
&lt;li&gt;A collection of properties (key-value pairs)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each edge consists of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A unique identifier&lt;/li&gt;
&lt;li&gt;The vertex at which the edge starts (the tail vertex)&lt;/li&gt;
&lt;li&gt;The vertex at which the edge ends (the head vertex)&lt;/li&gt;
&lt;li&gt;A label to describe the kind of relationship between the two vertices&lt;/li&gt;
&lt;li&gt;A collection of properties (key-value pairs)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Graphs are good for evolvability: as you add features to your application, a graph can easily be extended to accommodate changes in your application’s data structures.&lt;/strong&gt;&lt;/p&gt;

&lt;h1&gt;Chapter 3. Storage and Retrieval&lt;/h1&gt;

&lt;p&gt;We will examine two families of storage engines: &lt;strong&gt;log-structured storage engines, and page-oriented storage engines such as B-trees&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Data Structures That Power Your Database&lt;/h2&gt;

&lt;p&gt;In order to efficiently find the value for a particular key in the database, we need a different data structure: an index. &lt;/p&gt;

&lt;h3&gt;Hash Indexes&lt;/h3&gt;

&lt;p&gt;Let’s say our data storage consists only of appending to a file. Then the simplest possible indexing strategy is this: &lt;strong&gt;keep an in-memory hash map where every key is mapped to a byte offset in the data file&lt;/strong&gt;—the location at which the value can be found. This is essentially what Bitcask (the default storage engine in Riak) does&lt;/p&gt;

&lt;p&gt;A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. For example, the key might be the URL of a cat video, and the value might be the number of times it has been played. &lt;strong&gt;In this kind of workload, there are a lot of writes, but there are not too many distinct keys—you have a large number of writes per key, but it’s feasible to keep all keys in memory.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;How do we avoid eventually running out of disk space? &lt;strong&gt;A good solution is to break the log into segments of a certain size by closing a segment file when it reaches a certain size, and making subsequent writes to a new segment file.&lt;/strong&gt; We can then perform compaction on these segments. &lt;strong&gt;Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Each segment now has its own in-memory hash table, mapping keys to file offsets.&lt;/strong&gt; In order to find the value for a key, we first check the most recent segment’s hash map; if the key is not present we check the second-most-recent segment, and so on.&lt;/p&gt;

&lt;p&gt;An append-only design turns out to be good for several reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Concurrency and crash recovery are much simpler if segment files are append-only or immutable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Merging old segments avoids the problem of data files getting fragmented over time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the hash table index also has limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The hash table must fit in memory, so if you have a very large number of keys, you’re out of luck.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Range queries are not efficient. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;SSTables and LSM-Trees&lt;/h3&gt;

&lt;p&gt;Now we can make a simple change to the format of our segment files: we require that the sequence of key-value pairs is sorted by key. &lt;/p&gt;

&lt;p&gt;We call this format &lt;strong&gt;Sorted String Table, or SSTable&lt;/strong&gt; for short. We also require that each key only appears once within each merged segment file (the compaction process already ensures that). SSTables have several big advantages over log segments with hash indexes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Merging segments is simple and efficient&lt;/strong&gt;, even if the files are bigger than the available memory. The approach is like the one used in the mergesort algorithm.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to find a particular key in the file, you no longer need to keep an index of all the keys in memory. You still need an in-memory index to tell you the offsets for some of the keys, but &lt;strong&gt;it can be sparse&lt;/strong&gt;: one key for every few kilobytes of segment file is sufficient, because a few kilobytes can be scanned very quickly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-2.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Since read requests need to scan over several key-value pairs in the requested range anyway, &lt;strong&gt;it is possible to group those records into a block and compress it before writing it to disk&lt;/strong&gt;. Each entry of the sparse in-memory index then points at the start of a compressed block. Besides saving disk space, compression also reduces the I/O bandwidth use.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Constructing and maintaining SSTables&lt;/h4&gt;

&lt;p&gt;How do you get your data to be sorted by key in the first place? We can now make our storage engine work as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When a write comes in, add it to an in-memory balanced tree data structure.&lt;/li&gt;
&lt;li&gt;When the memtable gets bigger than some threshold—typically a few megabytes—write it out to disk as an SSTable file.&lt;/li&gt;
&lt;li&gt;In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.&lt;/li&gt;
&lt;li&gt;From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It only suffers from one problem: if the database crashes, the most recent writes (which are in the memtable but not yet written out to disk) are lost. In order to avoid that problem, we can &lt;strong&gt;keep a separate log on disk to which every write is immediately appended&lt;/strong&gt;, just like in the previous section. That log is not in sorted order, but that doesn’t matter, because &lt;strong&gt;its only purpose is to restore the memtable after a crash&lt;/strong&gt;. &lt;/p&gt;

&lt;h4&gt;Performance optimizations&lt;/h4&gt;

&lt;p&gt;The LSM-tree algorithm can be slow when looking up keys that do not exist in the database: you have to check the memtable, then the segments all the way back to the oldest before you can be sure that the key does not exist. &lt;strong&gt;In order to optimize this kind of access, storage engines often use additional Bloom filters.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are also different strategies to determine the order and timing of how SSTables are compacted and merged. The most common options are &lt;strong&gt;size-tiered and leveled compaction&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;B-Trees&lt;/h3&gt;

&lt;p&gt;The most widely used indexing structure is quite different: the B-tree.&lt;/p&gt;

&lt;p&gt;Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key- value lookups and range queries. &lt;/p&gt;

&lt;p&gt;The log-structured indexes we saw earlier break the database down into variable-size segments, typically several megabytes or more in size, and always write a segment sequentially. &lt;strong&gt;By contrast, B-trees break the database down into fixed-size blocks or pages&lt;/strong&gt;, traditionally 4 KB in size (sometimes bigger), and &lt;strong&gt;read or write one page at a time&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Each page can be identified using an address or location&lt;/strong&gt;, which allows one page to refer to another—similar to a pointer, but on disk instead of in memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-3.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;One page is designated as the root of the B-tree; whenever you want to look up a key in the index, you start here. The page contains several keys and references to child pages. Each child is responsible for a continuous range of keys, and &lt;strong&gt;the keys between the references indicate where the boundaries between those ranges lie&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The number of references to child pages in one page of the B-tree is called the &lt;strong&gt;branching factor&lt;/strong&gt;. If you want to add a new key, you need to find the page whose range encompasses the new key and add it to that page. If there isn’t enough free space in the page to accommodate the new key, it is split into two half-full pages, and the parent page is updated to account for the new subdivision of key ranges.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-4.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;This algorithm ensures that the tree remains balanced: a B-tree with n keys always has a depth of O(log n). Most databases can fit into a B-tree that is three or four levels deep, so you don’t need to follow many page references to find the page you are look‐ ing for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.)&lt;/p&gt;

&lt;h4&gt;Making B-trees reliable&lt;/h4&gt;

&lt;p&gt;If you split a page because an insertion caused it to be overfull, you need to write the two pages that were split, and also overwrite their parent page to update the references to the two child pages. &lt;strong&gt;This is a dangerous operation, because if the database crashes after only some of the pages have been written, you end up with a corrupted index.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to make the database resilient to crashes, &lt;strong&gt;it is common for B-tree implementations to include an additional data structure on disk: a write-ahead log (WAL, also known as a redo log)&lt;/strong&gt;.  This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself. When the database comes back up after a crash, this log is used to restore the B-tree back to a consistent state.&lt;/p&gt;

&lt;p&gt;An additional complication of updating pages in place is that careful concurrency control is required if multiple threads are going to access the B-tree at the same time. &lt;strong&gt;This is typically done by protecting the tree’s data structures with latches (lightweight locks).&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;B-tree optimizations&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Instead of overwriting pages and maintaining a WAL for crash recovery, some databases (like LMDB) use a copy-on-write scheme. A modified page is written to a different location, and a new version of the parent pages in the tree is created, pointing at the new location.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can save space in pages by not storing the entire key, but abbreviating it. Especially in pages on the interior of the tree, keys only need to provide enough information to act as boundaries between key ranges. Packing more keys into a page allows the tree to have a higher branching factor, and thus fewer levels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additional pointers have been added to the tree. For example, each leaf page may have references to its sibling pages to the left and right, which allows scanning keys in order without jumping back to parent pages.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Comparing B-Trees and LSM-Trees&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;As a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Advantages of LSM-trees&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;A B-tree index must write every piece of data at least twice: once to the write-ahead log, and once to the tree page itself (and perhaps again as pages are split).&lt;/strong&gt; There is also overhead from having to write an entire page at a time, even if only a few bytes in that page changed.&lt;/p&gt;

&lt;p&gt;Log-structured indexes also rewrite data multiple times due to repeated compaction and merging of SSTables. &lt;/p&gt;

&lt;p&gt;In write-heavy applications, the performance bottleneck might be the rate at which the database can write to disk.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moreover, LSM-trees are typically able to sustain higher write throughput than B-trees, partly because they sometimes have lower write amplification, and partly because they sequentially write compact SSTable files rather than having to overwrite several pages in the tree.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since LSM-trees are not page-oriented and periodically rewrite SSTables to remove fragmentation, they have lower storage overheads, especially when using leveled compaction.&lt;/p&gt;

&lt;h4&gt;Downsides of LSM-trees&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;A downside of log-structured storage is that the compaction process can sometimes interfere with the performance of ongoing reads and writes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An advantage of B-trees is that each key exists in exactly one place in the index&lt;/strong&gt;, whereas a log-structured storage engine may have multiple copies of the same key in different segments. This aspect makes B-trees attractive in databases that want to offer strong transactional semantics: &lt;strong&gt;in many relational databases, transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Other Indexing Structures&lt;/h3&gt;

&lt;p&gt;A secondary index can easily be constructed from a key-value index. The main difference is that keys are not unique. This can be solved in two ways: &lt;strong&gt;either by making each value in the index a list of matching row identifiers (like a postings list in a full-text index) or by making each key unique by appending a row identifier to it.&lt;/strong&gt; &lt;/p&gt;

&lt;h4&gt;Storing values within the index&lt;/h4&gt;

&lt;p&gt;The key in an index is the thing that queries search for, but the value can be one of two things: it could be the actual row (document, vertex) in question, or it could be a reference to the row stored elsewhere. In the latter case, the place where rows are stored is known as a &lt;strong&gt;heap file&lt;/strong&gt;. &lt;strong&gt;The heap file approach is common because it avoids duplicating data when multiple secondary indexes are present: each index just references a location in the heap file, and the actual data is kept in one place.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In some situations, the extra hop from the index to the heap file is too much of a performance penalty for reads, so it can be desirable to store the indexed row directly within an index. This is known as a &lt;strong&gt;clustered index&lt;/strong&gt;. For example, in MySQL’s InnoDB storage engine, the primary key of a table is always a clustered index, and secondary indexes refer to the primary key (rather than a heap file location)（每一 个 Secondary Index 包含主健，再根据主键索引找到相应的数据）&lt;/p&gt;

&lt;h4&gt;Multi-column indexes&lt;/h4&gt;

&lt;p&gt;The most common type of multi-column index is called a &lt;strong&gt;concatenated index&lt;/strong&gt;, which simply combines several fields into one key by appending one column to another(the index definition specifies in which order the fields are concatenated).&lt;/p&gt;

&lt;p&gt;Multi-dimensional indexes are a more general way of querying several columns at once, which is particularly important for geospatial data. One option is to translate a two-dimensional location into a single number using a space-filling curve, and then to use a regular B-tree index. &lt;strong&gt;More commonly, specialized spatial indexes such as R-trees are used.&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;Full-text search and fuzzy indexes&lt;/h4&gt;

&lt;p&gt;To cope with typos in documents or queries, Lucene is able to search text for words within a certain edit distance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lucene uses a SSTable-like structure for its term dictionary.&lt;/strong&gt; This structure requires a small in- memory index that tells queries at which offset in the sorted file they need to look for a key. In LevelDB, this in-memory index is a sparse collection of some of the keys, but &lt;strong&gt;in Lucene, the in-memory index is a finite state automaton over the characters in the keys, similar to a trie&lt;/strong&gt;. This automaton can be transformed into a Levenshtein automaton, which supports efficient search for words within a given edit distance.&lt;/p&gt;

&lt;h4&gt;Keeping everything in memory&lt;/h4&gt;

&lt;p&gt;Counterintuitively, the performance advantage of in-memory databases is not due to the fact that they don’t need to read from disk. Rather, &lt;strong&gt;they can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk.&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Transaction Processing or Analytics?&lt;/h2&gt;

&lt;h3&gt;Data Warehousing&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;A data warehouse is a separate database that analysts can query to their hearts’ content, without affecting OLTP operations.&lt;/strong&gt; The data warehouse contains a read-only copy of the data in all the various OLTP systems in the company. Data is extracted from OLTP databases (using either a periodic data dump or a continuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse. This process of getting data into the warehouse is known as Extract–Transform–Load (ETL).&lt;/p&gt;

&lt;p&gt;A big advantage of using a separate data warehouse, rather than querying OLTP systems directly for analytics, is that the data warehouse can be optimized for analytic access patterns. It turns out that the indexing algorithms discussed in the first half of this chapter work well for OLTP, but are not very good at answering analytic queries.&lt;/p&gt;

&lt;h4&gt;The divergence between OLTP databases and data warehouses&lt;/h4&gt;

&lt;p&gt;The data model of a data warehouse is most commonly relational, because SQL is generally a good fit for analytic queries. &lt;/p&gt;

&lt;h3&gt;Stars and Snowflakes: Schemas for Analytics&lt;/h3&gt;

&lt;p&gt;A wide range of different data models are used in the realm of transaction processing, depending on the needs of the application. On the other hand, &lt;strong&gt;in analytics, there is much less diversity of data models. Many data warehouses are used in a fairly formulaic style, known as a star schema&lt;/strong&gt; (also known as dimensional modeling).&lt;/p&gt;

&lt;h2&gt;Column-Oriented Storage&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;In most OLTP databases, storage is laid out in a row-oriented fashion&lt;/strong&gt;: all the values from one row of a table are stored next to each other. Document databases are similar: an entire document is typically stored as one contiguous sequence of bytes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-intensive-note/illustration-5.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;h3&gt;Column Compression&lt;/h3&gt;

&lt;p&gt;Depending on the data in the column, different compression techniques can be used. One technique that is particularly effective in data warehouses is bitmap encoding.&lt;/p&gt;

&lt;h3&gt;Sort Order in Column Storage&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Another advantage of sorted order is that it can help with compression of columns.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Writing to Column-Oriented Storage&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Column-oriented storage, compression, and sorting all help to make those read queries faster.&lt;/strong&gt; However, they have the downside of making writes more difficult.&lt;/p&gt;

&lt;p&gt;An update-in-place approach, like B-trees use, is not possible with compressed columns. If you wanted to insert a row in the middle of a sorted table, you would most likely have to rewrite all the column files.&lt;/p&gt;

&lt;p&gt;Fortunately, we have already seen a good solution earlier in this chapter: LSM-trees. All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn’t matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written to new files in bulk. &lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;On a high level, we saw that storage engines fall into two broad categories: those optimized for transaction processing (OLTP), and those optimized for analytics (OLAP). There are big differences between the access patterns in those use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;OLTP systems are typically user-facing, which means that they may see a huge volume of requests. &lt;strong&gt;Disk seek time is often the bottleneck here.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data warehouses and similar analytic systems are less well known, because they are primarily used by business analysts, not by end users. &lt;strong&gt;Disk bandwidth (not seek time) is often the bottleneck here&lt;/strong&gt;, and column-oriented storage is an increasingly popular solution for this kind of workload.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the OLTP side, we saw storage engines from two main schools of thought:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The log-structured school, which only permits appending to files and deleting obsolete files, but never updates a file that has been written. Bitcask, SSTables, LSM-trees, LevelDB, Cassandra, HBase, Lucene, and others belong to this group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The update-in-place school, which treats the disk as a set of fixed-size pages that can be overwritten. B-trees are the biggest example of this philosophy, being used in all major relational databases and also many nonrelational ones.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Analytic workloads are so different from OLTP: when your queries require sequentially scanning across a large number of rows, indexes are much less relevant. Instead it becomes important to encode data very compactly, to minimize the amount of data that the query needs to read from disk.&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jul 2018 18:57:23 +0800</pubDate>
        <link>http://masutangu.com/2018/07/data-intensive-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/data-intensive-note-1/</guid>
        
        
        <category>读书笔记</category>
        
      </item>
    
      <item>
        <title>TiKV 源码阅读（未完成）</title>
        <description>&lt;p&gt;TiKV 使用 RocksDB 做持久化存储引擎。将 key 分 range，每一段称为 Region。Region 分散在多台机器上以实现存储的水平扩展。每个 Region 会存放多个副本在不同机器上，使用 raft 算法管理：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tikv-note-1/illustration-1.png&quot; width=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;PD 负责整个 TiKV 集群的调度。&lt;/p&gt;

&lt;p&gt;TiKV 使用 version 的方式进行多版本控制（MVCC）：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Key1-Version3 -&amp;gt; Value
Key1-Version2 -&amp;gt; Value
Key1-Version1 -&amp;gt; Value
...
Key2-Version2 -&amp;gt; Value
Key2-Version1 -&amp;gt; Value
...
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;TiKV 事务使用 &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36726.pdf&quot;&gt;Percolator&lt;/a&gt; 模型。采用乐观锁，事务提交才进行冲突检测。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24564094&quot;&gt;https://zhuanlan.zhihu.com/p/24564094&lt;/a&gt;
&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-internal-1/&quot;&gt;https://pingcap.com/blog-cn/tidb-internal-1/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Jul 2018 21:59:16 +0800</pubDate>
        <link>http://masutangu.com/2018/07/tikv-note-1/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/tikv-note-1/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>raft-rust 初体验</title>
        <description>&lt;p&gt;之前分析了使用 golang 实现的 etcd-raft，这几天再读了下 rust 实现的 &lt;a href=&quot;https://github.com/pingcap/raft-rs&quot;&gt;raft-rs&lt;/a&gt;，简单说下对比。rust 版应该是基于 golang 版来实现的，所有的类、方法基本上是一致的。&lt;/p&gt;

&lt;p&gt;从样例看起，&lt;code&gt;let (sender, receiver) = mpsc::channel();&lt;/code&gt; 创建了 channel 用于线程之间数据传递（类似 golang 的 channel）。调用 &lt;code&gt;send_propose&lt;/code&gt; 创建一个线程，通过 &lt;code&gt;sender&lt;/code&gt; 发送 propose 请求。&lt;code&gt;main&lt;/code&gt; 主线程则在 loop 循环中监听 &lt;code&gt;receiver&lt;/code&gt; channel 的请求。如果是 propose 请求，调用 RawNode 的 &lt;code&gt;propose&lt;/code&gt; 方法处理（其内部调用 &lt;code&gt;self.raft.step&lt;/code&gt; 方法），如果是其他请求，直接调用 RawNode 的 &lt;code&gt;step&lt;/code&gt; 方法处理（其内部也是调用 &lt;code&gt;self.raft.step&lt;/code&gt; 方法）。loop 循环最后调用 &lt;code&gt;on_ready&lt;/code&gt;，处理 raft 层返回的 ready 对象，这个逻辑和之前 golang 实现的 etcd-raft 是很类似的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;// A simple example about how to use the Raft library in Rust.
fn main() {
    // Create a storage for Raft, and here we just use a simple memory storage.
    // You need to build your own persistent storage in your production.
    // Please check the Storage trait in src/storage.rs to see how to implement one.
    let storage = MemStorage::new();

    // Create the configuration for the Raft node.
    let cfg = Config {
        ..Default::default()
    };

    // Create the Raft node.
    let mut r = RawNode::new(&amp;amp;cfg, storage, vec![]).unwrap();

    let (sender, receiver) = mpsc::channel();

    // Use another thread to propose a Raft request.
    send_propose(sender);

    // Loop forever to drive the Raft.
    let mut t = Instant::now();
    let mut timeout = Duration::from_millis(100);

    // Use a HashMap to hold the `propose` callbacks.
    let mut cbs = HashMap::new();

    loop {
        match receiver.recv_timeout(timeout) {
            Ok(Msg::Propose { id, cb }) =&amp;gt; {
                cbs.insert(id, cb);
                r.propose(vec![], vec![id]).unwrap();
            }
            Ok(Msg::Raft(m)) =&amp;gt; r.step(m).unwrap(),
            Err(RecvTimeoutError::Timeout) =&amp;gt; (),
            Err(RecvTimeoutError::Disconnected) =&amp;gt; return,
        }

        let d = t.elapsed();
        if d &amp;gt;= timeout {
            t = Instant::now();
            timeout = Duration::from_millis(100);
            // We drive Raft every 100ms.
            r.tick();
        } else {
            timeout -= d;
        }

        on_ready(&amp;amp;mut r, &amp;amp;mut cbs);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;留意这里，往 sender 发了个包在 Box 里的闭包 &lt;code&gt;s1.send(0).unwrap()&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;fn send_propose(sender: mpsc::Sender&amp;lt;Msg&amp;gt;) {
    thread::spawn(move || {
        // Wait some time and send the request to the Raft.
        thread::sleep(Duration::from_secs(10));

        let (s1, r1) = mpsc::channel::&amp;lt;u8&amp;gt;();

        println!(&amp;quot;propose a request&amp;quot;);

        // Send a command to the Raft, wait for the Raft to apply it
        // and get the result.
        sender
            .send(Msg::Propose {
                id: 1,
                // cb 为 closure
                cb: Box::new(move || {
                    s1.send(0).unwrap();
                }),
            })
            .unwrap();

        // 当该 propose 请求被处理时，会调用 cb，往 s1 send 值，于是 r1 的 recv 会返回
        let n = r1.recv().unwrap();
        assert_eq!(n, 0);

        println!(&amp;quot;receive the propose callback&amp;quot;);
    });
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;fn on_ready(r: &amp;amp;mut RawNode&amp;lt;MemStorage&amp;gt;, cbs: &amp;amp;mut HashMap&amp;lt;u8, ProposeCallback&amp;gt;) {
    let mut ready = r.ready(); // 调用 ready 方法，拿到 ready 对象
    // 一波处理 忽略，处理完后也是调用了 advance 方法
    ...
    // Advance the Raft
    r.advance(ready);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;和 golang 版最大的区别，是 golang 用了 channel 来做进行模块之间的数据通信：&lt;code&gt;recvc&lt;/code&gt; channel 收发请求，&lt;code&gt;advancec&lt;/code&gt; channel 收发 advance 消息，&lt;code&gt;readyc&lt;/code&gt; channel 收发 ready 对象。而 rust 中则直接调用 &lt;code&gt;step&lt;/code&gt;、&lt;code&gt;propose&lt;/code&gt;、&lt;code&gt;ready&lt;/code&gt; 和 &lt;code&gt;advance&lt;/code&gt; 方法来驱动状态机，没有通过 rust 的 channel 机制做消息传递。也许是考虑到效率？由于 raft-rs 提供的例子有点简单，等之后读 tikv 的代码，再来做下一步的对比。&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Jul 2018 21:59:16 +0800</pubDate>
        <link>http://masutangu.com/2018/07/raft-rust/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/raft-rust/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（PreVote）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 的 PreVote 机制，避免由于网络分区导致 candidate 的 term 不断增大。&lt;/p&gt;

&lt;p&gt;Election timeout 之后，发送 type 为 pb.MsgHup 的请求，进入选举阶段：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// tickElection is run by followers and candidates after r.electionTimeout.
func (r *raft) tickElection() {
    r.electionElapsed++

    if r.promotable() &amp;amp;&amp;amp; r.pastElectionTimeout() {
        r.electionElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgHup})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;如果设置了 preVote 为 true，则先进入 prevote 阶段。调用 &lt;code&gt;r.campaign&lt;/code&gt; 传入 type &lt;code&gt;campaignPreElection&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) Step(m pb.Message) error {
    switch m.Type {
    case pb.MsgHup:
        if r.state != StateLeader {
            r.logger.Infof(&amp;quot;%x is starting a new election at term %d&amp;quot;, r.id, r.Term)
            if r.preVote {
                // 如果 preVote 设置为 true，先发起 campaignPreElection
                r.campaign(campaignPreElection)
            } else {
                r.campaign(campaignElection)
            }
        } else {
            r.logger.Debugf(&amp;quot;%x ignoring MsgHup because already leader&amp;quot;, r.id)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;campaign&lt;/code&gt; 方法处理选举逻辑。如果是 &lt;code&gt;campaignPreElection&lt;/code&gt;，设置节点状态为 &lt;code&gt;StatePreCandidate&lt;/code&gt;，此时不会递增节点的 Term（避免 term 增长过快）。然后向其他 peers 发送 type 为 &lt;code&gt;pb.MsgPreVote&lt;/code&gt; 的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) campaign(t CampaignType) {
    var term uint64
    var voteMsg pb.MessageType
    if t == campaignPreElection {
        r.becomePreCandidate()  // state 设置为 StatePreCandidate
        voteMsg = pb.MsgPreVote  // msg type 设置为 preVote
        // PreVote RPCs are sent for the next term before we&amp;#39;ve incremented r.Term.
        term = r.Term + 1  // preVote 不会递增 r.Term
    } else {
        ...
    }
    if r.quorum() == r.poll(r.id, voteRespMsgType(voteMsg), true) {
        // We won the election after voting for ourselves (which must mean that
        // this is a single-node cluster). Advance to the next state.
        if t == campaignPreElection {
            r.campaign(campaignElection)  // prevote 成功，可以发起 campaignElection 了
        } else {
            ... 
        }
        return
    }

    // 广播 pb.MsgPreVote
    for id := range r.prs {
        if id == r.id {
            continue
        }
        var ctx []byte
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;看看 &lt;code&gt;stepCandidate&lt;/code&gt; 如何处理 &lt;code&gt;pb.MsgPreVote&lt;/code&gt; 请求的回包。检查选票是否达到 quorum 数量，如果已经达到，prevote 成功，可以发起真正的选举了，调用 &lt;code&gt;r.campaign&lt;/code&gt; 传入 type &lt;code&gt;campaignElection&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// stepCandidate is shared by StateCandidate and StatePreCandidate; the difference is
// whether they respond to MsgVoteResp or MsgPreVoteResp.
func stepCandidate(r *raft, m pb.Message) error {
    // Only handle vote responses corresponding to our candidacy (while in
    // StateCandidate, we may get stale MsgPreVoteResp messages in this term from
    // our pre-candidate state).
    var myVoteRespType pb.MessageType
    if r.state == StatePreCandidate {
        myVoteRespType = pb.MsgPreVoteResp
    } else {
        myVoteRespType = pb.MsgVoteResp
    }
    switch m.Type {
    case myVoteRespType:
        gr := r.poll(m.From, m.Type, !m.Reject)
        switch r.quorum() {
        case gr:
            if r.state == StatePreCandidate {
                r.campaign(campaignElection)  // prevote 成功，可以发起 campaignElection
            } else {
                ...
            }
        case len(r.votes) - gr:  // prevote 失败（m.Reject 为 true，此时 m.Term &amp;gt; r.Term），转为 follower 角色
            // pb.MsgPreVoteResp contains future term of pre-candidate
            // m.Term &amp;gt; r.Term; reuse r.Term
            r.becomeFollower(r.Term, None)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;如果 campaign 类型为 &lt;code&gt;campaignElection&lt;/code&gt;，则调用 &lt;code&gt;r.becomeCandidate&lt;/code&gt;，此时设置节点状态为 &lt;code&gt;StateCandidate&lt;/code&gt;，递增节点的 Term，并向其他 peers 发送 &lt;code&gt;pb.MsgVote&lt;/code&gt; 请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) campaign(t CampaignType) {
    var term uint64
    var voteMsg pb.MessageType
    if t == campaignPreElection {
        ...
    } else {
        r.becomeCandidate()  // become cancdidate，term 递增
        voteMsg = pb.MsgVote
        term = r.Term
    }
    if r.quorum() == r.poll(r.id, voteRespMsgType(voteMsg), true) {
        // We won the election after voting for ourselves (which must mean that
        // this is a single-node cluster). Advance to the next state.
        if t == campaignPreElection {
            ...
        } else {
            r.becomeLeader()  // 得到 quorum 的选票，选举 leader 成功，become leader
        }
        return
    }

    // 广播 pb.MsgVote，进行选举
    for id := range r.prs {
        if id == r.id {
            continue
        }
        var ctx []byte
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;收到 &lt;code&gt;pb.MsgVote&lt;/code&gt; 的回包后，同样检查是否选票数量是否达到 quorum，成功则该节点当选 leader：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepCandidate(r *raft, m pb.Message) error {
    // Only handle vote responses corresponding to our candidacy (while in
    // StateCandidate, we may get stale MsgPreVoteResp messages in this term from
    // our pre-candidate state).
    var myVoteRespType pb.MessageType
    if r.state == StatePreCandidate {
        myVoteRespType = pb.MsgPreVoteResp
    } else {
        myVoteRespType = pb.MsgVoteResp
    }
    switch m.Type {
    case myVoteRespType:
        gr := r.poll(m.From, m.Type, !m.Reject)
        r.logger.Infof(&amp;quot;%x [quorum:%d] has received %d %s votes and %d vote rejections&amp;quot;, r.id, r.quorum(), gr, m.Type, len(r.votes)-gr)
        switch r.quorum() {
        case gr:
            if r.state == StatePreCandidate {
                ...
            } else {
                r.becomeLeader() // 收到 quorum 选票，选举成功
                r.bcastAppend()  // 广播 append 消息
            }
        case len(r.votes) - gr:
            // pb.MsgPreVoteResp contains future term of pre-candidate
            // m.Term &amp;gt; r.Term; reuse r.Term
            r.becomeFollower(r.Term, None)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Sun, 08 Jul 2018 10:19:08 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-6/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-6/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（Leader Transfer）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 如何实现 leadership transfer，把 leader 身份转移给某个 follower。&lt;/p&gt;

&lt;p&gt;应用层调用 &lt;code&gt;TransferLeadership&lt;/code&gt; 方法，发送一个 type 为 pb.MsgTransferLeader 的请求给 raft 处理。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (n *node) TransferLeadership(ctx context.Context, lead, transferee uint64) {
    select {
    // manually set &amp;#39;from&amp;#39; and &amp;#39;to&amp;#39;, so that leader can voluntarily transfers its leadership
    case n.recvc &amp;lt;- pb.Message{Type: pb.MsgTransferLeader, From: transferee, To: lead}:
    case &amp;lt;-n.done:
    case &amp;lt;-ctx.Done():
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;stepLeader&lt;/code&gt; 收到 pb.MsgTransferLeader 后，检查下是否有正在进行的 leader transfer，并检查 tranferee 的 log 是否是最新的，如果是，调用 &lt;code&gt;sendTimeoutNow&lt;/code&gt;，如果不是最新日志，则发送 appendEntriesReq，收到 MsgAppResp 后，如果条件符合，再调用 &lt;code&gt;sendTimeoutNow&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // All other message types require a progress for m.From (pr).
    pr := r.getProgress(m.From)
    // These message types do not require any progress for m.From.
    switch m.Type {
        case pb.MsgTransferLeader:
        leadTransferee := m.From
        lastLeadTransferee := r.leadTransferee
        if lastLeadTransferee != None {
            if lastLeadTransferee == leadTransferee {
                return nil
            }
            // 取消之前的
            r.abortLeaderTransfer()
        }
        if leadTransferee == r.id {
            // leadTransferee 已经是 leader 了
            return nil
        }
        // Transfer leadership should be finished in one electionTimeout, so reset r.electionElapsed.
        r.electionElapsed = 0
        r.leadTransferee = leadTransferee
        // 如果 leadTransferee 的 log 已经是最新的了 则马上调用 sendTimeoutNow，开始 transfer
        if pr.Match == r.raftLog.lastIndex() {
            r.sendTimeoutNow(leadTransferee)
        } else {
            // 否则先往 leadTransferee append 日志
            r.sendAppend(leadTransferee)
        }

        ...
        // 收到 append 回包后，检查是不是有 in progress 的 leader transfer，并且 log 也是最新了的话，则调用 sendTimeoutNow
        case pb.MsgAppResp:
        pr.RecentActive = true

        ...
        // Transfer leadership is in progress.
        if m.From == r.leadTransferee &amp;amp;&amp;amp; pr.Match == r.raftLog.lastIndex() {
            r.sendTimeoutNow(m.From)
        }
        ...
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;Leader transfer 过程中不处理 pb.MsgProp 类型的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgProp:
        ...
        if r.leadTransferee != None {
            // 正在 leader tranfer，不处理 Propose 请求
            return ErrProposalDropped
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;sendTimeoutNow&lt;/code&gt; 发送 pb.MsgTimeoutNow 的请求，看看 follower 如何处理：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepFollower(r *raft, m pb.Message) error {
    switch m.Type {
        case pb.MsgTimeoutNow:
        if r.promotable() {
            // Leadership transfers never use pre-vote even if r.preVote is true; we
            // know we are not recovering from a partition so there is no need for the
            // extra round trip.
            r.campaign(campaignTransfer)
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;campain&lt;/code&gt; 会发送 voteMsg 给 peers 进行选举：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;
func (r *raft) campaign(t CampaignType) {
    var term uint64
    var voteMsg pb.MessageType
    if t == campaignPreElection {
        r.becomePreCandidate()
        voteMsg = pb.MsgPreVote
        // PreVote RPCs are sent for the next term before we&amp;#39;ve incremented r.Term.
        term = r.Term + 1
    } else {
        r.becomeCandidate()  // 变成 Candidate term + 1，此时该节点 term 最大，所以该节点将成为新的 leader
        voteMsg = pb.MsgVote
        term = r.Term
    }
    if r.quorum() == r.poll(r.id, voteRespMsgType(voteMsg), true) {
        // We won the election after voting for ourselves (which must mean that
        // this is a single-node cluster). Advance to the next state.
        if t == campaignPreElection {
            r.campaign(campaignElection)
        } else {
            r.becomeLeader()
        }
        return
    }

    // 发送 voteMsg
    for id := range r.prs {
        if id == r.id {
            continue
        }
        var ctx []byte
        if t == campaignTransfer {
            ctx = []byte(t)
        }
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;当前 leader 变成 follower 之后，会调用 &lt;code&gt;reset&lt;/code&gt;，&lt;code&gt;reset&lt;/code&gt; 将调用 &lt;code&gt;abortLeaderTransfer&lt;/code&gt; 把 &lt;code&gt;r.leadTransferee&lt;/code&gt; 设置为 None，leader transfer 完成。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (r *raft) reset(term uint64) {
    if r.Term != term {
        r.Term = term
        r.Vote = None
    }
    r.lead = None

    r.electionElapsed = 0
    r.heartbeatElapsed = 0
    r.resetRandomizedElectionTimeout()
    r.abortLeaderTransfer()
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Fri, 06 Jul 2018 22:56:37 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-5/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-5/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
      <item>
        <title>etcd-raft 源码学习笔记（Linearizable Read 之 Lease）</title>
        <description>&lt;p&gt;这篇文章介绍 etcd-raft 如何实现 linearizable read（linearizable read 简单的说就是不返回 stale 数据，具体可以看这篇文章 &lt;a href=&quot;https://aphyr.com/posts/313-strong-consistency-models&quot;&gt;《Strong consistency models》&lt;/a&gt;）。&lt;/p&gt;

&lt;p&gt;除了基于 &lt;a href=&quot;http://masutangu.com/2018/07/etcd-raft-note-3/&quot;&gt;ReadIndex&lt;/a&gt; 之外，raft 论文第 8 节还阐述了另一种基于 heartbeat 的 lease 思路：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease, but this would rely on timing for safety (it
assumes bounded clock skew).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;raft 中，follower 至少会在 election timeout 之后才重新进行选举。leader 定期发送 heartbeat，在收到 quonum 节点的回包后的 election timeout 这段时间间隔内，不会有新一轮的选举（因为各个机器的 cpu 时钟有误差，所以这个方案有风险）。&lt;/p&gt;

&lt;p&gt;lease 模式对应用层提供的接口还是 &lt;code&gt;ReadIndex&lt;/code&gt;，应用层处理的方式也和基于 ReadIndex 模式相同。只是 raft 内部逻辑不同。&lt;/p&gt;

&lt;p&gt;如果指定了 leaseBase 的模式，那要求 &lt;code&gt;CheckQuorum&lt;/code&gt; 为 true，&lt;code&gt;validate&lt;/code&gt; 方法做了这个检查：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func (c *Config) validate() error {
    ...
    if c.ReadOnlyOption == ReadOnlyLeaseBased &amp;amp;&amp;amp; !c.CheckQuorum {
        return errors.New(&amp;quot;CheckQuorum must be enabled when ReadOnlyOption is ReadOnlyLeaseBased&amp;quot;)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;指定了 &lt;code&gt;checkQuorum&lt;/code&gt; 为 true 之后，每次 tick 都会看是否应该检查 Quorum（间隔 electionTimeout），通过发送 pb.MsgCheckQuorum 类型的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// tickHeartbeat is run by leaders to send a MsgBeat after r.heartbeatTimeout.
func (r *raft) tickHeartbeat() {
    r.heartbeatElapsed++
    r.electionElapsed++

    if r.electionElapsed &amp;gt;= r.electionTimeout {  // 每隔 electionTimeout 检查一次
        r.electionElapsed = 0
        if r.checkQuorum {
            r.Step(pb.Message{From: r.id, Type: pb.MsgCheckQuorum})  // 检查 Quorum
        }
        // If current leader cannot transfer leadership in electionTimeout, it becomes leader again.
        if r.state == StateLeader &amp;amp;&amp;amp; r.leadTransferee != None {
            r.abortLeaderTransfer()
        }
    }

    if r.state != StateLeader {
        return
    }

    if r.heartbeatElapsed &amp;gt;= r.heartbeatTimeout {
        r.heartbeatElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgBeat})
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;stepLeader&lt;/code&gt; 收到 pb.MsgCheckQuorum 后调用 &lt;code&gt;checkQuorumActive&lt;/code&gt; 进行检查，如果返回 false，此时把节点变更为 follower：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgCheckQuorum:
        if !r.checkQuorumActive() {  
            r.becomeFollower(r.Term, None)  // checkQuorumActive 失败，变成 follower
        }
        return nil
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;checkQuorumActive&lt;/code&gt; 即统计 active 的 peers 数量是否超过 quonum：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;// checkQuorumActive also resets all RecentActive to false.
func (r *raft) checkQuorumActive() bool {
    var act int

    r.forEachProgress(func(id uint64, pr *Progress) {
        if id == r.id { // self is always active
            act++
            return
        }

        if pr.RecentActive &amp;amp;&amp;amp; !pr.IsLearner {
            act++
        }

        pr.RecentActive = false
    })

    return act &amp;gt;= r.quorum()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;RecentActive&lt;/code&gt; 是 leader 收到 peers 的心跳回包或者 appendEntriesReq 的回包时设置的：&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // All other message types require a progress for m.From (pr).
    pr := r.getProgress(m.From)

    switch m.Type {
    case pb.MsgAppResp:
        pr.RecentActive = true
        ...
    case pb.MsgHeartbeatResp:
        pr.RecentActive = true
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;文章开头提到， leaseBase 模式下 etcd-raft 对应用层暴露的也是 &lt;code&gt;ReadIndex&lt;/code&gt; 接口。在收到 pb.MsgReadIndex 类型的请求时，由于 CheckQuonum 保证了我们 leader 有效，就可以直接 append 到 r.readStates 中。&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;func stepLeader(r *raft, m pb.Message) error {
    // These message types do not require any progress for m.From.
    switch m.Type {
    case pb.MsgReadIndex:
        // 5.4 safety
        if r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(r.raftLog.committed)) != r.Term {
            // Reject read only request when this leader has not committed any log entry at its term.
            return nil
        }

        // thinking: use an interally defined context instead of the user given context.
        // We can express this in terms of the term and index instead of a user-supplied value.
        // This would allow multiple reads to piggyback on the same message.
        switch r.readOnly.option {
        case ReadOnlyLeaseBased: // leaseBase 模式
            ri := r.raftLog.committed
            if m.From == None || m.From == r.id { // from local member
                r.readStates = append(r.readStates, ReadState{Index: r.raftLog.committed, RequestCtx: m.Entries[0].Data})
            } else {
                r.send(pb.Message{To: m.From, Type: pb.MsgReadIndexResp, Index: ri, Entries: m.Entries})
            }
        }

        return nil
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</description>
        <pubDate>Fri, 06 Jul 2018 08:46:43 +0800</pubDate>
        <link>http://masutangu.com/2018/07/etcd-raft-note-4/</link>
        <guid isPermaLink="true">http://masutangu.com/2018/07/etcd-raft-note-4/</guid>
        
        
        <category>源码阅读</category>
        
      </item>
    
  </channel>
</rss>
