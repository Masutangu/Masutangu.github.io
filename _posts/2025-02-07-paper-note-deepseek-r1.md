---
layout: post
date: 2025-2-7T16:23:15+08:00
title: 【论文笔记】DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
tags: 
  - 机器学习
  - 读书笔记
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


本文是[《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》](https://arxiv.org/pdf/2501.12948)的笔记。


## Abstract

我们介绍了我们的第一代推理模型，DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 是通过大规模强化学习（RL）进行训练的模型，没有经过监督微调（SFT）作为初步步骤，展示了卓越的推理能力。通过 RL，DeepSeek-R1-Zero 自然地展现出许多强大而有趣的推理行为。然而，它面临着诸如可读性差和语言混合等挑战。为了解决这些问题并进一步提升推理性能，我们引入了 DeepSeek-R1，它在 RL 之前采用了多阶段训练和冷启动数据。DeepSeek-R1 在推理任务上达到了与 OpenAI-o1-1217 相当的性能。

## 1. Introduction

最近 **post-training 已经成为完整训练流程中的一个重要组成部分**。研究表明，**后训练可以提高推理任务的准确性，与社会价值观保持一致，并适应用户的偏好，同时相对于预训练而言，所需的计算资源相对较少**。OpenAI 的 o1 系列模型是第一个通过增加思维链推理过程的长度来进行推理时扩展（inference-time scaling）的模型。这种方法在数学、编码和科学推理等各种推理任务中取得了显著的改进。然而，有效的 test-time scaling 仍然是尚未解决的问题。

在本文中，我们迈出了使用纯强化学习（RL）提高语言模型推理能力的第一步。我们的目标是探索 LLMs 在没有任何监督数据的情况下发展推理能力的潜力，专注于它们通过纯 RL 过程的自我进化。具体来说，我们使用 DeepSeek-V3-Base 作为基础模型，并采用 GRPO 作为 RL 框架来提高推理性能。

在训练期间，DeepSeek-R1-Zero 自然地出现了许多强大且有趣的推理行为。经过数千步的 RL 之后，DeepSeek-R1-Zero 在推理基准测试中表现出色。然而，DeepSeek-R1-Zero 遇到了可读性差和语言混合等问题。

为了解决这些问题并进一步提高推理性能，我们引入了 DeepSeek-R1，它结合了少量的冷启动数据和多阶段训练流程。首先收集数千个冷启动数据来微调 DeepSeek-V3-Base 模型。在此之后，执行类似 DeepSeek-R1-Zero 的面向推理的 RL。在 RL 过程接近收敛时，我们通过在 RL checkpoints 上进行拒绝抽样，结合来自 DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，创建新的 SFT 数据，然后重新训练 DeepSeek-V3-Base 模型。在使用新数据进行微调后，checkpoint 将经历额外的 RL 过程，考虑到所有场景的提示。经过这些步骤，我们获得了一个称为 DeepSeek-R1 的 checkpoint，其性能与 OpenAI-o1-1217 相当。

我们进一步探索了从 DeepSeek-R1 到更小的密集模型的蒸馏。使用 Qwen2.5-32B 作为基础模型，**直接从 DeepSeek-R1 进行蒸馏比在其上应用 RL 表现更好。这表明，较大基础模型所发现的推理模式对于提升推理能力至关重要。**值得注意的是，我们蒸馏的 14B 模型在推理基准测试中的表现大大超过了最先进的开源 QwQ-32B-Preview，而蒸馏的 32B 和 70B 模型在密集模型中设置了新的记录。

### 1.1. Contributions

**后训练：基础模型上的大规模强化学习**

我们直接将 RL 应用于基础模型，而不依赖于 SFT 作为初步步骤。这种方法**允许模型探索解决复杂问题的思维链（CoT）**，从而开发出 DeepSeek-R1-Zero。DeepSeekR1-Zero 展示了自我验证、反思和生成长思维链等能力，标志着研究领域的一个重要里程碑。值得注意的是，这是**第一个开放研究验证了 LLMs 的推理能力可以通过纯粹的 RL 来激励，而不需要 SFT**。这一突破为未来的发展铺平了道路。

我们介绍了开发 DeepSeek-R1 的流程。该流程包括两个旨在发现改进的推理模式和与人类偏好一致的 RL 阶段，以及两个作为模型推理和非推理能力基础的 SFT 阶段。

**蒸馏：小模型也可以很强大**

- 我们证明了**大模型的推理模式可以蒸馏到小模型中，从而比在小模型上通过 RL 发现的推理模式有更好的表现。**开源的 DeepSeek-R1 及其 API 将使研究社区在未来蒸馏出更好的小模型受益。
- 使用 DeepSeek-R1 生成的推理数据对研究界广泛使用的几个密集模型进行了微调。评估结果表明，蒸馏出的小密集模型在基准测试中表现异常出色，显著优于以前的开源模型，并且与o1-mini相当。

## 2. Approach

### 2.1. Overview

以往的研究在提升模型性能方面严重依赖大量的监督数据。在本研究中，我们证明通过大规模的强化学习，即使不使用监督微调作为冷启动，推理能力也可以显著提高。此外，通过引入少量的冷启动数据，性能还可以进一步提升。

### 2.2. DeepSeek-R1-Zero：基础模型上的强化学习

强化学习在推理任务中展现出了显著的有效性，然而，这些工作严重依赖于监督数据，而监督数据的收集是非常耗时的。在本节中，我们探索了大型语言模型在**没有任何监督数据**的情况下发展推理能力的潜力，重点关注它们通过纯粹的强化学习过程自我进化。

#### 2.2.1. 强化学习算法

为了节省强化学习的训练成本，我们采用了 Group Relative Policy Optimization（GRPO）。

#### 2.2.2. Reward Modeling

奖励是训练信号的来源，决定了 RL 的优化方向。为了训练 DeepSeek-R1-Zero，我们采用了一个基于规则的奖励系统，该系统主要包括两种类型的奖励：

- **准确性奖励（Accuracy rewards）**：准确性奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定的格式（例如，在一个框内）提供最终答案，从而实现可靠的原则性正确性验证。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。

- **格式奖励（Format rewards）**：除了准确性奖励模型外，我们还使用了一个格式奖励模型，该模型强制模型将其思考过程放在 `<think>` 和 `</think>` 标签之间。

我们在开发 DeepSeek-R1-Zero 时没有应用结果或过程神经奖励模型（neural reward model），因为我们发现神经奖励模型在大规模强化学习过程中可能会遭受奖励黑客（reward hacking）攻击，并且重新训练奖励模型需要额外的训练资源，这使得整个训练流程变得复杂。

#### 2.2.3. 训练模板

为了训练 DeepSeek-R1-Zero，我们首先设计了一个简单的模板，引导基础模型遵循我们指定的指令。如表 1 所示，该模板要求 DeepSeek-R1-Zero 首先生成一个推理过程，然后给出最终答案。我们有意将约束限制在这种结构格式上。

```
A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user
with the answer. The reasoning process and answer are enclosed within <think> </think> and
<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>
<answer> answer here </answer>. User: {prompt}. Assistant:
```

*表1：DeepSeek-R1-Zero 的模板。在训练过程中，prompt 将被具体的推理问题替换。*


### 2.3. Deepseek-R1: 冷启动的强化学习

受到 DeepSeek-R1-Zero 的启发，引申出两个的问题：1）通过将少量高质量数据作为冷启动，能否进一步提高推理性能或加快收敛速度？2）我们如何训练一个用户友好的模型，不仅能够产生清晰连贯的思维链，而且还展示出强大的通用能力？为了解决这些问题，我们设计了 DeepSeek-R1 的训练流程。该流程包括以下四个阶段。

#### 2.3.1. 冷启动

为了防止基础模型在强化学习训练的早期不稳定冷启动阶段出现问题，我们为 DeepSeek-R1 构建并收集了一小部分长 CoT 数据，用来微调模型作为初始 RL actor。为了收集这些数据，我们探索了几种方法：

- 使用以长 CoT 为例的 few shot prompting
- 直接 prompt 模型生成带有反思和验证的详细答案
- 收集 DeepSeek-R1-Zero 的可读格式输出
- 通过人工完善结果

在这项工作中，我们收集了数千条冷启动数据，以微调 DeepSeek-V3-Base 作为 RL 的起点。与 DeepSeek-R1-Zero 相比，冷启动数据的优势包括：

- **可读性**：DeepSeek-R1-Zero 的一个主要限制是其内容通常不适合阅读。响应可能混合多种语言或缺乏用于突出答案的 markdown 格式。相比之下，在为 DeepSeek-R1 创建冷启动数据时，我们设计了一种可读的模式，包括在每个响应的末尾加上摘要，并过滤掉不适合读者阅读的响应。这里，我们将输出格式定义为 `|special_token|<reasoning_process>|special_token|<summary>`，其中 reasoning_process 是针对 query 的 CoT，summary 用于总结推理结果。

- **潜力**：通过仔细设计带有人类先验的冷启动数据模式，我们观察到与 DeepSeek-R1-Zero 相比有更好的性能。我们认为迭代训练是推理模型的更好方式。

#### 2.3.2. 以推理为导向的强化学习

在基于冷启动数据对 DeepSeek-V3-Base 进行微调后，我们应用了与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程。这一阶段重点增强模型的推理能力，特别是在编码、数学、科学和逻辑推理等推理密集型任务中，这些任务涉及定义明确的问题并有清晰的解决方案。

在训练过程中，我们观察到 CoT 经常出现语言混合现象，特别是当 RL 提示涉及多种语言时。为了缓解语言混合问题，我们在 RL 训练中引入了语言一致性奖励，该奖励计算为 CoT 中目标语言单词的比例。尽管消融实验表明这种对齐会导致模型性能略有下降，但这种奖励符合人类偏好，使其更易读。

最后，我们通过直接将推理任务的准确性和语言一致性奖励相加来形成最终奖励。然后我们对微调后的模型进行 RL 训练，直到其在推理任务上达到收敛。

#### 2.3.3. 拒绝采样和监督微调

当面向推理的强化学习收敛时，我们利用得到的 checkpoint 来收集用于下一轮的 SFT 数据。与主要关注推理的初始冷启动数据不同，此阶段纳入了来自其他领域的数据，以增强模型在写作、角色扮演和其他通用任务方面的能力。我们按照如下方法生成数据。

**推理数据**

我们通过从上述强化学习训练的检查点中进行拒绝抽样，筛选推理提示并生成推理轨迹。在这个阶段，我们通过纳入额外的数据来扩展数据集，其中一些数据使用生成式奖励模型，将真实数据和模型预测输入 DeepSeek-V3 进行判断。此外，由于模型输出有时混乱且难以阅读，我们已经过滤掉了混合语言、长段落和代码块的思维链。对于每个提示，我们抽样多个回答，只保留正确的回答。总共，我们收集了约 60 万个与推理相关的训练样本。

**非推理数据**

对于非推理数据，如写作、事实问答、自我认知和翻译，我们采用 DeepSeek-V3 流程并重用部分 DeepSeek-V3 的 SFT 数据集。对于某些非推理任务，在回答问题之前，我们通过提示调用 DeepSeek-V3 生成潜在的思维链。对于像“hello”这样的简单查询，则在响应中不提供思维链。最终收集了总共约 20 万个与推理无关的训练样本。

我们使用上述约 80 万个样本的数据集对 DeepSeek-V3-Base 进行了两轮微调。

#### 2.3.4. 适用于所有场景的强化学习

为了进一步使模型与人类偏好保持一致，我们实施了一个旨在提高模型的有用性和无害性同时精炼其推理能力的二级强化学习阶段。我们结合了奖励信号和多样的 prompt 分布来训练模型。

对于推理数据，我们遵循 DeepSeek-R1-Zero 中概述的方法论，利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程。对于一般数据，我们借助于奖励模型来捕捉复杂和微妙场景中的人类偏好。我们基于 DeepSeek-V3 流程并采用类似的偏好对分布和训练提示。对于有用性，我们专注于最终摘要，确保评估强调响应对用户的有效性和相关性，同时最大程度地减少对底层推理过程的干扰。对于无害性，我们评估模型的整个响应，包括推理过程和摘要，以识别和减轻在生成过程中可能出现的任何潜在风险、偏见或有害内容。最终，奖励信号和多样数据分布的整合使我们能够训练出一个在推理方面表现出色同时优先考虑有用性和无害性的模型。

### 2.4. 蒸馏：赋予小模型推理能力

为了使更高效的小模型具备像 DeepSeek-R1 这样的推理能力，我们直接使用 DeepSeek-R1 精选的 80 万样本对开源模型 Qwen 和 Llama 进行了微调，具体细节见第 2.3.3 节。我们的研究表明，这种**直接的蒸馏方法显著提高了小模型的推理能力**。我们在这里使用的基础模型包括 Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B 和 Llama-3.3-70B-Instruct。我们选择 Llama-3.3 是因为其推理能力略优于 Llama-3.1。

**对于蒸馏模型，我们仅应用 SFT**，不包括 RL 阶段，即使加入 RL 可能会大幅提高模型性能。我们在这里的主要目标是展示蒸馏技术的有效性，将 RL 阶段的探索留给更广泛的研究社区。

## 4. 讨论

### 4.1. 蒸馏 VS 强化学习

如果不对模型进行蒸馏，通过本文中讨论的大规模强化学习训练，模型能否达到可比的性能？

为了回答这个问题，我们对 Qwen-32B-Base 进行大规模 RL 训练，使用数学、代码和 STEM 数据，训练超过 10K 步骤，得到 DeepSeek-R1-Zero-Qwen-32B。实验结果表明 32B 基础模型经过大规模 RL 训练后，其性能与 QwQ-32B-Preview 相当。然而，从 DeepSeek-R1 蒸馏得到的 DeepSeek-R1-Distill-Qwen-32B，在所有基准测试中的表现都显著优于 DeepSeek-R1-Zero-Qwen-32B。

因此，我们可以得出两个结论：
- **将更强大的模型蒸馏到更小的模型中可以产生优异的结果，而较小的模型依通过大规模 RL 需要巨大的计算能力，甚至可能无法达到蒸馏的性能。**
- 尽管蒸馏策略既经济又有效，但要超越智能的边界可能仍然需要更强大的基础模型和更大规模的强化学习。

## 5. 结论、局限性与未来工作

在这项工作中，我们分享了通过强化学习提升模型推理能力的探索之旅。DeepSeek-R1-Zero 代表了一种纯粹的 RL 方法，不依赖于冷启动数据，在各种任务中都取得了强大的性能。DeepSeek-R1 更为强大，它结合了冷启动数据和 RL 微调迭代。最终，DeepSeek-R1 在一系列任务上的表现与 OpenAI-o1-1217相当。

我们进一步探索将推理能力蒸馏到小型密集模型中。我们使用 DeepSeek-R1 作为教师模型生成 80 万训练样本，并对几个小型密集模型进行微调。结果令人鼓舞：DeepSeek-R1-Distill-Qwen-1.5B 在数学基准测试中超过了 GPT-4o 和 Claude-3.5-Sonnet，其他密集模型也取得了令人印象深刻的结果，显著优于基于相同底层检查点的其他指令调整模型。
