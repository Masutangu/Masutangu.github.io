---
layout: post
date: 2024-11-23T20:52:46+08:00
title: 【论文笔记】RoBERTa-A Robustly Optimized BERT Pretraining Approach
tags: 
  - 机器学习
  - 读书笔记
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

本文是 [《RoBERTa: A Robustly Optimized BERT Pretraining Approach》](https://arxiv.org/pdf/1907.11692) 的笔记。

## Abstract

语言模型预训练带来了显著的性能提升，但不同方法之间的仔细比较具有挑战性。训练计算成本高昂，通常在不同大小的私有数据集上进行，而且我们将展示，**超参数选择对最终结果有重大影响**。我们进行了 BERT 预训练（Devlin等人，2019）的复制研究，仔细测量了许多关键超参数和训练数据规模的影响。我们发现 BERT 的训练不足，并且可以匹配或超过在其之后发布的每一个模型的性能。我们最好的模型在 GLUE、RACE 和 SQuAD 上实现了最先进的结果，这些结果突出了以前被忽视的设计选择的重要性。

## 1 Introduction

自训练方法如 ELMo (Peters et al., 2018)、GPT (Radford et al., 2018)、BERT (Devlin et al., 2019)、XLM (Lample and Conneau, 2019) 和 XLNet (Yang et al., 2019) 带来了显著的性能提升，但确定这些方法中**哪些方面贡献最大**具有挑战性。训练计算成本高昂，限制了可以进行的调整量，并且通常使用不同规模的私有训练数据进行，限制了我们测量建模进展效果的能力。

我们提出了一个对 BERT 预训练 (Devlin et al., 2019) 的复制研究，其中包括对**超参数调整**和**训练集大小**影响的仔细评估。我们发现 BERT 显著欠训练（undertrained），并提出了一种改进的 BERT 模型训练方法，我们称之为 RoBERTa，它可以匹配或超过所有后 BERT 方法的性能。

我们的修改很简单，包括：(1) 更长的训练时间，使用更大的批次，覆盖更多的数据；(2) 移除下一句预测目标；(3) 在更长的序列上进行训练；以及 (4) 动态改变应用于训练数据的掩码模式。我们还收集了一个大型新数据集 (CC-NEWS)，其大小与其他私有使用的数据集相当，以更好地控制训练集大小的影响。

在控制训练数据的情况下，我们改进的训练过程在 GLUE 和 SQuAD 两个任务上都优于已发布的 BERT 结果。总体而言，我们重新确认了 BERT 的掩码语言模型训练目标与其他最近提出的训练目标（如扰动自回归语言建模）相比具有竞争力。

本文的贡献是：
* 我们提出了一组重要的 BERT 设计选择和训练策略，并介绍了一些替代方案，可以提高下游任务的性能
* 我们使用了一个新颖的数据集 CCNEWS，并确认使用更多数据进行预训练可以进一步提高下游任务的性能
* 我们的训练改进表明，在正确的设计选择下，掩码语言模型预训练与所有其他最近发布的方法相比具有竞争力

## 2 背景

在本节中，我们将简要概述 BERT（Devlin et al., 2019）预训练方法以及我们将在下一节中实验性检查的一些训练选择。

### 2.1 设置

BERT 模型的输入是由两个片段（token 序列）拼接而成的，$x_1, ..., x_N$ 和 $y_1, ..., y_M$。这些片段通常包含不止一个自然句子。这两个片段作为一个单一的输入序列呈现给 BERT，并使用特殊token 分隔： $[CLS], x_1, ..., x_N , [SEP], y_1, ..., y_M, [EOS]$。$M$ 和 $N$ 需要满足 $M + N < T$，其中 $T$ 是用于控制在训练过程中允许的最大序列长度的参数。

该模型首先在大规模无标签的文本语料库上进行预训练，然后使用特定任务的有标签数据进行微调。

### 2.2 架构

BERT 使用 Transformer 架构（Vaswani et al., 2017）。我们使用的 Transformer 架构包含 $L$ 层。每个块使用 $A$ 个自注意力头和隐藏维度 $H$。

### 2.3 训练目标

在预训练过程中，BERT 使用两个目标：

#### 掩码语言模型（MLM）

在输入序列中随机选择一些 token 并用特殊 token [**MASK**] 替换。MLM 目标是预测被掩码 token 的交叉熵损失。BERT 均匀地选择输入 token 的 15% 进行可能的替换。在选中的 token 中，80% 被替换为 [**MASK**]，10% 保持不变，另外 10% 被随机选择的词汇 token 替换。

在原始实现中，随机掩码和替换在开始时执行一次，并在训练期间保持不变，尽管实际上数据会被复制，因此每个训练句子的掩码并不总是相同的（见第 4.1 节）。

#### 下一句预测（NSP）

NSP 是一个用于预测两个片段在原始文本中是否相邻的二分类损失。正例是通过从文本语料库中取出连续的句子来创建的。反例是通过将不同文档中的片段配对来创建的。正例和负例以相同的概率被抽样。

NSP 目标旨在提高需要推理句子对之间关系的下游任务的性能。

### 2.4 优化

BERT 使用 Adam 优化器进行优化，参数设置如下：

- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\epsilon = 1e^{-6}$
- $L_2$ 权重衰减为 0.01

学习率在前 10,000 步内逐渐预热（warmed up）至峰值 $1e^{-4}$，然后线性衰减。BERT 在所有层和注意力权重上使用 0.1 的 dropout，并采用 GELU 激活函数。模型预训练 $S = 1,000,000$ 次更新，每个小批量包含 $B = 256$ 个序列，每个序列的最大长度为 $T = 512$ 个 token。

### 2.5 数据

BERT 模型是在 BOOKCORPUS（Zhu et al., 2015）和英文维基百科的组合上进行训练的，总共包含 16GB 的未压缩文本数据。

## 3 实验设置

在本节中，我们将描述我们对 BERT 进行复制研究的实验设置。

### 3.1 实现

我们在 FAIRSEQ（Ott等人，2019）中重新实现了 BERT。我们主要遵循原始 BERT 的优化超参数，如第2节所述，除了**峰值学习率**和**预热步数**，这两个参数针对每个设置分别进行调整。我们还发现训练对 Adam epsilon 项非常敏感，在某些情况下，调整它后可以获得更好的性能或提高稳定性。同样，我们发现在使用大批量训练时，设置 $\beta_2 = 0.98$ 可以提高稳定性。

我们的预训练序列最多包含 $T = 512$ 个token。与原始 BERT 不同，我们不会随机插入短序列，也不会在前 90% 的更新中使用缩短的序列长度进行训练。我们只使用完整长度的序列进行训练。

我们在 DGX-1 机器上使用混合精度浮点运算进行训练，每台机器配备 8 个 32GB 的 Nvidia V100 GPU，通过 Infiniband 相互连接。

### 3.2 数据

BERT 风格的预训练在很大程度上依赖于大量的文本数据。Baevski 等人（2019）证明增加数据量可以提高最终任务的性能。一些研究已经在比原始 BERT 更大、更多样化的数据集上进行训练。不幸的是，并非所有的额外数据集都可以公开发布。在我们的研究中，我们专注于收集尽可能多的数据以进行实验，使我们能够适当地匹配每个比较的整体质量和数据量。

我们考虑了五个不同大小和领域的英语语料库，总共超过 160GB 的未压缩文本。我们使用以下文本语料库：

- **BOOKCORPUS**（Zhu 等人，2015）加上**英文维基百科**。这是用于训练 BERT 的原始数据。（16GB）。

- **CC-NEWS**，我们从 CommonCrawl 新闻数据集的英文部分收集（Nagel，2016）。该数据包含 2016 年 9 月至 2019 年 2 月之间抓取的 6300 万篇英文新闻文章。（过滤后 76GB）。

- **OPENWEBTEXT**（Gokaslan 和 Cohen，2019），是对 Radford 等人（2019）的 WebText 语料库的开源再创造。文本是从 Reddit 上至少获得三个赞的 URL 中提取的网页内容。（38GB）。

- **STORIES**，Trinh 和 Le（2018）引入的一个数据集，包含 CommonCrawl 数据的一个子集，经过过滤以匹配 Winograd 模式的故事风格。（31GB）。

### 3.3 评估

在本研究中，我们使用以下三个基准测试来评估我们的预训练模型在下游任务上的性能。

#### GLUE

通用语言理解评估（GLUE）基准（Wang et al., 2019b）是一组用于评估自然语言理解系统的 9 个数据集的集合。任务被构建为单句分类或句子对分类任务。GLUE 组织者提供了训练和开发数据的分割，以及一个提交服务器和排行榜，允许参与者在私有保留的测试数据上评估和比较他们的系统。

#### SQuAD

斯坦福问答数据集（SQuAD）提供了一个段落和一个问题。任务是通过从上下文中提取相关片段来回答问题。我们在 SQuAD 的两个版本上进行了评估：V1.1 和 V2.0。在 V1.1 中，上下文总是包含一个答案，而在 V2.0 中，一些问题在提供的上下文中没有答案，这使得任务更具挑战性。

对于 SQuAD V1.1，我们采用了与 BERT 相同的片段预测方法（span prediction method）。对于 SQuAD V2.0，我们增加了一个额外的二元分类器来预测问题是否可回答，我们通过将分类和 span loss 项相加来联合训练该分类器。在评估期间，我们只对分类为可回答的进行片段预测。

#### RACE

阅读理解考试（RACE）（Lai et al., 2017）任务是一个大规模的阅读理解数据集，包含超过 28,000 篇文章和近 100,000 个问题。该数据集收集自中国为中学生设计的中英文考试。在 RACE中，每篇文章都与多个问题相关联。对于每个问题，任务是从四个选项中选择一个正确答案。RACE 的上下文比其他流行的阅读理解数据集要长得多，需要推理的问题比例也非常大。

## 4 训练过程分析

本节探讨并量化了成功预训练 BERT 模型的重要选择。我们保持模型架构不变。具体来说，我们首先使用与 BERTBASE 相同的配置（L = 12, H = 768, A = 12, 110M 参数）来训练 BERT 模型。

### 4.1 静态与动态掩码

在第2节中讨论过，BERT 依赖于随机掩码和预测 token。原始的 BERT 实现在数据预处理期间执行一次掩码，产生一个单一的*静态*掩码。为了避免在每个 epoch 的每个训练实例中使用相同的掩码，训练数据被复制了 10 次，以便在 40 个训练周期中每个序列以 10 种不同的方式被掩码。因此，在训练期间，每个训练序列都会以相同的掩码出现四次。

我们将这种策略与**动态掩码**进行比较，在动态掩码中，我们每次将序列输入模型时都生成掩码模式。当预训练更多步骤或使用更大的数据集时，这一点变得至关重要。

| 掩码方式 | SQuAD 2.0 | MNLI-m | SST-2 |
|----------|-----------|--------|-------|
| 参考值   | 76.3      | 84.3   | 92.8  |
| 我们的静态重实现 | 78.3 | 84.3      | 92.5   |
| 动态掩码 | 78.7      | 84.0   | 92.9  |

*表1：BERTBASE的静态与动态掩码比较。我们报告了 SQuAD 的 F1 值和 MNLI-m、SST-2 的准确率。报告的结果是基于5个随机初始化（种子）的中位数。*

我们发现，使用静态掩码的重实现与原始 BERT 模型的表现相似，而动态掩码的表现与静态掩码相比，效果相当或稍微更好。鉴于这些结果以及动态掩码带来的额外效率优势（无需复制），我们在其余实验中使用动态掩码。

### 4.2 模型输入格式与下一句预测

在原始的 BERT 预训练过程中，除了掩码语言建模目标外，模型还通过辅助的下一句预测（NSP）损失来预测观察到的文档片段是来自同一文档还是不同文档。

NSP 损失被假设为训练原始 BERT 模型的一个重要因素。原始 BERT 论文中提到，移除 NSP 会损害性能，在 QNLI、MNLI 和 SQuAD 1.1 上有显著的性能下降。然而，一些最近的工作质疑了 NSP 损失的必要性（Lample 和 Conneau，2019；Yang 等人，2019；Joshi 等人，2019）。

为了更好地理解这种差异，我们比较了几种替代的训练格式：

- **SEGMENT-PAIR+NSP**：遵循 BERT（Devlin 等人，2019）中使用的原始输入格式，并带有 NSP 损失。每个输入有一对片段，每个片段可以包含多个自然句子，但总组合长度必须小于 512 个 token。
- **SENTENCE-PAIR+NSP**：每个输入包含一对自然句子，这些句子要么从一个文档的连续部分采样，要么来自不同的文档。由于这些输入明显短于 512 个 token，我们增加了批量大小，以使总 token 数量与 SEGMENT-PAIR+NSP 相似。我们保留了 NSP 损失。
- **FULL-SENTENCES**：每个输入都打包了一个或多个文档中连续采样的完整句子，总长度最多为 512 个 token。输入可能会跨越文档边界。当我们到达一个文档的末尾时，我们开始从下一个文档采样句子，并在文档之间添加一个额外的分隔符 token。我们移除了 NSP 损失。
- **DOC-SENTENCES**：输入的构建类似于 FULL-SENTENCES，不同之处在于它们不能跨越文档边界。在文档末尾附近采样的输入可能短于 512 个 token，因此我们在这些情况下动态增加批量大小，以实现与 FULL-SENTENCES 类似的总 token 数量。我们移除了 NSP 损失。


| 模型 | SQuAD 1.1/2.0 | MNLI-m | SST-2 | RACE |
|------|---------------|--------|-------|------|
| 我们的重新实现（带 NSP 损失）：SEGMENT-PAIR | 90.4/78.7 | 84.0 | 92.9 | 64.2 |
| 我们的重新实现（带 NSP 损失）：SENTENCE-PAIR | 88.7/76.2 | 82.9 | 92.1 | 63.0 |
| 我们的重新实现（不带 NSP 损失）：FULL-SENTENCES | 90.4/79.1 | 84.7 | 92.5 | 64.8 |
| 我们的重新实现（不带 NSP 损失）：DOC-SENTENCES | 90.6/79.7 | 84.7 | 92.7 | 65.6 |
| $BERT_{BASE}$ | 88.5/76.3 | 84.3 | 92.8 | 64.3 |
| $XLNet_{BASE}$ (K = 7) | -/81.3 | 85.8 | 92.7 | 66.1 |
| $XLNet_{BASE}$ (K = 6) | -/81.0 | 85.6 | 93.4 | 66.7 |

*表 2：在 BOOKCORPUS 和 WIKIPEDIA 上预训练的基础模型的开发集结果。所有模型都训练了 1M 步，批量大小为 256 个序列。我们报告了 SQuAD 的 F1 分数和 MNLI-m、SST-2 和 RACE 的准确率。报告的结果是五次随机初始化（种子）的中位数。*

结果表 2 显示了四种不同设置的结果。我们首先比较了原始 SEGMENT-PAIR 输入格式和 SENTENCE-PAIR 格式；两种格式都保留了 NSP 损失，但后者使用单个句子。我们发现**使用单独的句子会损害下游任务的性能，我们假设这是因为模型无法学习长距离依赖**。

接下来，我们比较了不带 NSP 损失的训练和带有一个文档中的文本块的训练（DOC-SENTENCES）。我们发现这种设置**优于最初发布的 BERTBASE 结果**，并且**移除 NSP 损失**匹配或略微提高了下游任务性能，与 Devlin 等人（2019）相反。原始 BERT 实现可能只是移除了损失项，同时仍然保留了 SEGMENT-PAIR 输入格式。

最后我们发现，**限制序列来自单个文档（DOC-SENTENCES）的表现略优于从多个文档打包序列（FULL-SENTENCES）**。

然而，由于 DOC-SENTENCES 格式导致批量大小可变，我们在其余实验中使用 FULL-SENTENCES，以便与相关工作更容易比较。

### 4.3 使用大批量训练

过去的神经机器翻译工作表明，**当学习率适当增加时，使用非常大的 mini-batches 训练可以提高优化速度和最终任务的性能**（Ott et al., 2018）。最近的研究表明，BERT 也适用于大批量训练（You et al., 2019）。

Devlin et al. (2019) 最初使用 256 个序列的批量大小训练 BERTBASE 1M 步。通过梯度累积，这在计算成本上等同于使用 2K 序列的批量大小训练 125K 步，或使用 8K 序列的批量大小训练 31K 步。

在表 3 中，我们比较了随着批量大小的增加，$BERT_{BASE}$ 在保留的训练数据上的困惑度（perplexity，ppl）和最终任务性能，同时控制了对训练数据的遍历数（number of passes through）。

| bsz   | steps   | lr   | ppl   | MNLI-m   | SST-2   |
|-------|---------|------|-------|----------|---------|
| 256   | 1M      | 1e-4 | 3.99  | 84.7     | 92.7    |
| 2K    | 125K    | 7e-4 | 3.68  | 85.2     | 92.9    |
| 8K    | 31K     | 1e-3 | 3.77  | 84.6     | 92.8    |

*表 3：在使用不同批量大小（bsz）训练的基础模型上，我们比较了在 BOOKCORPUS 和 WIKIPEDIA 上的保留训练数据的困惑度（ppl）和开发集的准确率。我们针对每个设置调整学习率（lr）。这些模型对数据进行相同次数的遍历（epochs），并具有相同的计算成本。*

我们观察到，**使用大批量训练对困惑度以及最终任务的准确率都有所增益**。大批量也更容易通过分布式数据并行训练进行并行化。在后续实验中，我们使用 8K 序列的批量进行训练。

### 4.4 文本编码

#### Byte-Pair Encoding (BPE)
字节对编码（Byte-Pair Encoding，BPE）（Sennrich等，2016）是字符级和词级表示之间的混合形式，可以处理自然语言语料库中常见的大词汇量。BPE 不使用完整的单词，而是依赖于子词单元，这些子词单元是通过对训练语料库进行统计分析来提取的。

BPE 词汇表的大小通常在 10K-100K 子词单元之间。然而，在建模大型和多样化的语料库时，unicode 字符可以占据这个词汇表的相当大一部分。Radford et al. (2019) 引入了一种巧妙的 BPE 实现，它使用**字节**而不是 unicode 字符作为基本子词单元。使用字节可以学习到一个适度的子词词汇表（50K 单元），并且仍可以编码任何输入文本，而不引入任何“未知”标记。

原始的 BERT 实现使用了一个大小为 30K 的字符级 BPE 词汇表，该词汇表是在对输入进行预处理后，使用启发式分词规则学习得到的。根据 Radford等人（2019）的方法，我们改为使用一个更大的基于字节的 BPE 词汇表，其中包含 50K 个子词单元，而不需要对输入进行额外的预处理或分词。这导致 $BERT_{BASE}$ 和 $BERT_{LARGE}$ 分别增加了大约 15M 和 20M 个额外的参数。

早期的实验显示，这些编码之间只有轻微的差异，Radford 等人（2019）的 BPE 在某些任务上的最终任务性能稍差一些。尽管如此，我们认为**通用编码方案的优势超过了性能的轻微降低**，并在我们的其他实验中使用了这种编码。

## 5 Roberta

在本节中，我们将前几节提出的对 BERT 预训练过程的修改进行了汇总，并评估了它们综合影响。我们将这种配置称为 **RoBERTa**（Robustly optimized BERT approach）。具体来说，RoBERTa 采用了**动态掩码**（第4.1节）、**不带 NSP 损失的完整句子**（第4.2节）、**大 mini-batches**（第4.3节）和**更大的字节级 BPE**（第4.4节）进行训练。

此外，我们还研究了两个在前人工作中被低估的重要因素：（1）**用于预训练的数据**；（2）**对数据进行训练的次数**（the number of training passes through the data）。例如，最近提出的 XLNet 架构（Yang et al., 2019）使用的预训练数据几乎是原始 BERT 的 10 倍。它还使用了 8 倍大的批量大小，并进行了一半的优化 steps，因此在预训练期间看到的序列数量是 BERT 的四倍。

为了帮助区分这些因素与其他建模选择（例如，预训练目标）的重要性，我们首先按照 $BERT_{LARGE}$ 架构（L = 24, H = 1024, A = 16, 355M参数）训练 RoBERTa。我们在与 BERT 使用的相当的 BOOKCORPUS 加 WIKIPEDIA 数据集上预训练了 100K 步。使用 1024 个 V100 GPU 预训练我们的模型大约一天。

| 模型 | 数据 | bsz | 步数 | SQuAD (v1.1/2.0) | MNLI-m | SST-2 |
|------|------|-----|------|------------------|--------|-------|
| RoBERTa with BOOKS + WIKI | 16GB | 8K | 100K | 93.6/87.3 | 89.0 | 95.3 |
| + additional data (§3.2) | 160GB | 8K | 100K | 94.0/87.7 | 89.3 | 95.6 |
| + pretrain longer | 160GB | 8K | 300K | 94.4/88.7 | 90.0 | 96.1 |
| + pretrain even longer | 160GB | 8K | 500K | 94.6/89.4 | 90.2 | 96.4 |
| BERTLARGE with BOOKS + WIKI | 13GB | 256 | 1M | 90.9/81.8 | 86.6 | 93.7 |
| XLNetLARGE with BOOKS + WIKI | 13GB | 256 | 1M | 94.0/87.8 | 88.4 | 94.4 |
| + additional data | 126GB | 2K | 500K | 94.5/88.8 | 89.8 | 95.6 |

*表4：随着预训练数据量增加（16GB → 160GB文本）和预训练时间延长（100K → 300K → 500K步），RoBERTa 每一行都累积了上面行的改进。RoBERTa 与 $BERT_{LARGE}$ 的架构和训练目标相匹配。*

我们在表4中展示了我们的结果。在控制训练数据的情况下，我们观察到 RoBERTa 相比于最初报告的 $BERT_{LARGE}$ 结果有了很大的改进，这再次证实了我们第4节探讨的设计选择的重要性。

接下来，我们将这些数据与第3.2节中描述的另外三个数据集结合起来。我们在与之前相同数量的训练步骤（100K）下，在合并的数据上训练 RoBERTa。我们总共在 160GB 的文本上进行了预训练。我们观察到在所有下游任务中都有进一步的性能提升，验证了**预训练中数据大小和多样性的重要性**。最后，我们将 RoBERTa 的预训练时间显著延长，将预训练步骤从 100K 增加到 300K，然后进一步增加到 500K。我们再次观察到下游任务性能的显著提升，300K 和 500K 步模型在大多数任务上超过了 XLNetLARGE。我们注意到，**即使是我们训练时间最长的模型也没有对我们的数据过拟合，很可能还会从额外的训练中受益**。

## 6 相关工作

### 预训练方法

预训练方法的设计有不同的训练目标，包括：

- 语言建模（Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018）
- 机器翻译（McCann et al., 2017）
- 掩码语言建模（Devlin et al., 2019; Lample and Conneau, 2019）

许多近期的论文使用了微调模型的基本配方，针对每个端任务进行微调，并以某种变体的掩码语言模型目标进行预训练。然而，较新的方法通过以下方式提高了性能：

- 多任务微调（Dong et al., 2019）
- 结合实体嵌入 incorporating entity embeddings （Sun et al., 2019）
- 范围预测 span prediction（Joshi et al., 2019）
- 多种变体的自回归预训练 multiple variants of autoregressive pretraining（Song et al., 2019; Chan et al., 2019; Yang et al., 2019）

性能通常也通过训练更大的模型和使用更多的数据得到改善。

## 7 Conclusion

我们仔细评估了在预训练 BERT 模型时所做的多项设计决策。我们发现通过以下方法可以显著提高模型性能：

- **训练模型更长时间**
- **使用更大的批次处理更多数据**
- **移除下一句预测目标**
- **在更长的序列上进行训练**
- **动态掩码**
