---
layout: post
date: 2025-3-24T13:28:16+08:00
title: 【论文笔记】Deep Residual Learning for Image Recognition
tags: 
  - 机器学习
  - 读书笔记
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

本文是 [《Deep Residual Learning for Image Recognition》](https://arxiv.org/pdf/1512.03385) 的笔记。

深度神经网络更难训练，我们提出了一种残差（residual）学习框架，以简化训练比以前更深的网络。我们明确地将层重新定义为学习相对于层输入的残差函数，而不是学习无参考函数。我们提供了全面的实证证据，表明这些残差网络更容易优化，并且可以从显著增加的深度中获得准确性。

## Introduction

由深度的重要性，引出了一个问题：学习更好的网络是否就像堆叠更多的层那样简单？回答这个问题的一个障碍是臭名昭著的**梯度消失/爆炸**问题，这从一开始就阻碍了收敛。然而，通过归一化初始化和中间归一化层，这个问题在很大程度上得到了解决，使得具有数十层的网络能够在使用反向传播的随机梯度下降（SGD）时开始收敛。

当深度网络能够开始收敛时，一个退化问题就暴露出来：随着网络深度的增加，准确性会达到饱和点，然后迅速下降。令人意外的是，这种退化并不是由过拟合引起的，而且在[11, 42]中提出并经过我们的实验证实，**向一个适当深度的模型添加更多层会导致更高的训练误差**。图 1 展示了一个典型的例子。

<img src="/assets/images/paper-note-deep-residual-learning-for-image-recognition/illustration-1.png" width="600" alt=""/>

*图 1. 在 CIFAR-10 数据集上，使用 20 层和 56 层的“普通”网络的训练误差（左）和测试误差（右）。深层网络具有更高的训练误差，因此也有更高的测试误差。*

这种退化（训练准确性的下降）表明，并非所有系统都同样容易优化。让我们考虑一个较浅的架构及其对应的深层模型，后者在其基础上添加了更多层。对于深层模型，存在一种通过构造的解决方案：添加的层是恒等映射，而其他层则是从学习到的较浅模型中复制过来的。这个方案的存在表明，深层模型的训练误差不应该比对应的浅层模型更高。但实验证明，我们目前手头上的求解器无法找到与该构造性解决方案相当或更好的解决方案（或者无法在可行的时间内找到）。

在本文中，我们通过引入深度残差学习框架来解决退化问题。**我们不再希望每几个堆叠的层直接拟合期望的底层映射，而是让这些层拟合一个残差映射。**形式上，将期望的底层映射表示为 $\mathcal{H}(x)$，我们让堆叠的非线性层拟合另一个映射 $\mathcal{F}(x) := \mathcal{H}(x)−x$。原始映射被重新表述为 $\mathcal{F}(x)+x$。**我们假设优化残差映射比优化原始的无参考映射更容易。极端情况下，如果恒等映射是最优的，那么将残差推向零（pushthe residual to zero）比通过一系列非线性层拟合恒等映射更容易。**

$\mathcal{F}(x) + x$ 的表述可以通过具有“快捷连接（shortcut connections）”（图2）的前馈神经网络实现。在我们的情况下，快捷连接简单地执行恒等映射，并将其输出与堆叠层的输出相加（图2）。恒等快捷连接既不增加额外的参数，也不增加计算复杂度。整个网络仍然可以通过随机梯度下降（SGD）和反向传播进行端到端训练，并且可以在常见的库中轻松实现，而无需修改求解器。

<img src="/assets/images/paper-note-deep-residual-learning-for-image-recognition/illustration-2.png" width="600" alt=""/>

*图 2. 残差学习：一个构建块。*

我们在 ImageNet 数据集上进行了全面的实验，以展示退化问题并评估我们的方法。我们展示了以下结果：

1. 我们极深的残差网络易于优化，但当深度增加时，相应的“普通”网络（仅堆叠层）的训练误差更高
2. 我们的深度残差网络可以轻松地从大幅增加的深度中获得准确性的提升，产生比以前的网络更好的结果

## Deep Residual Learning

### Residual Learning

让我们将 $\mathcal{H}(x)$ 视为一个底层映射，由几个堆叠的层（不一定是整个网络）来拟合，其中 $x$ 表示这些层的输入。如果假设多个非线性层可以渐近地逼近复杂的函数，那么等价地假设它们可以渐近地逼近残差函数，即 $\mathcal{H}(x)−x$（假设输入和输出具有相同的维度）。因此，我们不再期望堆叠的层逼近 $\mathcal{H}(x)$，而是明确让这些层逼近一个残差函数 $\mathcal{F}(x) := \mathcal{H}(x)−x$，原始函数则变成了 $\mathcal{F}(x)+x$。虽然这两种形式都应该能够渐近地逼近所需的函数（如假设的那样），但**学习的难易程度可能不同**。

这种重新表述是受到退化问题中反直觉现象的启发。正如我们在引言中讨论的那样，如果添加的层可以构造成恒等映射，那么深层模型的训练误差不应该超过较浅的对应模型。**退化问题表明，求解器可能难以通过多个非线性层逼近恒等映射。通过残差学习的重新表述，如果恒等映射是最优的，求解器可以简单地将多个非线性层的权重驱向零，以逼近恒等映射。**

在实际情况中，恒等映射很少是最优的。如果最优函数更接近于恒等映射而不是零映射，那么对于求解器来说，通过参考恒等映射来找到扰动应该比学习一个全新的函数更容易。我们通过实验证明（图7），学习到的残差函数通常具有较小的响应，这表明恒等映射提供了合理的预处理。

### Identity Mapping by Shortcuts

在本文中，我们正式定义一个构建块如下：

$$
\mathbf{y}= \mathcal{F}(\mathbf{x},\{W_i\}) + \mathbf{x} \tag{1}
$$

在这里，$x$ 和 $y$ 分别是层的输入和输出向量。函数 $\mathcal{F}(x,\\{W_i\\})$ 表示要学习的残差映射。对于图 2 中的示例，它包含两个层，$\mathcal{F}= W_2σ(W_1x)$，其中 $σ$ 为 ReLU ，为简化表示省略了偏置项。操作 $\mathcal{F}+ x$ 通过快捷连接和逐元素相加来执行。我们在加法之后采用第二个非线性函数（即 $σ(y)$，参见图 2）。

方程（1）中的快捷连接既不引入额外的参数，也不增加计算复杂性。这不仅在实践中具有吸引力，而且在我们对比普通网络和残差网络时也很重要。我们可以公平地比较具有相同参数数量、深度、宽度和计算成本（除了可以忽略的逐元素相加）的纯网络和残差网络。

在公式(1)中，$x$ 和 $\mathcal{F}$ 的维度必须相等。如果不是这种情况，我们可以执行线性投影 $W_s$ 来匹配维度：

$$
\mathbf{y}= \mathcal{F}(\mathbf{x},\{W_i\}) + W_s\mathbf{x} \tag{2}
$$

残差函数 $\mathcal{F}$ 的形式是灵活的。本文的实验涉及一个具有两层或三层的函数 $\mathcal{F}$ ，更多层也是可能的。但如果 $\mathcal{F}$  只有一层，公式(1) 类似于一个线性层：$\mathbf{y}= W_1\mathbf{x} + \mathbf{x}$。对此我们没有观察到改进。

我们还注意到，尽管上述符号是关于全连接层的，它们也适用于卷积层。函数 $\mathcal{F}(x,\\{W_i\\})$ 可以表示多个卷积层。逐元素加法是在两个特征图上进行的，逐通道进行。

## Experiments

### ImageNet Classification

我们在 ImageNet 2012 分类数据集上评估我们的方法，该数据集包含 1000 个类别。

#### Plain Networks

我们首先评估了 18 层和 34 层的普通网络。

结果显示，较深的 34 层普通网络的验证错误率高于较浅的 18 层普通网络。为了揭示原因，我们在图4（左）中比较了它们在训练过程中的训练/验证错误。**我们观察到了退化问题——34层普通网络在整个训练过程中具有更高的训练错误，即使 18 层普通网络的解空间是 34 层网络的子空间。**

<img src="/assets/images/paper-note-deep-residual-learning-for-image-recognition/illustration-3.png" width="600" alt=""/>

我们认为这种优化困难不太可能是由梯度消失引起的。这些普通网络使用了 BN（Batch Normalization）进行训练，确保前向传播的信号具有非零方差。我们还验证了使用 BN 的反向传播梯度表现出健康的范数。因此，前向和反向信号都没有消失。事实上，34 层普通网络仍然能够达到有竞争力的准确性，这表明求解器在某种程度上是有效的。我们推测，深层普通网络可能具有指数级低的收敛速度（exponentially low convergence rates），这影响了训练错误的降低。对于这种优化困难的原因将在未来进行研究。

#### Residual Networks

接下来，我们评估 18 层和 34 层的残差网络。架构与上述普通网络相同，只是在每对 3×3 滤波器中添加了一个快捷连接。在第一次比较中（表 2 和图 4 右），我们对所有快捷连接使用恒等映射，并使用零填充来增加维度。因此，与普通网络相比，它们没有额外的参数。

<img src="/assets/images/paper-note-deep-residual-learning-for-image-recognition/illustration-4.png" width="600" alt=""/>

从表 2 和图 4 中我们有三个主要观察。首先，使用残差学习的情况与普通网络相反——34 层的 ResNet 比 18 层的 ResNet 更好（提高了 2.8%）。更重要的是，34 层的 ResNet 表现出显著更低的训练错误，并且能泛化到验证数据。这表明在这种设置下，退化问题得到了很好的解决，我们成功地从增加的深度中获得了准确性的提升。

其次，与其普通网络相比，34 层的 ResNet 将 top-1 错误率降低了 3.5%（表2），这得益于成功降低的训练错误（图 4 右对比左）。这一比较验证了残差学习在极深系统中的有效性。

最后，我们还注意到 18 层的普通/残差网络具有相当的准确性（表2），但 18 层的 ResNet 收敛更快（图 4 右对比左）。当网络“不过于深”时，当前的 SGD 求解器仍然能够为普通网络找到好的解决方案。ResNet 通过在早期阶段提供更快的收敛来简化优化。

#### Identity vs. Projection Shortcuts.

我们已经展示了无参数的恒等 shortcuts 有助于训练。接下来我们研究投影 shortcuts（公式(2)）。我们比较了三种选项：（A）使用零填充 shortcuts 来增加维度，所有 shortcuts 都是无参数的（与表 2 和图 4 右相同）；（B）使用投影 shortcuts 来增加维度，其他 shortcuts 为恒等；（C）所有 shortcuts 都是投影。结果显示，所有三种选项都比普通网络好得多。B 略优于 A。我们认为这是因为 A 中的零填充维度实际上没有进行残差学习。C 略优于 B，我们将其归因于投影 shortcuts 引入的额外参数。但 A/B/C 之间的小差异表明，投影 shortcuts 对于解决退化问题并不是必需的。